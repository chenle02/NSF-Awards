* 1409847
* CSR: Medium: A Computing Cloud for Graphical Simulation
* CSE,CNS
* 08/01/2014,07/31/2019
* Philip Levis, Stanford University
* Continuing Grant
* Marilyn McClure
* 07/31/2019
* USD 1,093,026.00

Today, many graphical simulations run on a single powerful server or a small
cluster of high-performance, high-cost nodes. This research aims to answer the
question -- is it possible to run graphical simulations in the computational
cloud? -- by designing and implementing Nimbus, a software for graphical
simulation in the computing cloud. The goal is to be able to run large, complex
simulations using on-demand cloud computing systems. Nimbus supports PhysBAM, an
open-source graphical simulation package developed and maintained by Principal
Investigator Fedkiw. The project will collaborate with existing PhysBAM users to
support the Nimbus software for broader use and
adoption.&lt;br/&gt;&lt;br/&gt;Nimbus focuses on three important principles to
support graphical simulations running on hundreds to thousands of cloud servers.
First is decoupling data access and layout. Nimbus represents data in three
layers: program, logical, and physical. These layers separate the units which a
program operates on (program) from the units which the Nimbus software manages
and transfers (logical) from how they are laid out in actual computer memory
(physical). Second is non-uniform, geometry-aware data placement. Nimbus uses
the fact that simulations have a basic underlying geometry to intelligently
place data and computation. This geometry is explicit in the Nimbus software,
which knows that nearby regions of the simulation should be placed on nearby
computers. Third is dynamic assignment and load balancing: Graphical simulations
today divide the simulation volume equally across computers, despite the fact
that some regions require much more computation than others. Nimbus divides a
simulation into a larger number of smaller partitions, which it dynamically
assigns and moves as load changes to reduce running time while considering
inter-partition communication. These three principles allow Nimbus to provide
tremendous flexibility. The system breaks a simulation into small pieces that a
controller computer sends to worker computers to compute. These worker computers
decide when to schedule these simulation pieces and how to assign processors to
different pieces. The runtime automatically moves data in the most efficient
manner possible as needed, compressing data and replicating it when having
multiple copies for different pieces increases performance. Discovering how
these applications can be run on modern data center computing systems will help
bring arithmetically intensive scientific computing to the cloud. As Exascale
and other supercomputing efforts gain momentum, their scale will need to deal
with the same issues cloud systems have been tackling for the past decade,
stragglers, failures, and heterogeneity. By focusing on one particular
compelling application, this work will establish an intellectual framework for
future, broader efforts.