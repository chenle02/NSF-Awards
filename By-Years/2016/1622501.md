* 1622501
* High-Performance, High-Level Tools for Statistical Inference and Unsupervised Learning
* MPS,DMS
* 09/15/2016,12/31/2020
* Alan Edelman, University of California-Davis
* Continuing Grant
* Christopher Stark
* 12/31/2020
* USD 480,000.00

Using the "Julia" language for scientific computing developed at MIT, the UC
Davis, MIT, and Julia Computing, Inc. teams funded by this project will extend
the Julia language and runtime to utilize massively-parallel graphics processing
units (GPUs) as first-class processors for scientific computing. Julia offers
the twin advantages of straightforward, high-level programmability as well as
excellent performance; adding GPU capability within Julia opens the door to even
greater performance. The team will use Julia and its new GPU capabilities to
address emerging important problems in statistical inference and unsupervised
learning, an application area that aims to draw useful conclusions from massive
amounts of data. Using a high-level, high-performance language such as Julia
will allow non-computer-science experts to address these important
problems.&lt;br/&gt;&lt;br/&gt;The project team brings together three threads of
expertise to address the challenge of delivering best-of-breed performance from
a high-level language in the context of the important application domain of
statistical inference and unsupervised learning: (1) application experts in this
domain; (2) the designers of the programming language Julia, which allows its
users to express their ideas in high-level abstractions that are natural to
statisticians and mathematicians; and (3) parallel computing experts, who will
develop the new support within Julia to target high-performance GPUs as first-
class processors. The major outcome of this project will be a significantly
enhanced Julia language and runtime that will deliver both high-level
programmability, targeted at scientists who are not parallel computing experts,
and best-of-breed performance.