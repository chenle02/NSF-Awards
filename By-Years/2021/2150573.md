* 2150573
* Understanding How Approaches to Calibrating and Scoring Survey Item Responses Affect  Results from Growth Mixture Models
* SBE,BCS
* 09/01/2022,08/31/2025
* James Soland, University of Virginia Main Campus
* Standard Grant
* Leher Singh
* 08/31/2025
* USD 420,000.00

Researchers, educators, psychologists, and medical clinicians often want to know
how individuals grow and develop. Further, there is interest in whether there
are distinct--and oftentimes unseen--groups of children or adults based on their
development. For example, researchers might want to understand whether a given
child’s development of self-control is typical or not, or whether there are
distinct patterns in how groups of students learn math as they move through
school. Growth mixture models (GMMs) are statistical tools designed for exactly
that purpose: identifying distinct growth patterns, including groups of
individuals who follow those patterns, when such groupings remain unseen.
Although GMMs have seen widespread use in developmental research in recent
decades, best practices for such tools are not fully established. In particular,
one important question remains unaddressed: when common forms of measurement
bias are present in scores from self-report measures administered over time
(e.g., self- control surveys given to students throughout middle school), how
trustworthy are GMM results using these scores? Among others, one outstanding
issue in GMM is how they are affected by response style bias, which may occur
when respondents have characteristic patterns of responding to questions
regardless of question content, such as always picking the highest or lowest
response option. Failure to apply scoring models which account for response
styles may yield biased score estimates--but the extent to which this impacts
GMM results based on these scores is unknown.&lt;br/&gt;&lt;br/&gt;The current
project uses advances in item response theory (IRT) to examine the effects of
scoring--including failure to account for response styles specifically, as well
as mis-specification of the scoring model more broadly--on class recovery and
parameter estimates in GMM. This is accomplished both through Monte Carlo
simulation and analysis of empirical data. In the simulation study, the
generation of artificial data with different features--including sample size,
number of classes in the GMM, and different measurement issues such as response
style bias--allows a comprehensive examination of when and under what conditions
scoring model mis-specification matters for GMMs. The empirical study, which
applies GMMs to scores of socioemotional development from two large studies,
permits insight into the real-world consequences of scoring decisions in
developmental science. Finally, guidance for researchers will be made available
in the form of a ‘measurement checklist’ informed by the study
results.&lt;br/&gt;&lt;br/&gt;This award reflects NSF's statutory mission and
has been deemed worthy of support through evaluation using the Foundation's
intellectual merit and broader impacts review criteria.