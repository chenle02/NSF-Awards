* 1426787
* NRI: Collaborative Research: Shall I Touch This?: Navigating the Look and Feel of Complex Surfaces
* CSE,IIS
* 07/15/2014,06/30/2018
* Katherine Kuchenbecker, University of Pennsylvania
* Standard Grant
* Jie Yang
* 06/30/2018
* USD 408,000.00

This project improves autonomous robotic perception so that future co-robots can
glance around any scene and accurately estimate how it would feel to grasp or
step on all of the visible surfaces. Just as people do, robots should use such
these physical predictions to guide their interactions with the world, for
example avoiding dangerous ice patches on the ground when walking and driving,
and adeptly anticipating the grasp force needed to pick up everything from ice
cubes to stuffed animals. These research activities are accompanied by
significant outreach efforts, including a new program on "Look and Touch
Robotics" to get middle-school students, particularly those from
under­represented groups, excited about computer science, engineering, and
robotics. This program uses simple experiments to highlight the dual importance
of visual and haptic information during interactions with physical objects,
along with demonstrations of a robot showing visuo-­haptic intelligence. This
project also integrates research and education by involving undergraduates in
the research and via hands-on projects in the vision and robotics classes taught
by the Principal Investigators.

This research involves extensive collection of data from real objects and
surfaces using both visual and haptic sensors. The recorded interactions are
analyzed to uncover visual clues that can allow a robot to infer the physical
characteristics of the surface, such as slipperiness, hardness, and roughness.
This problem is addressed using deep learning, a recently developed approach
that has been successful in enabling robots to visually recognize a wide variety
of objects in diverse circumstances. The research team also builds the database
of visuo-haptic recordings and the learned cross-modal sensory, and makes it
available to other robotics researchers at the end of the project.