* 2139304
* CIF: Small: Information-theoretic privacy and security for personalized distributed learning
* CSE,CCF
* 03/01/2022,02/28/2025
* Suhas Diggavi, University of California-Los Angeles
* Standard Grant
* Phillip Regalia
* 02/28/2025
* USD 500,000.00

Personalized recommendation systems have been widely deployed with centralized
data collection over the web and mobile applications with great success.
Concurrently, they have created significant and justified public concerns about
privacy. Society is in the midst of crossing a new frontier where millions of
devices ranging from simpler Internet-of-Things devices (sensing) to (semi)
autonomous vehicles (cars, drones) are connected over networks. A natural
question is whether and how one can leverage large-scale local data collection
in this emerging ecosystem to collaboratively build distributed personalized
learning systems. This motivates the central question of this project, namely,
how to design personalized learning models with information-theoretic privacy
and security guarantees. The research outcomes of this project will be broadly
disseminated, through publications, involvement of the PI in teaching and
interaction with industry. &lt;br/&gt;&lt;br/&gt;One would ideally like to
design personalized systems that can leverage large-scale collaboration,
maintain privacy of local data, and require trust only on one's own devices as
opposed to other entities. This project therefore explores how to design privacy
schemes which also give good personalized learning performance, and how to
design robust collaborative schemes that give good personalized learning
performance despite malicious participants. In particular, in the task on
privacy for personalized learning, the project plans to explore designs of
personalized privacy mechanisms that are robust to iterative interactions
necessary for collaborative learning and analyze its privacy-performance trade-
off using information-theoretic and statistical tools. In the task on security
for personalized learning, the project leverages ideas from high-dimensional
robust statistics and information theory to develop robust mechanisms with
theoretical guarantees to enable personalized learning in the presence of
malicious participating devices, including investigating when collaboration is
beneficial. The successful completion of the project will advance the state of
the art and build bridges between information theory and trustworthy
federated/distributed machine learning and optimization, through formal privacy
and security guarantees without adversary computational
assumptions.&lt;br/&gt;&lt;br/&gt;This award reflects NSF's statutory mission
and has been deemed worthy of support through evaluation using the Foundation's
intellectual merit and broader impacts review criteria.