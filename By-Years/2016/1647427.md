* 1647427
* EAGER: Planning Believable Narratives by Modeling Agent Beliefs
* CSE,IIS
* 08/01/2016,07/31/2019
* Stephen Ware, University of New Orleans
* Standard Grant
* Todd Leen
* 07/31/2019
* USD 156,969.00

The goal of this project is to improve the software that generates stories
automatically for virtual environments like training simulations and educational
games. Specifically, the software will be able to reason about what is actually
true, what each character thinks is true, what they think others think is true,
and so on, to improve the way virtual characters act and make them seem more
believable and more human. Current approaches to designing these narratives
often assume agents know everything about others' beliefs and goals; this often
leads to inconsistent or un-believable behaviors by the agents, which damage the
credibility of the software and quality of the experience for their human users.
The proposal will extend the lead researcher's existing narrative planning
system, using an approach that lets agents consider multiple sets of beliefs
that are consistent with their own and others' actions so far, ruling out
situations where agents have beliefs that are inconsistent with their actions.
Compared to existing approaches, this should allow the narrative planner to
generate a wider variety of narratives that are also more believable to humans,
as well as to handle situations such as trickery and uncertainty where reasoning
about beliefs is crucial. The research team will test the software and these
assumptions through several experiments that ask people to compare narratives
generated by the new software to those generated by state of the art methods. If
successful, the project sets the stage to improve the quality of systems where
virtual agents interact with humans such as smart phone assistants, online
games, automated customer chat tools, and educational software. In particular,
the work will lead to training scenarios where understanding others' beliefs is
crucial, such as officer-citizen interactions. The work is also
interdisciplinary, ranging from computer science to psychology, and the lead
researcher is committed to training young researchers to do work that crosses
these intellectual boundaries and to recruiting researchers who might not
otherwise participate in computer science-related research.

In the work, the lead researcher proposes to develop a model of agent belief
based on doxastic modal logic and possible worlds reasoning suitable for use in
a planning algorithm that coordinates a virtual environment. By supporting a
single modal 'believes' predicate, the planner can treat the narrative search
space as a Kripke structure to reason about epistemically accessible states.
This improves on previous models by allowing arbitrarily nested beliefs while
simultaneously reducing the burden on the virtual environment's author to write
alternative scenarios, thus increasing their flexibility and expressiveness. The
research team will integrate this model of beliefs into a prototype system based
on the Glaive narrative planner previously developed by the lead researcher.
This prototype will take advantage of Glaive's existing heuristic-driven state-
space search techniques: in addition to expanding temporally accessible states,
Glaive will also expand epistemically accessible states and track when an action
taken by an epistemic child can be anticipated by its epistemic parent in the
Kripke structure. The initial prototype will be too slow for real-time use, but
it will be suitable for conducting the proposed experiments that investigate to
what extent such a model improves the believability of agent behavior in
automatically generated stories. In particular, the team will study whether the
planner produces narratives whose structure better meets the expectations of a
human audience: that is, the model will answer questions about agent beliefs
more similarly to a human audience and the resulting planner will generate
stories more like those composed by human authors. Further, the prototype is
expected to solve certain narrative planning problems which algorithms that lack
a model of agent beliefs cannot solve. These claims will be evaluated by having
the new prototype and two state-of-the-art planners generate narratives for a
library of scenarios to be developed by the team that rely on agents having a
theory of mind for other agents, then asking both the systems and human users a
number of questions about the generated narrative and agents' beliefs to
evaluate how well the planners' output conforms with humans' expectations and
believability.