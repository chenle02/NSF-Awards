* 1520587
* SBIR Phase I:  Say What I Feel
* TIP,TI
* 07/01/2015,12/31/2015
* Lois Brady, iTherapy LLC
* Standard Grant
* Glenn H. Larsen
* 12/31/2015
* USD 149,964.00

This SBIR Phase I project aims to create an emotionally expressive software-
based speech-generating communication system, which is designed for individuals
with little or no verbal communication ability -- particularly those with autism
spectrum disorder. Current alternative communication technology does not offer
the ability to express emotional content: something that is crucial to
effective, comprehensible communication at home, in the community, and at work.
The ability to convey emotions -- sadness, anger, or happiness, for example --
through one's tone of voice or facial expressions has far-reaching educational
and vocational benefits for individuals with communication challenges. In other
words, emotional content helps clarify the communicative intent behind a spoken
message. When non-verbal individuals can communicate thoroughly and effectively,
they increase their ability to receive a free and appropriate public education,
and they expand their vocational opportunities. When individuals can receive an
education and find a job, they can contribute back to society. The proposed work
supports progress in science and engineering, yet it also enhances the potential
for current educational applications and future research studies. Finally, this
project builds on Federal and State efforts to offer people with disabilities
educational and vocational services.

This project will develop and combine novel software algorithms that target
digital facial recognition patterns and speech-based waveforms, in order to
enhance the emotional content of conversational communication for individuals
who cannot speak volitionally. The ability to add emotional content onto
synthesized speech is a new technology that could substantially benefit
individuals with communication challenges, such as people with autism spectrum
disorder. This project seeks to facilitate and augment emotional expression and,
therefore, increase communicative competence for individuals who use software-
based synthesized speech systems to express their ideas, feelings, and wishes.
Measuring whether or not this project accurately conveys human emotion in speech
and facial expressions requires a perceptual test involving neurotypical adults.
Specifically, the perceptual test will include both trained and untrained
judges. The purpose of this is to ensure that the facial expressions as well as
the pitch contour and vocal emphasis align with the emotion labels (i.e., happy,
sad, or mad). In order to accurately calculate the judges' inter-rater
reliability, the study will use a method that determines the extent to which
judges' ratings agree, relative to how much they would likely conform, if they
were to randomly rate the same stimuli.