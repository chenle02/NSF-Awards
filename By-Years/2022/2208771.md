* 2208771
* Collaborative Research: FET: Medium:  Energy-Efficient Persistent Learning-in-Memory with Quantum Tunneling Dynamic Synapses
* CSE,CCF
* 10/01/2022,09/30/2025
* Gert Cauwenberghs, University of California-San Diego
* Standard Grant
* Sankar Basu
* 09/30/2025
* USD 525,000.00

This research project investigates a framework that can significantly improve
the energy-efficiency of training artificial intelligence (AI) systems using
circuits and system architectures that are based on quantum-tunneling dynamic-
analog-memory (DAM) devices. In 2019, the energy required to train a top-of-the-
line AI system was more than the energy required to operate five US cars over
their entire lifetime. The energy requirements for training large-scale AI
systems have only gotten worse since to the point of being unsustainable. The
proposed research aims to develop novel learning hardware that will make the
training of ML and AI systems more energy sustainable. The project is also
developing software tools for training AI systems that can be disseminated and
adopted by the research community. The novel online learning and memory
consolidation algorithms that are being developed in this project will be
integrated with an openly shared, general-purpose neuromorphic cognitive
computing platform available through the Neuroscience Gateway (NSG) Portal at
the San Diego Supercomputer Center. In collaboration with Efabless Inc. the
project is supporting open-source development of mixed-signal integrated
circuits (IC) design tools that is being evaluated through in class-room
instruction and projects.&lt;br/&gt;&lt;br/&gt;The technical activities of this
research project are based on an ultra-energy-efficient synaptic element called
Fowler-Nordheim Dynamic Analog Memory (FN-DAM) that can be easily fabricated on
a standard integrated circuits process. The memory retention property of the
synaptic element has been previously shown to be adaptive and can be traded-off
with the energy required for synaptic updates. These FN-DAM properties are being
explored within the context of the following research objectives: 1)
Investigation into novel FN-DAM based neural network training and learning
algorithms and architecture: Mechanisms are being explored that can connect the
dynamics of FN-DAM array with the training formulations of standard
convolutional neural network. Efficient one-shot continual online learning
techniques are being investigated that exploit the dynamics of FN-DAM to improve
the speed and robustness of learning. The framework is being used to explore
connections between the FN-DAM based architectures with neuromorphic memory
architectures that combines episodic-memories with incremental learning
paradigms; 2) Investigation into novel FN-DAM based compute-in-memory and on-
chip learning architectures: Analog compute-in-memory learning architectures are
being investigated that integrate FN-DAM arrays with CMOS computing circuits and
on-chip adaptation and learning strategies; 3) Validation of the FN-DAM based
hardware-software co-design framework: The project is validating the co-design
framework for achieving high energy-efficiency in neural network training using
the NSF CISE Community Research Infrastructure (CRI) for large-scale
neuromorphic cognitive computing developed and maintained at University of
California at San Diego (UCSD). The project is also validating the energy-
efficiency improvements that can be achieved using prototypes that will be
fabricated in a standard integrated circuits process.&lt;br/&gt;&lt;br/&gt;This
award reflects NSF's statutory mission and has been deemed worthy of support
through evaluation using the Foundation's intellectual merit and broader impacts
review criteria.