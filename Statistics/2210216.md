
* 2210216
* Numerical Construction of Optimal Estimators Using Machine Learning Tools
* DMS,STATISTICS
* 09/15/2022,06/16/2022
* Alex Luedtke,WA,University of Washington
* Standard Grant
* Yong Zeng
* 08/31/2025
* USD 175,000.00

Optimal statistical procedures make maximal use of available data, making it
possible to answer pressing scientific questions more precisely and cost-
effectively. These procedures are traditionally derived via analytic
calculations that require expert knowledge achieved over many years of training.
In this project, the investigators will study two novel strategies for deriving
optimal procedures. Compared to existing approaches, these strategies require
more expertise in computational methods and less expertise in statistical
theory. As a result, this project will broaden the pool of researchers who can
develop optimal statistical procedures. If preliminary results support the
strong performance of the new methods, the investigators will incorporate them
into vaccine clinical trial data analyses. Through this project, the
investigators will engage undergraduates in statistical research and advance the
understanding of mentored graduate students.

The investigators will consider both local and global notions of optimality. The
first strategy will use novel representations of the efficient influence
function (EIF). The EIF is a critical ingredient for constructing asymptotically
efficient estimators, particularly in nonparametric and semiparametric models.
It also provides a principled approach to debias machine learning-based
estimators to recover valid statistical inference. Unfortunately, the
conventional approach for deriving the EIF involves advanced theory that is
often not taught in statistical curricula. Additionally, in some problems, the
EIF does not have a closed form, rendering its use difficult even for experts.
The investigators will derive a novel representation of the EIF that lends
itself to computerization and study how it can be used to derive novel
asymptotically efficient estimators. The second strategy will use ideas from
deep reinforcement learning, as used recently to build self-learning game
playing algorithms with super-human performance, to adversarially learn
(globally and locally) minimax optimal statistical procedures with computational
tools. Except in simple cases, analytic calculations have thus far only been
successfully used to derive estimators that are asymptotically minimax optimal.
However, asymptotic optimality does not generally guarantee optimality in small
samples. Existing works on numerically learning minimax optimal estimators use a
Bayesian formulation of the minimax problem. This formulation results in
learning schemes that are too computationally prohibitive to be applicable to
most problems. This project will develop and study an alternative means to
construct these estimators that can readily leverage massively parallel
computing.

This award reflects NSF's statutory mission and has been deemed worthy of
support through evaluation using the Foundation's intellectual merit and broader
impacts review criteria.
