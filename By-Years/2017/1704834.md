* 1704834
* SHF: Medium: A Cloudless Universal Translator
* CSE,CCF
* 06/01/2017,03/31/2024
* Paul Whatmough, Harvard University
* Continuing Grant
* Danella Zhao
* 03/31/2024
* USD 1,150,000.00

This project explores the research foundations necessary to build a universal
language translator on a portable computing device for secure private use
without the need for reliance on cloud servers. Transformative developments in
both machine learning and computer hardware design have made this exciting
challenge feasible. The project will nurture a true bidirectional co-design
process between researchers in both fields. The broader impacts of the project
include: 1) the practical applications of widely available language translation
technology, and 2) the training of graduate engineers who have specialization in
machine learning as well as hardware and circuit design, skills in broad demand
in US industry.&lt;br/&gt;&lt;br/&gt;The problem of developing hardware to fit
deep learning models is not simply one of fitting current machine learning
models on current circuit technology, as the models are much too large, too
slow, and too energy-hungry. This project will need to develop novel machine
learning techniques that take these factors into account. Machine learning
researchers mostly optimize for accuracy; however, the project goal will require
considering trade-offs on model size, speed, and computation. Conversely, the
hardware design will have to consider and exploit the unique properties of the
neural models, such as high-tolerance to certain types of noise, repeated
computational structure, and non-linear interactions. The research approach
includes three major areas for interaction: model compression, approximation in
architecture, and training for unreliable hardware. Succeeding in these goals
will be necessary to build a successful on-device system.