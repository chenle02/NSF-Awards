* 1464128
* CRII: III: Learning to Extract Events from Knowledge Base Revisions
* CSE,IIS
* 09/01/2015,08/31/2018
* Alan Ritter, Ohio State University
* Standard Grant
* Maria Zemankova
* 08/31/2018
* USD 151,299.00

Encyclopedic knowledge bases (KBs) such as Wikipedia and Freebase form the
underlying intelligence behind Google's Knowledge Graph, Facebook's Graph
Search, IBM's Watson and more. These broad-coverage databases contain facts
about entities, for example a person's employer or a city's mayor. KBs should
not simply be viewed as static snapshots, however, as we live in a constantly
changing world. For example, an election event can change the Leader of a
country, or a divorce/wedding can change the Spouse of a person. Today's
knowledge bases rely on human editors to stay up-to-date; this works for
prominent entities, such as celebrities or politicians, but manual editing will
not scale to tracking the huge number of concepts covered by these massive KBs.
The project will therefore investigate methods to continuously track real-time
text streams, including news and social media, and automatically update concepts
in a KB, as soon as new information becomes available. This will enable new
kinds of intelligent systems that constantly read all the text that is publicly
written each day, and maintain a detailed up-to-the-minute knowledge base
describing the current state of the world. The expected results in weakly
supervised information extraction techniques are expected to have a broad range
of applications, including detecting cyber security events discussed on Twitter.
The project will provide research training and educational experience for
students at Ohio State University and beyond, as the research outcomes will be
used in developing an open-source toolkit for weakly supervised information
extraction that will be widely distributed.

When important events occur, KB contributors often edit properties of affected
entities in near-real-time, for instance on Wikipedia. At the same time, many
people discuss these events on social media and in the news. Because the set of
events that alter properties of KB entities is large and not fixed in advance,
this project will investigate, implement and evaluate new models for learning
text extractors from KB revisions. The project will conduct experiments learning
extractors for news and Twitter using Wikipedia infobox edits as distant
supervision. Rather than making the closed world assumption, which is common in
previous work, the proposed methods will regularize the label distribution over
events that do not match knowledge revisions towards a user-provided
expectation. It is expected that the results of this research will help to
address the problem of false positives due to events that are not reflected in
the revision history. The approach's ability to automatically propose Wikipedia
infobox edits in real-time will be tested as public knowledge of an event
becomes available. Previous studies on weakly supervised event extraction have
mostly been conducted in limited domains. In contrast, this work aims to scale
up while simultaneously grounding events mentioned in text to revisions of an
entity's properties in a knowledge base. The project web site
(http://aritter.github.io/crii/) will include information on the project, links
to publications, software and datasets produced as a result of this research.