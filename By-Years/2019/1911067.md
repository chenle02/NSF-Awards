* 1911067
* FET: Small: Heterogeneous Learning Architectures and Training Algorithms for Hardware Accelerated Deep Spiking Neural Computation
* CSE,CCF
* 10/01/2019,11/30/2019
* Peng Li, Texas A&M Engineering Experiment Station
* Standard Grant
* Sankar Basu
* 11/30/2019
* USD 499,337.00

This project aims to address the present performance and energy efficiency
crisis in computing across broad areas of data-driven applications by developing
energy-efficient new spiking neural architectures, training algorithms, and
hardware computing devices. Inspirations from biological brains will be taken to
support the development of algorithms and hardware systems to close the widening
gap between the supply and demand of computing power. The outcomes from this
project will be strongly interdisciplinary and are expected to stimulate
technical advancements in machine learning and bridge between neural networks,
neuroscience, and hardware engineering. The research will provide rich training
and educational opportunities to students. Research participation from
undergraduate students and underrepresented groups will be promoted through
various outreach programs. The results of this project will be disseminated in
broad research and industrial communities and integrated into the graduate-level
curriculum. Research collaboration with industry will be sought to guide this
work toward addressing real-world challenges and provide mentoring and training
of students in the industrial setting. &lt;br/&gt;&lt;br/&gt;Brain-inspired
models of computation and hardware computing systems hold the promise of
delivering the amount of computing power required in processing increasingly
large volumes of data in the post Moore's Law era, without a correspondingly
high energy cost. This project will focus on improving the performances of
spiking neural models for real-life learning tasks by addressing two pressing
inter-dependent research roadblocks: lack of computationally powerful learning
architectures, and lack of practical algorithms that can effectively train
complex spiking neural models. Synergies between neuroscience and deep learning
will be explored to develop heterogeneous deep spiking neural architectures and
learning algorithms to address the corresponding training bottlenecks. Efficient
spiking neural processors will be demonstrated on reconfigurable computing
devices.&lt;br/&gt;&lt;br/&gt;This award reflects NSF's statutory mission and
has been deemed worthy of support through evaluation using the Foundation's
intellectual merit and broader impacts review criteria.