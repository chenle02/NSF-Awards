* 1844724
* CAREER: Understanding visual learning with self-supervised neural network models
* CSE,IIS
* 09/01/2019,08/31/2024
* Daniel Yamins, Stanford University
* Continuing Grant
* Kenneth Whang
* 08/31/2024
* USD 600,000.00

A central problem in artificial intelligence today is that machine learning
algorithms often require supervised training with huge amounts of hand-curated
data. As a result, such algorithms are largely limited in scope to domains where
well-funded organizations can build massive, expertly-annotated, and typically
proprietary, labelled datasets. In contrast, real biological systems such as
human infants learn much more efficiently, combining a small amount of explicit
supervision with powerful -- but not fully understood -- mechanisms of self-
supervision. This proposal seeks to build biologically-inspired general-purpose
self-supervised systems that can learn without needing to be spoon-fed millions
of labeled examples. &lt;br/&gt;&lt;br/&gt;The basic strategy to achieve this
goal will be to develop and refine techniques in the emerging field of
unsupervised deep learning, in which neural networks train themselves to capture
the subtle statistical patterns present in their sensory surroundings. These
networks will be augmented to operate as agents in a rich interactive physical
domain, where they will seek out challenging but ultimately solvable self-
supervised "goals" that will teach them to flexibly represent and respond to
their environment. If successful, such systems will have the ability to use the
wealth of unlabeled data that is ubiquitously available in the physical world.
The proposal also seeks to use these algorithmic ideas as hypotheses for
quantitative models of learning in real biological systems. Using recently
developed techniques from computational neuroscience, the neural networks will
be compared to neural and behavioral data collected using a wide spectrum of
experimental paradigms. It will then be determined which self-supervised neural
network learning models best capture the empirical data -- and equally
importantly, where the most glaring mismatches between experiment and
computational models lie. Quantifying these model-data comparisons will in turn
allow for feedback to build better neural network algorithms. The ultimate goal
of this work is to set up a tight loop between experimental observation and
computational algorithm development, accelerating progress both in artificial
intelligence and neuroscience.&lt;br/&gt;&lt;br/&gt;This award reflects NSF's
statutory mission and has been deemed worthy of support through evaluation using
the Foundation's intellectual merit and broader impacts review criteria.