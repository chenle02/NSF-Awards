* 2308699
* Deep transformers for integrating protein sequence, structure and interaction data to predict function
* BIO,DBI
* 06/01/2023,05/31/2026
* Jianlin Cheng, University of Missouri-Columbia
* Continuing Grant
* Jennifer Weller
* 05/31/2026
* USD 204,957.00

Proteins are fundamental macromolecules in the living systems. The knowledge
about the function of proteins is important for biological research and
technology development. However, the function of most proteins is still unknown.
To fill the gap, this project aims to develop deep learning methods, one of the
most powerful artificial intelligence (AI) technologies, to integrate multiple
sources of protein data such as protein sequences, structures, and interaction
to accurately predict protein function. The methods will advance the state of
the art of protein function prediction and can be broadly applied in many
domains such as life science research, biotechnology development, agriculture,
and healthcare. The project will provide unique interdisciplinary research
opportunities to train students at multiple levels including under-represented
minority students with diverse backgrounds to apply AI to address fundamental
scientific and technological problems.

The project will develop deep transformer models based on self-attention to
integrate protein sequence, structure, and interaction data to significantly
advance the prediction of both protein-level function and amino acid-level
function. Specifically, it aims to achieve three objectives: (1) develop 1D and
3D transformers to predict protein function from multiple sequence alignments
and structures; (2) develop 2D graph transformers to predict protein function
from protein-protein interactions and integrate them with sequences and
structures; and (3) implement transformers as user-friendly, accurate, robust
open-source protein function prediction tools for the community. Cutting-edge
deep transformer models based on the self-attention mechanism will be developed
to integrate protein sequence, structure, and interaction data to predict
protein function for the first time. 1D sequence-based transformer, 2D graph
transformer, and 3D-equivariant graph transformer can extract amino acid
conservation and long-range co-evolutionary signals in multiple sequence
alignments, long-range interactions in protein-protein networks, and rotation-
and translation-invariant/equivariant properties of protein structures better
than the existing deep learning methods based on traditional convolutional and
recurrent mechanisms. Predicting both overall protein-level function terms and
residue-level function sites via multi-task learning and novel deep learning
architectures can leverage the compliment of the two prediction tasks to provide
more accurate, more complete, and more interpretable function prediction. The
project will deliver user-friendly open-source tools for the community to
accurately predict function from sequence, structure, and interaction data,
which will help reduce the vast knowledge gap between protein sequence and
function. The open-source deep learning tools can be used to predict and study
protein function in many domains. The methods and tools will be leveraged to
train students at multiple levels and increase the diversity in scientific
research and education. The results of the project can be found at
https://calla.rnet.missouri.edu/cheng/nsf_protein_function.html

This award reflects NSF's statutory mission and has been deemed worthy of
support through evaluation using the Foundation's intellectual merit and broader
impacts review criteria.