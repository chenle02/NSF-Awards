* 1740225
* E2CDA: Type I: Collaborative Research: Energy-Efficient Artificial Intelligence with Binary RRAM and Analog Epitaxial Synaptic Arrays
* CSE,CCF
* 09/15/2017,08/31/2021
* Jae-sun Seo, Arizona State University
* Continuing Grant
* Sankar Basu
* 08/31/2021
* USD 579,066.00

In recent years, deep learning and artificial neural networks have been very
successful in large-scale recognition and classification tasks, some even
surpassing human-level accuracy. However, state-of-the-art deep learning
algorithms tend to present very large network models, which poses significant
challenges for hardware, especially for memory. Emerging resistive devices have
been proposed as an alternative solution for weight storage and parallel neural
computing, but severe limitations still exist for applying resistive random
access memories (RRAMs) for practical large-scale neural computing. This
proposal targets on addressing limitations in resistive device based neural
computing through novel device engineering, new bitcell designs, new neuron
circuits, energy-aware architecture, and a new circuit-level benchmark
simulator. A successful completion of this research is likely to have
consequences to our society, enabling wide adoption of dense and energy-
efficient intelligent hardware to power-/area-constrained local mobile/wearable
devices. Furthermore, a self-learning chip that learns in near real-time and
consumes very low-power can be integrated in smart biomedical devices,
personalizing healthcare. This project will have a strong effort on integrating
the research outcomes with education and outreach through summer outreach
programs for high school students, undergraduate/graduate student training, and
organization of tutorials and workshops at conferences for knowledge
dissemination.&lt;br/&gt;&lt;br/&gt;The proposal will perform innovative and
interdisciplinary research to address many limitations in today?s resistive
device based neural computing and make a leap progress towards energy-efficient
intelligent computing. Severe limitations of applying resistive random access
memories (RRAMs) for practical large-scale neural computing include: (1) device-
level non-idealities, e.g., non-linearity, variability, selector, and endurance,
(2) inefficiency in representing negative weights and neurons, and (3) limited
demonstration on simpler networks, instead of cutting-edge convolutional and
recurrent neural networks. To address these limitations, novel technologies from
devices to architectures will be investigated. First, new bitcell circuits will
be designed for today's binary resistive devices, efficiently mapping XNOR
functionality with (+1, -1) weights and neurons. Second, a novel epitaxial
resistive device (EpiRAM) that exhibits many idealistic properties will be
investigated, including linear programming for analog weights, suppressed
variability, self-selectivity, and high endurance. Third, new neuron circuits
will be explored for integration with new resistive devices for
feedforward/feedback deep neural networks. Finally, new data-mapping techniques
that efficiently map state-of-the-art deep neural networks onto the hardware
framework with RRAM arrays will be developed, and the overall energy-efficiency
will be verified with a new benchmark simulator ?NeuroSim?. With vertical
innovations across material, device, circuit and architecture, tremendous
potential and research needs will be pursued towards energy-efficient artificial
intelligence in ubiquitous resource-constrained hardware systems.