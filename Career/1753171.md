
* 1753171
* CAREER: Calibrating Regularization for Enhanced Statistical Inference
* DMS,STATISTICS
* 07/01/2018,10/21/2020
* Daniel McDonald,IN,Indiana University
* Continuing Grant
* Pena Edsel
* 06/30/2020
* USD 127,696.00

Volumes of data that seemed unimaginable only a decade ago are now ubiquitous in
scientific research. Investment decisions are based on prices, updated every
microsecond, for thousands of securities; atmospheric scientists use
multiresolution satellite images to understand climate change; and internet
companies exploit massive music collections to infer trends in tastes and
preferences. Rigorous analysis of these large datasets requires a balance
between computational constraints and statistical performance, and
operationalizing such tradeoffs involves a combination of algorithmic and
design-based approximations. For example, decreasing the resolution of an image
or subsampling high-throughput data enables faster computations but removes
potentially valuable information. At the same time, elaborate explanations of
the scientific process are credible because they account for real-world
complexity, but estimating complex models requires both more data and larger
computers. The proposed work investigates the viability of modern statistical
and machine learning methodologies for answering applied scientific questions.
The project will create new algorithms and open-source software for combining
computational approximations with regularization for analyzing large datasets as
well as providing theoretical justification for their statistical properties. A
more complete picture of the interplay between statistical regularization,
computational approximation, and scientific parsimony will enable fundamental
scientific advancement. The PI will employ the methodologies developed in this
project to facilitate novel science with large datasets in climate science,
biology, music analysis, astronomy, economics, and chemistry. Furthermore, the
PI will carefully integrate the research aims with educational and outreach
objectives to engage elementary and high school music students, introducing them
to modern statistics and computer science, as well as integrating
underrepresented populations in research.

Computational tractability and statistical efficiency for large datasets
necessitate approximation or regularization, either of which heuristically
balances fidelity to the data with scientific goals like parsimony, smoothness,
sparsity, or interpretability. The PI seeks to elucidate the dual roles of
regularization and approximation as tools for better scientific understanding.
Current research in computer science has focused on improving algorithms so as
to enable computation with minimal approximation. Meanwhile, statisticians have
developed regularization techniques in order to take advantage of simple
structures - graph topologies, sparse linear models, smooth functions - that, if
representative of the truth, will improve inference and prediction. The
challenge is to understand the consequences of this coupling for scientific
interpretability. The PI seeks to address two important issues (1) how do we
select tuning parameters when computations are at a premium? and (2) how does
the accuracy and stability of scientific conclusions relate to the
approximation/regularization methods used to obtain those conclusions? This
project seeks to enable fundamental scientific progress by understanding the
connections between computational approximations and statistical regularization,
thereby facilitating improved inferences. The PI will fill this gap by deriving
practical algorithms with accompanying theoretical justification under more
reasonable statistical assumptions. These tools will be tightly coupled with
applications in neuroscience, genetics, atmospheric science, and music.

This award reflects NSF's statutory mission and has been deemed worthy of
support through evaluation using the Foundation's intellectual merit and broader
impacts review criteria.
