* 9216975
* Lossy Data Compression
* NONE,NONE
* 05/15/1993,10/31/1996
* Toby Berger, Cornell University
* Continuing grant
* Thomas E. Fuja
* 10/31/1996
* USD 405,010.00

This research addresses practical and theoretical issues encountered in the
lossy compression of information. The problem of successive refinement, or
progressive transmission, concerns efficient approaches to incremental
specification of data produced by an information source in order to render it in
more detail at each of several encoding stages. A prime theoretical issue is the
determination of conditions under which this can be done in such a way the total
data rate required is no less than that needed by someone tasked with only the
final rendering in the sequence without the requirement of generating the
intermediate versions. Practical algorithms inspired by theoretical advances in
universal lossless data compression, especially the Lempe-Ziv and arithmetic
coding formalisms, have profoundly affected the way text and data files are
compressed for transmission and storage. Analogous theoretical issues and
practical techniques are sought for universal lossy data compression based on
searching the imprecisely described past for an approximate rather than an exact
match of the next segment of the yet-to-be-encoded data. the results should
significantly impact the way actual data sources, especially images and video,
get encoded in practice.