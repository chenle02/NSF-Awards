* 1652294
* CAREER:  In-Situ Compute Memories for Accelerating Data Parallel Applications
* CSE,CCF
* 02/01/2017,01/31/2024
* Reetuparna Das, Regents of the University of Michigan - Ann Arbor
* Continuing Grant
* Almadena Chtchelkanova
* 01/31/2024
* USD 573,554.00

As computing today is dominated by Big Data, there is a strong impetus for
specialization for this important domain. Performance of these data-centric
applications depends critically on efficient access and processing of data.
These applications tend to be highly data-parallel and deal with large amounts.
Recent studies show that by the year 2020, data production from individuals and
corporations is expected to grow to 73.5 zetabytes, a 4.4× increase from the
year 2015. In addition, they tend to expend disproportionately large fraction of
time and energy in moving data from storage to compute units, and in instruction
processing, when compared to the actual computation. This research seeks to
design specialized data-centric computing systems that dramatically reduce these
overheads. &lt;br/&gt; &lt;br/&gt;In a general-purpose computing system, the
majority of the aggregate die area (over 90%) is devoted for storing and
retrieving information at several levels in the memory hierarchy: on-chip
caches, main memory (DRAM), and non-volatile memory (NVM). The central vision of
this research is to create in-situ compute memories, which re-purpose the
elements used in these storage structures and transform them into active
computational units. In contrast to prior processing in memory approaches, which
augment logic outside the memory arrays, the underpinning principle behind in-
situ compute memories is to enable computation in-place within each memory
array, without transferring the data in or out of it. Such a transformation
could unlock massive data-parallel compute capabilities (up to 100×), and reduce
energy spent in data movement through various levels of memory hierarchy (up to
20×), thereby directly address the needs of data-centric applications. This work
develops in-situ compute memory technology, adapts the system software stack and
re-designs data-centric applications to take advantage of those memories.