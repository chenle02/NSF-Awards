* 1931352
* Elements: PASSPP: Provenance-Aware Scalable Seismic Data Processing with Portability
* CSE,OAC
* 11/01/2019,10/31/2022
* Yinzhi Wang, University of Texas at Austin
* Standard Grant
* Rob Beverly
* 10/31/2022
* USD 232,770.00

Most of what we know about the Earth's deep interior comes from the analysis of
ground motion data recording seismic waves produced by large earthquakes from
instruments around the entire planet. Seismologists have developed a long list
of methods to process modern seismic data to ?image? the Earth?s interior. Much
of our understanding of Earth's interior has been limited by the resolution of
the tools available to construct these "images". At present, the massive
increase in data volume has pushed the data processing infrastructure of
seismology to the breaking point. The inability to handle data of this scale has
imposed significant barrier to scientific discoveries, especially for the
smaller research groups with limited resources. Aiming to help improve this
situation, this project introduces a new data management and processing system
that is portable and scalable to run on any platforms from a personal computer
to a large-scale supercomputer. By leveraging and integrating sophisticated
tools from cloud computing and high-performance computing (HPC) communities, the
system can fill in the widening gap between the massive data made available by
data centers and the inadequacy of data management and processing capability
provided with current tools. Seamless discovery, access, transfer, and
processing of data and metadata outside of data centers will become possible for
the community. This project will also serve as the foundation to enable novel
research utilizing massive data to change the way we study the structure,
composition, and evolution of the Earth.&lt;br/&gt; &lt;br/&gt;This project aims
to develop a seismic data management and processing system that is composed of a
scalable parallel processing framework based on dataflow computation model, a
NoSQL database system centered on document store, and a container-based
virtualization environment. The scalable processing component will be based on
the iterative map-reduce model using Apache Spark to handle scheduling and flow
of data through systems of different scales. The provenance-aware data
management will be enabled by managing all data created during processing with
MongoDB, including process generated metadata, processed waveform data,
processing parameters, and the log outputs. All these core components as well as
a script to configure and deploy the framework on different systems will be
containerized with Singularity to provide portability. All these components
serve the two primary goals of the project: produce a system that will allow
common seismology algorithms to run effectively on modern HPC platforms; and
provide the means for seismologists with average experience in programming to
implement their own algorithms to extend the system. The system will serve as
the infrastructure to make data intensive research such as deep learning
possible for smaller research groups that usually don't have the necessary
manpower to manage and process massive data in a sustainable fashion. By
enabling the ability to process massive data collected by increasing number of
instruments, it will facilitate the transition of the field into data-intensive
paradigm of science discovery.&lt;br/&gt;&lt;br/&gt;This award reflects NSF's
statutory mission and has been deemed worthy of support through evaluation using
the Foundation's intellectual merit and broader impacts review criteria.