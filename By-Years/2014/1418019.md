* 1418019
* Investigating How to Enhance Scientific Argumentation through Automated Feedback in the Context of Two High School Earth Science Curriculum Units
* EHR,DRL
* 09/01/2014,08/31/2019
* Hee-Sun Lee, Educational Testing Service
* Continuing Grant
* Michael Ford
* 08/31/2019
* USD 2,495,604.00

With the current emphasis on learning science by actively engaging in the
practices of science, and the call for integration of instruction and
assessment; new resources, models, and technologies are being developed to
improve K-12 science learning. Student assessment has become a nationwide
educational priority due, in part, to the need for relevant and timely data that
inform teachers, administrators, researchers, and the public about how all
students perform and think while learning science. This project responds to the
need for technology-enhanced assessments that promote the critical practice of
scientific argumentation--making and explaining a claim from evidence about a
scientific question and critically evaluating sources of uncertainty in the
claim. It will investigate how to enhance this practice through automated
scoring and immediate feedback in the context of two high school curriculum
units--climate change and fresh-water availability--in schools with diverse
student populations. The project will apply advanced automated scoring tools to
students' written scientific arguments, provide individual students with
customized feedback, and teachers with class-level information to assist them
with improving scientific argumentation. The key outcome of this effort will be
a technology-supported assessment model of how to advance the understanding of
argumentation, and the use of multi-level feedback as a component of effective
teaching and learning. The project will strengthen the program's current set of
funded activities on assessment, focusing these efforts on students'
argumentation as a complex science practice.&lt;br/&gt;&lt;br/&gt;This design
and development research targets high school students (n=1,940) and teachers
(n=22) in up to 10 states over four years. The research questions are: (1) To
what extent can automated scoring tools, such as c-rater and c-rater-ML,
diagnose students' explanations and uncertainty articulations as compared to
human diagnosis?; (2) How should feedback be designed and delivered to help
students improve scientific argumentation?; (3) How do teachers use and interact
with class-level automated scores and feedback to support students' scientific
argumentation with real-data and models?; and (4) How do students perceive their
overall experience with the automated scores and immediate feedback when
learning core ideas in climate change and fresh-water availability topics
through scientific argumentation enhanced with modeling? In Years 1 and 2, plans
are to conduct feasibility studies to build automated scoring models and design
feedback for previously tested assessments for the two curriculum units. In Year
3, the project will implement design studies in order to identify effective
feedback through random assignment. In Year 4, a pilot study will investigate if
effective feedback should be offered with or without scores. The project will
employ a mixed-methods approach. Data-gathering strategies will include
classroom observations; screencast and log data of teachers' and students'
interaction with automated feedback; teachers' and students' surveys with
selected- and open-ended questions; and in-depth interviews with teachers and
students. All constructed-response explanations and uncertainty items will be
scored using automated scoring engines with fine-grained rubrics. Data analysis
strategies will include multiple criteria to evaluate the quality of automated
scores; descriptive statistical abalyses; analysis of variance to investigate
differences in outcomes from the designed studies' pre/posttests and embedded
assessments; analysis of covariance to investigate student learning
trajectories; two-level hierarchical linear modeling to study the clustering of
students within a class; and analysis of screencasts and log data.