* 1525754
* SHF: Small: Solving the Problems of Scalability and Portability while Maximizing Performance of Multiprecision Scalar and Vector Arithmetic on Clusters of GPUs
* CSE,CCF
* 07/15/2015,06/30/2019
* Charles Weems, University of Massachusetts Amherst
* Standard Grant
* Almadena Chtchelkanova
* 06/30/2019
* USD 400,000.00

This project extends the PI's prior research into achieving high performance for
multiprecision arithmetic utilizing commodity graphics processors (GPUs).
Multiprecision (MP) arithmetic has important applications in science,
engineering, and mathematics when computations require greater numerical
precision than standard computer systems support. It is also an important part
of cryptography used in secure internet communication. GPUs can accelerate MP
arithmetic by more than two orders of magnitude. However, achieving this
performance requires novel algorithms and software tools. The world-record
performance for exponentiation achieved under the prior grant will be extended
to include floating point vector arithmetic. A new code generation model will
enable handling a wider range of precisions across newer generations of graphics
processors. Support for clusters of GPUs to work together on larger problems,
and practical demonstrations of the effectiveness of MP library such as showing
how one GPU can offload decryption work from more than a hundred servers, with
higher levels of security than are currently in common use, is being developed.
&lt;br/&gt;&lt;br/&gt;Each generation of GPU architecture requires extensive
experimentation and reworking of multiprecision code to obtain a new optimum.
Yet the potential benefits of a portable and scalable package could be
transformational in certain application areas. This effort extends PI's prior
work to include floating point and vectors, and begin the transition to GPU
clusters. The result will be a publicly available multi-precision arithmetic
package and implementation toolset that enables the scientific community to
easily take full advantage of GPU scaling to obtain at least an order of
magnitude improvement in performance per dollar and performance per watt over
CPUs at the same technology step. The approach relies on a novel set of models
for GPU storage that provide a higher level of abstraction over which the code
generation tools can search for optimal combinations of algorithm,
register/memory layout, and kernel launch geometry for a given precision size
and GPU architectural generation to achieve maximum resource utilization.