* 1629898
* CI-SUSTAIN: Collaborative Research: Extending a Large Multimodal Corpus of Spontaneous Behavior for Automated Emotion Analysis
* CSE,CNS
* 09/01/2016,08/31/2022
* Lijun Yin, SUNY at Binghamton
* Standard Grant
* Balakrishnan Prabhakaran
* 08/31/2022
* USD 491,581.00

This project will extend and sustain a widely-used data infrastructure for
studying human emotion, hosted at the lead investigator's university and
available to the research community. The first two versions of the dataset (BP4D
and BP4D+) contain videos of people reacting to varied emotion-eliciting
situations, their self-reported emotion, and expert annotations of their facial
expression. Version 1, BP4D (n=41), has been used by over 100 research groups
and supported a successful community competition around recognizing emotion. The
second version (BP4D+) adds participants (n = 140), thermal imaging, and
measures of peripheral physiology. The current project greatly broadens and
extends this corpus to produce a new dataset (BP4D++) that enables deep-learning
approaches, increases generalizability, and builds research infrastructure and
community in computer and behavioral science. The collaborators will (1)
increase participant diversity; 2) add videos of pairs of people interacting to
the current mix of individual and interviewer-mediated video; 3) increase the
number of participants to meet the demands of recent advances in "big data"
approaches to machine learning; and 4) expand the size and scope of annotations
in the videos. They will also involve the community through an oversight and
coordinating consortium that includes researchers in computer vision,
biometrics, robotics, and cognitive and behavioral science. The consortium will
be composed of special interest groups that focus on various aspects of the
corpus, including groups responsible for completing the needed annotations,
generating meta-data, and expanding the database application scope. Having an
infrastructure to support emotion recognition research matters because computer
systems that interact with people (such as phone assistants or characters in
virtual reality environments) will be more useful if they react appropriately to
what people are doing, thinking, and feeling. &lt;br/&gt;&lt;br/&gt;The team
will triple the number of participants in the combined corpora to 540. They will
develop a dyadic interaction task and capture data from 100 interacting dyads to
support dynamic modeling of interpersonal influence across expressive behavior
and physiology, as well as analysis of emotional synchrony. They will increase
the density of facial annotations to about 15 million frames in total, allowing
the database to become sufficiently large to support deep-learning approaches to
multimodal emotion detection. These annotations will be accomplished through a
hybrid approach that combines expert coding using the Facial Action Coding
System, automated face analysis, and crowdsourcing with expert input from the
research community. Finally, the recorded data will be augmented with a wide
range of meta-data derived from 2D videos, 3D videos, thermal videos, and
physiological signals. To ensure the community is involved in sustaining the
infrastructure, in addition to the governance consortium described above, the
investigators will involve the community in jointly building both APIs that
allow adding meta-data and annotations and tools to support the submission and
evaluation of new recognition algorithms, then organizing community-wide
competitions using those tools. The research team will also reach out to new
research communities around health computing, biometrics, and affective
computing to widen the utility of the enhanced infrastructure, grow the
community of expert annotators through training workshops, and build an
educational community around the infrastructure that facilitates the development
and sharing of course materials that use it. Long-term, the infrastructure will
be funded through a combination of commercial licensing and support from the
lead university's system administration group.