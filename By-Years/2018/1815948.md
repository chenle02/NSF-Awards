* 1815948
* III: Small: Collaborative Research: Explainable Natural Language Inference
* CSE,IIS
* 09/01/2018,08/31/2023
* Mihai Surdeanu, University of Arizona
* Standard Grant
* Sylvia Spengler
* 08/31/2023
* USD 262,463.00

Natural language inference (NLI) can support decision-making using information
contained in natural language texts (e.g, detecting undiagnosed medical
conditions in medical records, finding alternate treatments from scientific
literature). This requires gathering facts extracted from text and reasoning
over them. Current automated solutions for NLI are largely incapable of
producing explanations for their inferences, but this capacity is essential for
users to trust their reasoning in domains such as scientific discovery and
medicine where the cost of making errors is high. This project develops natural
language inference methods that are both accurate and explainable. They are
accurate because they build on state-of-the-art deep learning frameworks which
use powerful, automatically learned, representations of text. They are
explainable because they aggregate information in units that can be represented
in both a human readable explanation and a machine-usable vector representation.
This project will advance methods in explainable natural language inference to
enable the application of automated inference methods in critical domains such
as medical knowledge extraction. The project will also evaluate the
explainability of the inference decisions in collaboration with domain
experts.&lt;br/&gt;&lt;br/&gt;This project reframes natural language inference
as the task of constructing and reasoning over explanations. In particular,
inference assembles smaller component facts into a graph (explanation graph)
that it reasons over to make decisions. In this view, generating explanations is
an integral part of the inference process and not a separate post-hoc mechanism.
The project has three main goals: (a) Develop multiagent reinforcement learning
models that can effectively and efficiently explore the space of explanation
graphs, (b) Develop deep learning based aggregation mechanisms that can prevent
inference from combining semantically incompatible evidence, and (c) Build a
continuum of hypergraph based text representations that combine discrete forms
of structured knowledge with their continuous embedding based representations.
The techniques will be evaluated on three application domains: complex question
answering, medical relation extraction, and clinical event detection from
medical records. The results of the project will be disseminated through the
project website, scholarly venues, and the software and datasets will be made
available to the public.&lt;br/&gt;&lt;br/&gt;This award reflects NSF's
statutory mission and has been deemed worthy of support through evaluation using
the Foundation's intellectual merit and broader impacts review criteria.