* 2118745
* Collaborative Research: PPoSS: LARGE: ScaleStuds: Foundations for Correctness Checkability and Performance Predictability of Systems at Scale
* CSE,CCF
* 10/01/2021,09/30/2026
* Yang Wang, Ohio State University
* Continuing Grant
* Alexander Jones
* 09/30/2026
* USD 374,793.00

In light of the limits of Moore's Law and Dennard scaling and the ever
increasing computing demand, the last decade has seen unprecedented deployment
scales; Google is known to run clusters with thousands of machines each, Apple
deploys a total of 100,000 database machines, and Netflix runs tens of database
clusters with 500 nodes each. This era of extreme-scale distributed systems has
given birth to a new class of faults, "scalability faults" -- complex latent
faults that are scale-dependent, whose symptoms surface in large-scale
deployments but not necessarily in small/medium-scale deployments. Many
fundamental research questions are not answerable today. On correctness: How to
detect bugs that only manifest under large scale through program analysis? How
to test and reproduce various dimensions of system scales efficiently on one
machine? How to prevent and fix scalability-related faults? On performance: How
to reason about software performance on various heterogeneous devices? How to
accurately predict performance of fine-grained tasks to reduce inaccuracies at
the aggregate level and project performance to future architectures? Finally, in
combination: How to answer all these questions for the larger connected
ecosystem -- not just the individual software and hardware components -- and to
eventually build future-generation systems that are reproducible and verifiable
by construction with respect to correctness and performance at scale?
&lt;br/&gt;&lt;br/&gt;The ScaleStuds project involves a team of ten researchers
to develop the foundations of correctness checkability (CC) and performance
predictability (PP) of systems at scale. The key principle of this project is to
"check large with large" -- check large-scale systems with a large fleet of
data, analysis, tests, learning, models, and proofs. The vision is to build an
ecosystem of distributed "CC+PP-certified" software-software and -hardware
interactions. The project is paving the vision one "floor" at a time, creating
composable building blocks ("the studs"). The project first builds new
mechanisms such as a scale-testing platform and a unified database of software
program properties and hardware performance profiles exposing clear APIs. These
studs then enable multi-dimensional automated scalability tests and program
analysis and performance learning and prediction at various levels of the
software/hardware stack. Ultimately all of these experiences are intended to
lead to correct and performant cross-layer/service interactions and future
design principles including reproducible- and verified-by-construction
development methods. The project novelties include the advancement of debugging,
testing, learning, and prediction methods to ensure correctness checkability and
performance predictability of extreme-scale systems and applications both on
classical hardware platforms and emerging ones; a unified data ecosystem of
software/hardware properties and profiles that facilitates automated analyses
via clear APIs; a multi-dimensional scale-testing framework that empowers the
development of new large-scale unit-tests and program analysis; detailed device
profiling and observation to enable large-scale performance learning/prediction
and deliver lessons for learning/predicting the behavior of other devices and
layers in an end-to-end hardware/software stack; and ultimately a clear
definition of CC+PP-certifiability for today's systems and future
verifiable/reproducible-by-construction development
methods.&lt;br/&gt;&lt;br/&gt;This award reflects NSF's statutory mission and
has been deemed worthy of support through evaluation using the Foundation's
intellectual merit and broader impacts review criteria.