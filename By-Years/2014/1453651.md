* 1453651
* CAREER: New Directions in Deep Representation Learning from Complex Multimodal Data
* CSE,IIS
* 09/01/2015,08/31/2023
* Honglak Lee, Regents of the University of Michigan - Ann Arbor
* Continuing Grant
* Sylvia Spengler
* 08/31/2023
* USD 488,617.00

The goal of deep learning is to learn an abstract representation of data with a
hierarchical and compositional structure. Deep learning methods can effectively
learn discriminative features from high-dimensional input data (e.g., for
classification), and have been successfully applied to many real-world problems,
such as image classification, speech recognition, and text modeling. Despite
these successes, there still remains a challenging open question: how can we
learn a robust deep representation that allows for holistic understanding and
high-level reasoning from complex data? This CAREER project aims to address this
question and is expected to result in novel deep architectures, graphical
models, and algorithmic advances for inference, learning, and optimization in
deep representation learning. The research outcomes will be disseminated through
publications, talks, and tutorials. In addition to advancing the state of the
art in deep learning and the many applications it entails, the project will
integrate research and education through 1) developing courses in machine
learning that include deep learning as a key topic; 2) mentoring significant
graduate and undergraduate research activities; and 3) reaching out to K-12
students via hosting demo sessions and mentoring for science fair/research
projects. &lt;br/&gt; &lt;br/&gt;This project investigates the following closely
interrelated and complementary thrusts: First, it develops deep learning
algorithms to disentangle factors of variation from complex data. This is done
by modeling higher-order interactions between multiple groups of latent
variables with a deep generative model (e.g., modeling face images via
interaction of latent factors that correspond to identity, viewpoint, and
emotion). In addition to better generalization, this approach is amenable to
high-level reasoning, such as making analogies. Modeling higher-order
interaction will be approached by learning a sub-manifold for each factor of
variation, where correspondence information is used for regularizing the latent
representation. The project will also develop weakly-supervised and semi-
supervised disentangling algorithms that automatically establish correspondences
without manual supervision. Second, the project develops deep representation
learning methods for structured prediction problems. Specifically, it will
develop a graphical model with deep representations that can model complex
dependencies between output variables. This framework can be also viewed as
data-driven modeling of higher-order prior on structured data, and can be used
for modeling higher-order conditional random fields that permit efficient
inference and learning. In addition, the project develops stochastic conditional
generative models for structured prediction problems that involve uncertainty
(i.e., one-to-many mappings). Third, the project develops novel deep learning
algorithms for constructing shared representations from multiple heterogeneous
input modalities, such as image and text, audio and video, and multiple sensor
streams. The main idea is to separately model conditional distribution of each
input modality given other modalities. This approach addresses the well-known
difficulty of modeling a joint distribution across heterogeneous multimodal
input, and provides a theoretical analysis on conditions under which the
approach can recover a consistent generative model. This formulation allows for
robust recognition and high-level reasoning from heterogeneous multimodal data.
Overall, these three thrusts are complementary and are expected to play
synergistic roles in tackling a broader range of AI problems and moving beyond
the current state-of-the-art in deep learning.