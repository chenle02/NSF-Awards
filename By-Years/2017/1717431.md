* 1717431
* RI:Small: Unsupervised Discriminatively-Generative Learning:
* CSE,IIS
* 08/01/2017,07/31/2022
* Zhuowen Tu, University of California-San Diego
* Standard Grant
* Jie Yang
* 07/31/2022
* USD 450,000.00

Great success has been achieved in obtaining powerful discriminative classifiers
via supervised training where humans provide manual annotations to the training
data. Unsupervised learning, in which the input data is not accompanied with
task-specific annotations, is of great importance since a large number of tasks
have no to little supervision. However, it still remains to be one of the most
difficult problems in machine learning. A typical unsupervised learning task
learns effective generative representations for highly structured data such as
images, videos, speech, and text. Existing generative models for unsupervised
learning are often constrained by their simplified assumptions, while existing
discriminative models for supervised learning are of limited generation
capabilities. This project develops a new introspective machine learning
framework that greatly enhances and expands the power of both generation and
discrimination for a single model. The outcome of the project, introspective
generative/discriminative learning, significantly improves the learning
capabilities of the existing algorithms by building stronger computational
models for a wide range of fields including computer vision, machine learning,
cognitive science, computational linguistics, and data mining.
&lt;br/&gt;&lt;br/&gt;This research investigates a new machine learning
framework, introspective generative/discriminative learning (IGDL), which
attains a single generator/discriminator capable of performing both generation
and classification. The IGDL generator is itself a discriminator, capable of
introspection --- being able to self-evaluate the difference between its
generated samples and the given training data. When followed by iterative
discriminative learning, desirable properties of modern discriminative
classifiers such as convolutional neural networks (CNN) can be directly
inherited by the IGDL generator. Moreover, the discriminator aspect of IGDL also
produces competitive results in fully supervised classification by using self-
generated new data (called pseudo-negatives) to enhance the classification
performance against adversarial samples. The training process of IGDL is carried
out using a two-step synthesis-by-classification algorithm via efficient
backpropagation. Effective stochastic gradient descent Monte Carlo sampling
processes for IGDL training are studied. Across three key areas in machine
learning including unsupervised, semi-supervised, and fully-supervised learning,
IGDL produces competitive results in a wide range of applications including
texture synthesis, object modeling, and image classification.