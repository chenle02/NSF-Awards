* 0964681
* RI: Medium: Learned Dynamic Prioritization
* CSE,IIS
* 08/15/2010,07/31/2014
* Jason Eisner, Johns Hopkins University
* Continuing Grant
* Tatiana Korelsky
* 07/31/2014
* USD 899,976.00

This project uses machine learning to accelerate the execution of a class of
computer programs relevant to AI. Given a program and a class of inputs, the new
methods automatically seek execution strategies that are fast while still
achieving a high level of accuracy.

The project focuses on the main inference algorithms that underlie statistical
AI: dynamic programming, belief propagation, Markov chain Monte Carlo, and
backtracking search. Each of these inference algorithms faces an enormous search
space, iteratively extending or refining its picture of this space. Each
algorithm must continually choose which computational step to take next.

The opportunity is to learn a strategy for making these choices. Some choices
are on the "critical path" and help the system find an accurate output, while
others lead mainly to wasted work. The learned strategy for evaluating choices
in context may itself be computationally intensive, so the method learns to
speed that up as well, within the same framework.

The project will disseminate software and will have broader impact on several
fields. The targeted algorithms are central to natural language processing,
speech processing, machine vision, computational biology, health informatics and
music processing. Their ability to form a coherent global analysis of a set of
observations is a hallmark of intelligence, and will enable artificial systems
that aid human understanding and performance. Speeding them up is critical as
researchers develop increasingly sophisticated statistical models. Furthermore,
the learning methodologies developed will be useful in other settings that
attempt to learn computational or behavioral strategies.