* 2047933
* CAREER: The Nature of Average-Case Computation
* CSE,CCF
* 02/15/2021,01/31/2026
* Pravesh Kothari, Carnegie-Mellon University
* Continuing Grant
* Tracy Kimbrel
* 01/31/2026
* USD 399,652.00

The recent surge in the applications of machine learning is powered by
algorithms that learn hidden patterns in large volumes of data. Designing faster
and more reliable data analysis algorithms is a key challenge in broadening the
scope of such applications. However, researchers have realized that the
classical framework of algorithm design is inadequate for this task. This is
because large data in almost every application is modeled using statistical
models as opposed to the standard worst-case model used in algorithm design.
Consequently, central challenges that involve an interplay between algorithms
and statistically generated data remain widely unresolved not just in machine
learning but also in statistical physics and cryptography. This project will
address this critical deficiency by building a principled theory of algorithm
design for statistical (aka average-case) data. The new paradigms explored in
this work will unify the currently fragmented set of approaches for studying
average-case computation. The curriculum development plan outlined in this
project will train the next generation of scientists in the algorithmic methods
tailor-made for problems in large scale statistical data analysis and
disseminate the modern paradigms for understanding computation to both graduate
and undergraduate students.&lt;br/&gt;&lt;br/&gt;Average-case complexity is a
central thrust in the theory of computation with a direct impact on potential
technological advances in machine learning and cryptography as well as basic
questions in statistical physics. Examples include training expressive
statistical models such as Gaussian mixture models and Sparse PCA to find
patterns in large data in machine learning, ascertaining the security of pseudo-
random generators in cryptography, and finding the lowest-energy states of spin-
glass systems in statistical physics. Our current understanding of such problems
is based on fragmented, domain-specific algorithmic schemes such as statistical
query methods and method of moments (in machine learning), belief propagation
(in statistical physics), and semidefinite programming hierarchies (in
computational complexity). This project is devoted to building a unified theory
of average-case computation that offers new tools to design better algorithms,
prove sharp lower-bounds, and allow rigorously transferring insights between
different specific frameworks. This investigation will build new bridges between
theoretical computer science and several adjacent areas including machine
learning, statistical physics, algebraic geometry, and probability. In addition,
it will further develop the burgeoning understanding of the sum-of-squares
semidefinite programming hierarchy, mixture models, and use of solution-space
geometry in solving random constraint satisfaction
problems.&lt;br/&gt;&lt;br/&gt;This award reflects NSF's statutory mission and
has been deemed worthy of support through evaluation using the Foundation's
intellectual merit and broader impacts review criteria.