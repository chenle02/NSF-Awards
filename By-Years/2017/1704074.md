* 1704074
* III: Medium: High-Performance Factorization Tools for Constrained and Hidden Tensor Models
* CSE,IIS
* 09/01/2017,08/31/2023
* George Karypis, University of Minnesota-Twin Cities
* Continuing Grant
* Sylvia Spengler
* 08/31/2023
* USD 1,200,000.00

Tensors generalize matrices to higher dimensions (called modes) and are designed
to model multi-way data. Tensor factorization algorithms analyze such multi-way
data to uncover relations between the different modes that can be used to both
gain insights and to predict unknown aspects of the underlying system/process.
For example, medical diagnosis and treatment records can be modeled via a four-
mode tensor whose modes correspond to patients, physicians, diagnosis, and
treatments and its factorization can provide insights on the co-occurrence of
medical conditions, treatment approaches, any treatment differences based on the
physician, and identify potential instances of medical fraud. This project's
research is designed to address current limitations of tensor analysis by
developing new theory and algorithms and high-performance scalable parallel
formulations of the various computational kernels used by these algorithms, and
a flexible open source software toolkit that can be used to perform constrained
and hidden tensor factorization of very large and sparse multi-way datasets. The
success of this project will allow researchers to leverage the power of multi-
way ``Big Data'' analysis to solve various problems in diverse application
domains such as healthcare, medical imaging, cybersecurity, social and
behavioral sciences, and e-commerce. At the same time, the project will provide
data science training to the students involved by combining cutting-edge data
and signal analytics, data mining, and high-performance
computing.&lt;br/&gt;&lt;br/&gt;Constrained matrix and tensor factorization
techniques are widely used for dimensionality reduction, clustering, and
estimation in machine learning, signal processing, and many other walks of
science and engineering. Unconstrained matrix and tensor factorization
algorithms are relatively mature, but constrained counterparts are lagging in
terms of speed, scalability, and flexibility. In many applications (e.g.,
medical imaging and recommender systems), instead of observing the actual
entries of a tensor, we observe a limited number of linear combinations (e.g.,
partial sums) of these entries and need to identify the tensor's latent factors
from these measurements. Being able to directly identify the latent factors from
linear measurements, which we refer to as hidden tensor factorization, has
important advantages in terms of complexity, memory footprint, and the ability
to handle very large data sets. Developing open source high-performance parallel
tools for constrained and hidden tensor factorization in both shared- and
distributed-memory systems will significantly enhance the ability to analyze
very large multi-way data. The research will evolve along two synergistic
thrusts. First, it will develop new theory and algorithms for constrained and
hidden tensor factorization by (i) building fast first-order (FFO) and fast
stochastic first-order (FSFO) constrained tensor decomposition algorithms that
strike favorable trade-offs between simplicity, scalability, and speed of
convergence, and (ii) tackling important identifiability and algorithmic issues
related to hidden tensor factorization. Second, it will undertake a multi-
pronged effort towards developing high-performance parallel formulations for the
computational kernels used in constrained and unconstrained tensor and hidden
tensor factorization and develop a high-performance tensor factorization
software toolbox. The release of the high-performance tensor factorization
toolbox will enable researchers and practitioners to scale up not only the size
of data but also the variety of constraints and types of data they can analyze.
The research will involve students that will be trained in data science,
combining cutting-edge signal and data analytics, data mining, and high-
performance computing.