* 1719205
* CIF:Small:Collaborative Research:Distributed Fog Computing for Non-Convex Big-Data Analytics
* CSE,CCF
* 09/01/2017,08/31/2021
* Gesualdo Scutari, Purdue University
* Standard Grant
* Phillip Regalia
* 08/31/2021
* USD 270,000.00

In our data-deluge era, massive chunks of information, perpetually collected by
pervasive sensors, are communicated and processed by distributed computational
architectures. To address emergent big-data computational issues, this project
embarks on an ambitious multidisciplinary research effort that aims at advancing
the state-of-the-art in-network/distributed big-data processing via a general
algorithmic framework for data analytics over massively distributed data sets.
The proposed algorithmic framework enables fully distributed and parallel big-
data analytics, for a variety of heterogeneous data sets over a wide range of
computational architectures. The developed research directions are beneficial
also to domains far beyond big-data analytics, such as signal processing,
machine learning, next-generation wireless communications, smart-city and smart-
grid networks. Research results are distributed through archival publications,
courses, undergraduate research opportunities, tutorials and conference
presentations.&lt;br/&gt;&lt;br/&gt;The developed scheme relies on a novel
convexification/decomposition technique which accommodates a rich class of non-
convex, unstructured and stochastic optimization tasks with non-separable
objective functions. Algorithms are designed for settings where data are
distributed across a large number of multi-core computational nodes, within a
network of arbitrary topology with (possibly) time-varying and even random
links. This new class of algorithms addresses shortcomings of current (non-
parallel and non-distributed) convexification techniques via (i) full control of
the degree of parallelism and distribution of the computation/signaling among
processors/network nodes, and (ii) by offering a plethora of convex
approximants, regularization terms, step-size rules, and communication
protocols. Designed for time-varying or even random network topologies, the
advocated framework demonstrates also another desirable attribute for
distributed computations: resiliency to (random) network failures.