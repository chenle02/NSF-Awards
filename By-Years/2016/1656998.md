* 1656998
* CRII: RI: Methods for Learning and Recovering Partially Embedded Logical Representations for Question Answering
* CSE,IIS
* 08/15/2017,07/31/2020
* Yoav Artzi, Cornell University
* Standard Grant
* D.  Langendoen
* 07/31/2020
* USD 191,000.00

Providing effective access to non-experts to the ever-increasing amount of
publicly available information is a challenging open problem. While search has
been the dominant mode of access, it does not support complex queries, and
provides a poor interface for natural language interaction with the growing
number of virtual and embodied agents. The goal of this project is to develop
representations and learning methods to enable systems that can respond
accurately to complex natural language questions. This work will advance the
field of natural language processing and have impact through the development of
algorithms, linguistic representations, and applications, including natural
language question answering interfaces for large knowledge bases.

A common approach to question answering, commonly known as semantic parsing, is
to map questions to expressive logical representations. This is a challenging
task that requires deriving the meaning of words, and combining them following
the compositional structure of the sentence. Despite increasing research
attention, building systems capable of such understanding requires significant
expertise, and state-of-the-art systems are limited by the coverage of existing
manually-curated knowledge bases, largely failing to benefit from unstructured
text. This project proposes an approach that will (a) significantly reduce the
engineering effort and expertise required to build question answering systems,
(b) generalize beyond curated data to learn to answer complex factoid questions
without a knowledge base, and (c) lay the foundations for a new general approach
to recover semantic meaning. The key to this approach is the integration of
logical and embedded representations of meaning. It is expected that mapping
sentences to representations that combine logical and embedded elements will
greatly reduce the amount of representation engineering required, enable complex
sentence-level reasoning, and allow effective learning from raw text without
access to a structured knowledge base.