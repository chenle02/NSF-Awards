* 9209432
* Implicit Learning of Sequential Inputs:  Developing a       Computational Model
* SBE,BCS
* 08/15/1992,07/31/1996
* Jeffrey Elman, University of California-San Diego
* Continuing Grant
* Jasmine Young
* 07/31/1996
* USD 169,996.00

Many of the most important things people learn are learned without explicit
instruction; often, people are not even aware that they are learning something.
One of the most dramatic examples of this phenomenon is language. Virtually all
speakers of a language learn to speak and understand before formal instruction
even begins, and the learning process seems to be largely unaffected by attempts
of adults to guide it. This style of learning contrasts with the more explicit
learning which also occurs when people consciously attempt to master a domain,
and which typically involves explicit formulation and testing of hypotheses.
Although considerable research has been carried out on explicit learning,
implicit learning has been investigated only recently. Furthermore, most of this
research has been experimental, with relatively little work focussed on
developing theories or computational models. The purpose of this research is to
develop both a theory and a testable computational model of implicit learning of
sequential behaviors. The research will use experimental techniques to ask the
following questions: Under what conditions is implicit learning triggered? Are
there domains in which implicit learning is more effective than explicit
learning? What are the constraints on the types of things which can be learned
with implicit learning? A second component of the research will focus on trying
to understand the possible mechanism for implicit learning by developing a
computational model. This model, based on an artificial neural network
architecture proposed by Elman, has been shown to have interesting properties
which resemble the behaviors of humans engaged in implicit learning tasks.
Elaboration of this model should allow for better understanding of the
circumstances which facilitate learning of domains such as language and might
make it possible to structure training environments in order to maximize
learning. This work should also provide a foundation for the construction of
machine-based systems for learning domains currently only well-mastered by
humans.