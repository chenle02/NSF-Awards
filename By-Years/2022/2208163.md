* 2208163
* New Algorithms for Markov Decision Processes and Reinforcement Learning
* MPS,DMS
* 09/01/2022,08/31/2025
* Lexing Ying, Stanford University
* Continuing Grant
* Stacey Levine
* 08/31/2025
* USD 177,701.00

Markov decision processes and reinforcement learning have had significant recent
success in applications, ranging from outperforming humans in Atari games to
AlphaFold overshadowing competing methods in predicting protein folding. This
success results from several fundamental developments, including deep neural
networks providing a powerful mechanism for representing high dimensional
functions, unprecedented computing power provided by graphical processing units
and tensor processing units, and the development of novel algorithms for both
prediction and control. However, there are still many challenges in applying
these recent techniques to mission-critical applications in health, social and
economic planning, and defense. This project aims to develop and analyze novel
algorithms for Markov decision processes and reinforcement learning with the
intention of making these approaches more broadly applicable. Educational
impacts include postdoctoral and graduate student training, as well as
undergraduate course development centered around machine learning.
&lt;br/&gt;&lt;br/&gt;This project involves the development of a unified
framework for Markov decision processes based on linear programming, where the
primal, dual, and primal-dual problems are studied for both the regularized and
non-regularized cases. Existing algorithms based on Markov decision processes
will then be connected to this unified framework. For the tabular setting, a
quasi-Newton type policy gradient algorithm will be developed for general
entropic regularizers. For the primal-dual problem, a rapidly converging
gradient ascent descent algorithm based on a strictly convexified formulation
with a non-standard preconditioning metric will be developed. The nonlinear
approximation setting will be addressed by variational actor-critic algorithms
that are stable and converge at least to a local minimum. Finally, to address
the double sampling issue, new algorithms based on the borrowing-from-the-future
idea will be developed to significantly reduce the
bias.&lt;br/&gt;&lt;br/&gt;This award reflects NSF's statutory mission and has
been deemed worthy of support through evaluation using the Foundation's
intellectual merit and broader impacts review criteria.