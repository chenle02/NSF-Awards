* 2203399
* CCF: SHF: Small: Transformer synthesis
* CSE,CCF
* 02/01/2022,01/31/2025
* Niraj Jha, Princeton University
* Continuing Grant
* Sankar Basu
* 01/31/2025
* USD 394,332.00

Just within four years of being first proposed, transformers have had a dramatic
impact on the natural language processing (NLP) field and are also beginning to
have an impact on other fields, such as computer vision. This success has
largely been driven by large-scale pre-training datasets, increasing
computational power, and robust training techniques. However, a major challenge
that remains is efficient optimal transformer model synthesis for a specific
task and set of user requirements. However, this is not easy to do since the
design space of transformer models is vast. This project addresses this
challenge through the development of transformer-synthesis methodologies and
tools. Given the importance of transformers, such tools are likely to have a
transformative impact on many application areas. The research will be
disseminated to industry via tech transfer e.g., via open-source online
distribution of source codes, summer internships, and by leveraging PIs
involvement with local companies. Outreach and curriculum development plans will
also be undertaken within the context of the proposed
research.&lt;br/&gt;&lt;br/&gt;There is currently no universal framework that
can navigate the vast transformer hyperparameter design space. Previously
proposed transformer models are homogeneous in terms of data flow through the
network. Unfortunately, this leads to very suboptimal transformer architectures.
This project expands the transformer design space to incorporate heterogeneous
architectures that venture beyond self-attention by employing other operations
like convolutions and linear transforms. It will also explore novel projection
layers and positional encodings to make hidden sizes flexible across various
transformer layers. It will use a dense embedding to capture model similarity to
significantly enhance search efficiency. It will develop a heteroscedastic
surrogate model to further speed up search. It will include operations that
optimize long-range interactions for long input sequences. It will also explore
skipped connections and block-level grow-and-prune synthesis to improve
architectural search efficiency.&lt;br/&gt;&lt;br/&gt;This award reflects NSF's
statutory mission and has been deemed worthy of support through evaluation using
the Foundation's intellectual merit and broader impacts review criteria.