* 2007719
* RI: Small: Deep Variational Data Compression
* CSE,IIS
* 10/01/2020,09/30/2023
* Stephan Mandt, University of California-Irvine
* Standard Grant
* Rebecca Hwa
* 09/30/2023
* USD 425,000.00

The internet and the world’s IT systems could not exist without data
compression. From the efficient storage of large business databases to massive
datasets collected by the Large Hadron Collider to video streaming—compression
is a tool of fundamental importance that enables many of the systems our
societies have come to depend on. It is estimated that by 2021, compressed video
data alone will account for over 80% of internet traffic (an estimate made
before COVID-19). Any gains that can be made in video coding efficiency will
have a dramatic societal impact. Over the past few years, it has become clear
that neural networks can significantly improve classical compression methods in
terms of how they trade off data quality losses for file size. Besides this,
neural compression methods also have other benefits: they can be fine-tuned to
specific data modalities (e.g., medical images), do not show the common block-
coding visual artifacts, and can be ‘supervised’ to allocate more attention to
specific features of interest. This award contributes to better compression
algorithms by improving video coding, enabling faster data transfer between
machine learning systems, and improving the modularity of neural codec design,
potentially impacting a wide range of applications.&lt;br/&gt;&lt;br/&gt;This
project draws on deep latent variable modeling and promotes several new ideas
for neural data compression: (i) hierarchical generative video coding; (ii)
supervised compression, and (iii) plug and play compression of trained
generative models. Part (i) proposes to combine normalizing flows with
sequential variational autoencoders to predict future frames with higher
confidence and shorter expected code lengths. Part (ii) leverages the ability of
deep neural networks to be trained towards multiple tasks, such as data
reconstruction and classification. Part (iii) describes a fundamentally new
approach that decouples discretization from training, and that instead performs
discretization and entropy coding jointly. The algorithm takes posterior
uncertainties into account to allocate more bits to the features that are most
important to reconstruct a given input data point while assigning fewer bits to
features where some quantization error can be
tolerated.&lt;br/&gt;&lt;br/&gt;This award reflects NSF's statutory mission and
has been deemed worthy of support through evaluation using the Foundation's
intellectual merit and broader impacts review criteria.