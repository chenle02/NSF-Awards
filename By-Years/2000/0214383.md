* 0214383
* Learning Minimal Representations for Visual Navigation and Recognition II
* SBE,BCS
* 08/01/2003,07/31/2010
* William Warren, Brown University
* Continuing Grant
* Lawrence Gottlob
* 07/31/2010
* USD 426,799.00

Consider how you find your way to the grocery store or learn the layout of a new
mall, or how scientists might build a robot that can be dropped on Mars to
navigate its surface. People, animals, and robots must navigate complex
environments, but different strategies are applied in different situations. One
may get to the grocery store by dead reckoning like ants, following landmarks
like honeybees, or one can use a precise "memory map" of the environment.
Moreover, clever combinations of strategies can make it easier to find the way.
The present research effort specifically explores how these strategies are
integrated to allow robust visual navigation.&lt;br/&gt;&lt;br/&gt;With NSF
support, Dr. Michael Tarr and Dr. William Warren study how people learn the
layout of new environments, the geometry of the resulting spatial knowledge, and
how it is used to navigate. The uniqueness of their approach is to study actual
navigation behavior, as people actively walk through a computer-generated
virtual environment (the VENLab - see http://www.cog.brown.edu/Research/ven_lab/
). Participants wear a head-mounted virtual reality display and walk freely in a
40 x 40 ft area. Their movements are recorded by a tracking system in the
ceiling. After participants learn the layout, the environment can be
surreptitiously changed, and they must, in effect, find a new route to the
grocery store. By distorting the virtual world or changing the properties of
landmarks, these scientists determine the navigational strategies people use and
how they rely on routes, landmarks, and the geometry of space.&lt;br/&gt;