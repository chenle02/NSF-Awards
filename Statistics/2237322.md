
* 2237322
* CAREER: Towards Tight Guarantees of Markov Chain Sampling Algorithms in High Dimensional Statistical Inference
* DMS,STATISTICS
* 07/01/2023,01/20/2023
* Yuansi Chen,NC,Duke University
* Continuing Grant
* Yulia Gel
* 06/30/2028
* USD 84,817.00

Drawing samples from a distribution is a core computational challenge in fields
such as Bayesian statistics, machine learning, statistical physics, and many
other areas involving stochastic models. Among all methods, Markov Chain Monte
Carlo (MCMC) algorithms stand out as the most widely used class of sampling
algorithms with a broad range of applications, notably in high dimensional
Bayesian inference. While MCMC algorithms have been proposed, studied, and
implemented since the foundational work of Metropolis et al. in 1953, many
convergence properties of algorithms used in practice are not well understood.
Practitioners in Bayesian statistics are often faced with a series of key
challenges to be addressed rigorously: the choice of algorithm hyper-parameters,
the estimated computational cost and the choice of the best algorithm, etc. This
project focuses on developing theoretical guarantees of MCMC sampling algorithms
that arise in large-scale Bayesian statistical inference problems. The project
will also offer numerous interdisciplinary research training, outreach and
mentoring opportunities for the next generation of statisticians and data
scientists at all levels, from undergraduate to doctoral students.

This project will address three specific research problems centered around MCMC
algorithms in high dimensional inference. First, the project intends to
rigorously rank the efficiency of MCMC algorithms for sampling log-concave
distributions and to provide succinct non-asymptotic mini-max analysis of mixing
time. Log-concave distributions in sampling are as important as convex functions
in optimization, and one cannot expect to build a foundational theory basis
without determining the fundamental limits of sampling algorithms on log-concave
distributions. Widely-used algorithms such as Hamiltonian Monte Carlo, Gibbs
sampling and hit-and-run will be studied rigorously. Second, as concentration
inequalities constitute an essential component in understanding the efficiency
of MCMC sampling algorithms, the project will develop a fine-grained
understanding of concentration of high dimensional log-concave distributions via
new technical tools such as stochastic localization. Finally, the project will
unify the existing theoretical tools for studying discrete-state and continuous-
state sampling algorithms through localization schemes. The proposed research
aims to advance the field with a comprehensive understanding of MCMC sampling
algorithms and their optimal settings in both discrete and continuous cases. The
project will provide a wide range of interdisciplinary initiatives to enhance
professional development of undergraduate and graduate students in statistical
sciences.

This award reflects NSF's statutory mission and has been deemed worthy of
support through evaluation using the Foundation's intellectual merit and broader
impacts review criteria.
