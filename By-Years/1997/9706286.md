* 9706286
* Memory Prefetching
* CSE,CCF
* 07/01/1997,06/30/2001
* Andrew Pleszkun, University of Colorado at Boulder
* Standard Grant
* Yavuz A. Oruc
* 06/30/2001
* USD 450,000.00

Early computers fetched data from memory only after it had been requested by the
processor. When memory latency is high, this demand fetch policy also has high
latency. Many modern computers prefetch data from the memory before it is
requested, which can reduce the latency of memory as long as the prefetching is
accurate, and timely. This project is a preliminary investigation of a Markov
prefetcher, which uses a Markov model to predict subsequent memory references
from earlier references. The prefetcher acts as an interface between on-chip and
off-chip caches that predicts multiple references to the memory system and
prioritizes their delivery to the processor. Cycle-level simulations show that
use of a Markov prefetcher reduces the overall execution stalls due to memory
operations by about 54% while also reducing memory usage. This project is
extending the initial investigation to provide more realistic analysis of the
effectiveness of Markov prefetching, reduce implementation costs, and apply the
technique to multiprocessor systems. In addition, new aspects of prefetching are
being examined, including prediction of access methods (read or write),
prefetching between processors, and prefetching between different levels of the
memory hierarchy. The primary research method is cycle-level simulation.