* 1718944
* RI: Small: Integrative, Semantic-Aware, Speech-Driven Models for Believable Conversational Agents with Meaningful Behaviors
* CSE,IIS
* 09/01/2017,08/31/2022
* Carlos Busso, University of Texas at Dallas
* Standard Grant
* Tatiana Korelsky
* 08/31/2022
* USD 526,116.00

This project will analyze, model and synthesize human behaviors to create a
believable Conversational Agent (CA). A CA is a virtual agent that interacts
with a user, displaying human-like behaviors not only through speech but also
through facial expressions and head movements. Replicating or representing human
behavior includes generating gestures that are synchronized with speech, convey
appropriate meaning in the message, and respond to the behaviors displayed by
the user. An appealing approach to synthesize human-like behaviors is the use of
data-driven methods, which have the potential of capturing naturalistic
variations of the behaviors. Modeling the dependencies between speech and
gestures brings insights about verbal and nonverbal communication, underlying
the production and coordination mechanisms used during natural human
interactions. CAs can be used in a variety of health care applications, such as
helping hearing impaired individuals and teaching social skills to autistic
children. Tutoring systems that display human-like behaviors to communicate and
acknowledge active listening will engage better with the students, helping them
in their learning. The project promises a fertile ground for interdisciplinary
training of graduate and undergraduate students. The models will be evaluated
with an assistive agent (CA or embodied robot) interacting with UT Dallas
students, serving as a platform to reach out students from all majors,
especially woman and underrepresented minorities.&lt;br/&gt;&lt;br/&gt;The
project will take an integrative, cross-disciplinary approach to generate
believable and meaningful behaviors by exploring the intrinsic relation between
speech, head motion, and facial expressions, constrained by important aspects of
spoken language. The planned research leverages some of the latest developments
in the field of deep learning in an integrative fashion, pulling together
acoustic features and semantic language structure, to build models that are able
to account for the correlation between various facial and head movements. The
speech-driven approach will capture the variability of human behavior in a
manner that is not easily possible with rule-based approaches. Dialog acts and
emotions will be inferred and used to constrain the speech driven models,
capturing the relation between high-level conversational functions and facial
gestures. The project will offer novel, principled methods to generate behaviors
driven by synthesized speech, opening new application domain when only text is
available. The approach will capture the acoustic variability in synthesized
speech, while maintaining the temporal dependency between gestures and speech.
The project will also explore schemes to modify the behaviors of the user by
displaying carefully designed gestures generated with our data-driven framework.
By tracking the behaviors of the user, the system will provide appropriate
responses, closing the loop in the interaction.