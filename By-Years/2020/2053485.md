* 2053485
* Collaborative Research: Langevin Markov Chain Monte Carlo Methods for Machine Learning
* MPS,DMS
* 06/01/2021,05/31/2024
* Mert Gurbuzbalaban, Rutgers University Newark
* Standard Grant
* Pena Edsel
* 05/31/2024
* USD 180,009.00

The research in this project will focus on a particular class of algorithms for
machine learning and data science. In particular, the investigators consider the
large class of Markov Chain Monte Carlo (MCMC) methods which arise in several
contexts in machine learning and data science. The project will develop new
algorithms within the subclass called Langevin MCMC methods. These new
algorithms will be scalable to high dimensions and large datasets and will be
faster than traditional ones. The features of scalability and fast convergence
are important for use in Bayesian statistical inference as well as in non-convex
stochastic optimization methods for machine learning. The algorithms will allow
efficient training and calibration of predictive machine learning models from
large-scale data and have a direct impact on a broad range of data-driven
application areas from information technology to computer vision. Graduate
students will be trained and involved in research.&lt;br/&gt; &lt;br/&gt;In this
project, the PIs investigate a new class of algorithms within the class of
Langevin MCMC methods. These algorithms can be applied in three contexts of
machine learning and data science. First, they can be used for Bayesian
(learning) inference problems with high-dimensional models, where the objective
is to sample from a posterior distribution given a prior distribution on the
parameter space and the likelihood of the observed data. Second, they can be
used for solving stochastic non-convex optimization problems including the
challenging problems arising in deep learning. Third, they arise in modeling and
approximating workhorse algorithms in data science such as stochastic gradient
descent methods. By leveraging out the connections between stochastic gradient
algorithms and MCMC algorithms, the proposed approach results in a new class of
stochastic gradient algorithms called Hamiltonian Accelerated Stochastic
Gradient that can outperform existing methods in deep learning practice. A first
goal of the project is to study theoretical convergence properties of the
proposed algorithms further to fill out the current gap between theory and
practice, as well as to develop new scalable algorithms that can extend the
existing framework. A second goal is to investigate existing Langevin algorithms
further to provide non-asymptotic rigorous performance guarantees relevant to
machine learning and data science practice.&lt;br/&gt;&lt;br/&gt;This award
reflects NSF's statutory mission and has been deemed worthy of support through
evaluation using the Foundation's intellectual merit and broader impacts review
criteria.