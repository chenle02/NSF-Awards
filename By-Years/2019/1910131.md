* 1910131
* RI: SMALL: Inducing Answer Set Programs to Provide Accurate and Concise Explanation of Machine-learned Models
* CSE,IIS
* 07/15/2019,06/30/2024
* Gopal Gupta, University of Texas at Dallas
* Standard Grant
* Roger Mailler
* 06/30/2024
* USD 450,000.00

Artificial Intelligence (AI)/Machine Learning is gaining prominence as an
important technology that will have significant impact on our economy, industry,
society, and academia. A major problem with modern machine learning methods is
their inability to explain their decisions to human users. Statistical machine
learning methods produce models that are complex algebraic solutions to
optimization problems such as risk minimization or data likelihood maximization.
Lack of intuitive descriptions makes it hard for users to understand, verify or
trust the underlying rules that govern the model. Also, these methods cannot
produce a justification for a prediction they compute for a new data sample. As
a result, there is significant research interest in what is termed as
Explainable AI. This project will develop methods to capture the logic behind
machine learning models, making the models explainable to users. This will allow
users to improve the models and will enhance users' trust in these models.
&lt;br/&gt;&lt;br/&gt;Inductive Logic Programming (ILP) is an established
technique to find the rules underlying a machine-learned model that are
comprehensible to humans. The rules learned are represented as logic programs or
Horn clauses. However, due to lack of negation-as-failure, Horn clauses offer
limited expressiveness for representation and reasoning when the background
knowledge about the domain being studied is incomplete. Additionally, ILP learns
rules under the assumption that there are no exceptions to them. This results in
exceptions and noise in the data being indistinguishable from each other. Often,
the exceptions to the rules themselves follow a pattern, and these exceptions
can be (recursively) learned as a default theory. It is hypothesized that a
learned program that includes such a default theory describes the underlying
model more accurately. This project extends heuristics-based, scalable ILP
algorithms that learn default theories as answer set programs given background
knowledge as well as positive and negative examples. These answer-set programs
aim to capture the logic underlying a learned model to provide justifications
for its decisions and to improve users' trust in it, discover any biases in the
model, and comply with outside requirements such as governmental regulations.
The aim of this project is to advance the state-of-the-art in ILP research and
to contribute to the general area of machine learning and explainable AI.
Results of the project will be open-sourced, with the aim to enabling industries
that make use of machine learning to develop better understanding of and trust
in the learned models they use.&lt;br/&gt;&lt;br/&gt;This award reflects NSF's
statutory mission and has been deemed worthy of support through evaluation using
the Foundation's intellectual merit and broader impacts review criteria.