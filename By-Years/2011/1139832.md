* 1139832
* III:  EAGER:  Collaborative Research:  A Community Experiment Platform for Reproducibility and Generalizability
* CSE,IIS
* 04/01/2011,08/31/2013
* Juliana Freire, Polytechnic University of New York
* Standard Grant
* Maria Zemankova
* 08/31/2013
* USD 190,000.00

A hallmark of the scientific method has been that experiments should be
described in enough detail that they can be repeated and perhaps generalized.
This implies the possibility of repeating results on nominally equal
configurations and then generalizing the results by replaying them on new data
sets, and seeing how they vary with different parameters. In principle, this
should be easier for computational experiments than for natural science
experiments, because not only can computational processes be automated but also
computational systems do not suffer from the "biological variation" that plagues
the life sciences. Unfortunately, the state of the art falls far short of this
goal. Most computational experiments are specified only informally in papers,
where experimental results are briefly described in figure captions; the code
that produced the results is seldom available; and configuration parameters
change results in unforeseen ways. Because important scientific discoveries are
often the result of sequences of smaller, less significant steps, the ability to
publish results that are fully documented and reproducible is necessary for
advancing science. While concern about repeatability and generalizability cuts
across virtually all natural, computational, and social science fields, no
single field has identified this concern as a target of a research
effort.&lt;br/&gt;&lt;br/&gt;This collaborative project between the University
of Utah and New York University consists of tools and infrastructure that
supports the process of sharing, testing and re-using scientific experiments and
results by leveraging and extending the infrastructure provided by provenance-
enabled scientific workflow systems. The project explores three key research
questions: (1) How to package and publish compendia of scientific results that
are reproducible and generalizable. (2) What are appropriate algorithms and
interfaces for exploring, comparing, re-using the results or potentially
discovering better approaches for a given problem? 3) How to aid reviewers to
generate experiments that are most informative given a time/resource
limit.&lt;br/&gt;&lt;br/&gt;An expected result of this work is a software
infrastructure that allows authors to create workflows that encode the
computational processes that derive the results (including data used,
configuration parameters set, and underlying software), publish and connect
these to publications where the results are reported. Testers (or reviewers) can
repeat and validate results, ask questions anonymously, and modify experimental
conditions. Researchers, who want to build upon previous works, are able to
search, reproduce, compare and analyze experiments and results. The
infrastructure supports scientists, in many disciplines, to derive, publish and
share reproducible results. Results of this research, including developed
software will be available via the project web site (
http://www.vistrails.org/index.php/RepeatabilityCentral).