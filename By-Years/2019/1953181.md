* 1953181
* Interpolation Methods in Statistics and Machine Learning
* MPS,DMS
* 06/01/2020,05/31/2024
* Alexander Rakhlin, Massachusetts Institute of Technology
* Continuing Grant
* Pena Edsel
* 05/31/2024
* USD 200,000.00

One of the key tenets taught in courses on Statistics and Machine Learning is
that data interpolation (or, data memorization) inevitably leads to overfitting
and poor prediction performance. Yet, most of the modern large-scale models,
including over-parametrized neural networks, are routinely optimized to achieve
zero error on training data. The research objective of this project is to
challenge the common wisdom and develop theoretical and algorithmic foundations
for methods that interpolate the training data. &lt;br/&gt; &lt;br/&gt;The
project will focus on the statistical and computational aspects of interpolation
methods. Consistency and finite-sample bounds will be derived for regression and
classification methods in the interpolation regime, and information-theoretic
limits of interpolating rules will be developed. The project will also focus on
the computational aspects of interpolation. The PI aims to shed light on the
relative advantages and disadvantages of over-parametrized models that have
capacity to perfectly fit the data.&lt;br/&gt;&lt;br/&gt;This award reflects
NSF's statutory mission and has been deemed worthy of support through evaluation
using the Foundation's intellectual merit and broader impacts review criteria.