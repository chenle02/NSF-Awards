* 2238605
* CAREER: Modeling Spoken Language Without Parallel Text Annotations
* CSE,IIS
* 02/01/2023,01/31/2028
* David Harwath, University of Texas at Austin
* Continuing Grant
* Tatiana Korelsky
* 01/31/2028
* USD 113,102.00

Automatic speech recognition and understanding technology has been widely
adopted into personal digital assistants, automatic transcription of videos and
meetings, and many more applications. Building these systems requires massive
datasets of speech audio that is human-transcribed into text. When sufficient
data is available for a particular domain, modern models based on deep neural
networks are capable of highly accurate speech recognition and downstream
language understanding tasks. However, for the vast majority of the world's
7,000 languages and even more numerous dialects, large scale annotated datasets
simply do not exist, preventing speech technology from serving these languages
and their speakers. Inspired by the fact that humans learn to speak long before
they can read or write, this CAREER project explores a new paradigm for speech
processing that does not rely on transcribed speech. Instead, it develops new
models that are capable of learning spoken language directly from speech audio,
and applies these models to tasks including building speech recognizers without
transcribed speech and automatically translating speech from one language into
another. These advances fit within a larger movement in the research community
to dramatically reduce the cost and increase the availability of speech
recognition and understanding technology to many more languages and users than
are served today.&lt;br/&gt;&lt;br/&gt;This project leverages self-supervised
and multimodal learning approaches to automatically discover linguistic
structure (phones, words, phrases, etc.) in the raw speech signal which can be
treated as ``pseudo-text'' and used in place of conventional text for downstream
tasks. It develops new neural network layers for attention-based segmentation of
speech, applied in a hierarchical fashion to discover speech units at multiple
levels of abstraction. A second novel technique involves adding self-prediction
layers and training objectives to a model using the segmentation layers, where
the higher layers that would capture word-like structure attempt to predict the
tokenization of lower layers that capture sub-word structure. In this way, the
model can automatically learn a pronunciation lexicon that captures the
compositional relationship between the different tiers of discovered speech
units. The project applies these techniques to three downstream applications
that are steadily growing in importance in the speech field: unsupervised speech
recognition, textless speech-to-speech translation, and textless generation
speech for dialog and image captioning.&lt;br/&gt;&lt;br/&gt;This award reflects
NSF's statutory mission and has been deemed worthy of support through evaluation
using the Foundation's intellectual merit and broader impacts review criteria.