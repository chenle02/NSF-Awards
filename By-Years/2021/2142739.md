* 2142739
* CAREER: Language Technologies Against the Language of Social Discrimination
* CSE,IIS
* 09/01/2022,08/31/2027
* Yulia Tsvetkov, University of Washington
* Continuing Grant
* Tatiana Korelsky
* 08/31/2027
* USD 132,317.00

This award is funded in whole or in part under the American Rescue Plan Act of
2021 (Public Law 117-2).&lt;br/&gt;&lt;br/&gt;The exponential growth of online
social platforms provides an unprecedented source of equal opportunities for
accessing expert- and crowd-wisdom, for finding education, employment, and
friendships. One key root cause that can deeply impede these experiences is the
exposure to implicit social bias. The risk is high, since biases are pernicious
and pervasive, and it has been well established that language is a primary means
through which stereotypes and prejudice are communicated and perpetuated. This
project develops language technologies to detect and intervene in the language
of social discrimination—sexist, racist, homophobic microaggressions,
condescension, objectification, dehumanizing metaphors, and the like—which can
be unconscious and unintentional, but cause prolonged personal and professional
harms. The program opens up new research opportunities with implications to
natural language processing, machine learning, data science, and computational
social science. It develops new Web-scale algorithms to automatically detect
implicit and disguised toxicity, as well as hate speech and abusive language
online. Technologically, it develops new methods to surface and demote spurious
patterns in deep-learning models, and new techniques to interpret deep-learning
models, thereby opening new avenues to reliable and interpretable machine
learning. Successful completion of the program will pave the ground for a
paradigm shift in existing ways for monitoring civility in cyberspace, shielding
vulnerable populations from discrimination and aggression, and reducing the
mental load of platform moderators. Therefore, this project can benefit and
empower a dramatic number of individuals—representatives of disadvantaged groups
discriminated by gender, race, age, sexual orientation, ethnicity—who use social
media or AI technologies built upon user generated content. Finally, the
educational curriculum developed by this program will equip future technologists
with theoretical and practical tools for building ethical AI, and will
substantially promote diversity, equity and inclusion in STEM education, helping
to foster a new, more diverse generation of researchers entering AI.
&lt;br/&gt;&lt;br/&gt;The overarching goal of this CAREER project is to develop
lightly supervised, interpretable machine learning approaches—grounded in social
psychology and causal reasoning—to detect implicit social bias in written
discourse and narrative text. More specifically, the first phase of the project
develops algorithms and models for identifying and explaining gendered
microaggressions in short comments on social media, first unsupervisedly, then
with active learning, given limited supervision by trained annotators. It
provides transformative solutions to making existing overparameterized black-box
neural networks more robust and more interpretable. Since microaggressions are
often implicit, it also develops approaches to generate explanations to the
microaggression detector’s decisions. In the second phase, the project addresses
the challenging task of detecting biased framing about members of the LGBTQ
community in narrative domains of digital media and develops data analytic tools
by operationalizing, across languages, well-established social psychology
theories. The expected outcomes of this five-year program include new datasets,
algorithms, and models that provide people-centered text analytics, and pinpoint
and explain potentially biased framings, across languages, data domains, and
social contexts.&lt;br/&gt;&lt;br/&gt;This award reflects NSF's statutory
mission and has been deemed worthy of support through evaluation using the
Foundation's intellectual merit and broader impacts review criteria.