* 2226601
* Collaborative Research: Machine Learning for Student Reasoning during Challenging Concept Questions
* ENG,EEC
* 04/01/2023,03/31/2026
* Anna Rumshisky, University of Massachusetts Lowell
* Standard Grant
* Jumoke Ladeji-Osias
* 03/31/2026
* USD 171,743.00

Artificial Intelligence (AI), and more specifically, language models, have been
drastically changing how students and instructors think about learning and
assessment. While there are legitimate concerns about how the use of these tools
could be detrimental to learning, this research project aims to leverage
language models to better prepare engineering learners of the 21st Century. The
research project will use modern AI and machine learning (ML) tools to automate
analysis of student-written responses to challenging concept questions. These
qualitative questions are often used in large STEM classes to support active
learning pedagogies; they require minimum calculations and focus on the
application of underlying physical and chemical phenomena to various situations.
With previous NSF funding, we have developed the Concept Warehouse (NSF DUE
1023099, 1821439, 2135190), a classroom response system where students provide
written justifications to concept questions. Providing written justifications
targets development of reasoning and sense-making skills in students and can
also better prepare them for discussions with peers resulting in broader
effectiveness of active learning pedagogies. However, expository prose also
presents a daunting amount of information for instructors to process. In this
project, we will leverage recent advancements in machine learning tools and
natural language processing technologies to develop automated processes to
analyze student-written justifications to challenging concept
questions.&lt;br/&gt;&lt;br/&gt;This project will join engineering education
researchers at Tufts University and AI/ML researchers at the University of
Massachusetts Lowell. We will focus on the following research questions: (1)
Based on human coding, what ideas do students use in explaining challenging
concept questions in statics? How do these vary among challenging concept
questions studied? (2) How well can Transformer-based ML models replicate the
coding done by the human coders? For isomorphic question pairs, how well do ML
models trained on the first question’s explanations perform on the second
question? More generally, can ML-based coding based on one question be applied
successfully to code the data for other questions, and what are the limits to
this generalizability? We will complete three research tasks: (1) data
collection of written responses for the same concept questions from nine or more
engineering statics instructors at different institutions; (2) manual coding of
a subset of students’ written explanations, and (3) developing and evaluating ML
coding methods, followed by ML coding of the complete set of collected written
explanations. While the project focuses on engineering statics, it is expected
that findings will transfer to challenging questions in other engineering and
science topics. Ultimately, successful implementation of machine learning will
support learning and instruction of challenging concepts. Expected outcomes
include a developing understanding of advantages and disadvantages of different
ML approaches including their accuracy, determination of minimum data size
requirements to apply the algorithms, and the ability to transfer learning from
one question to isomorphic questions that require similar reasoning patterns.
For instructors, data generated can provide real-time information about the
different ways students are reasoning with examples of common cases. For
engineering education researchers, characterizing explanations in different
settings will support investigations of how student thinking relates to
instructional practices and environments.&lt;br/&gt;&lt;br/&gt;This award
reflects NSF's statutory mission and has been deemed worthy of support through
evaluation using the Foundation's intellectual merit and broader impacts review
criteria.