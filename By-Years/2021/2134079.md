* 2134079
* Collaborative Research: Towards a Theoretic Foundation for Optimal Deep Graph Learning
* MPS,DMS
* 01/01/2022,12/31/2024
* Hanghang Tong, University of Illinois at Urbana-Champaign
* Continuing Grant
* Yong Zeng
* 12/31/2024
* USD 350,000.00

Graph learning has become the cornerstone in numerous real-world applications,
such as social media mining, brain connectivity analysis, computational
epidemiology and financial fraud detection. Graph neural networks (GNNs for
short) represent an important and emerging family of deep graph learning models.
By producing a vector representation of graph elements, GNNs have largely
streamlined a multitude of graph learning problems. In the vast majority of the
existing works, they require a given graph, including its topology, the
associated attribute information and labels for (semi-)supervised learning
tasks, as part of the input of the corresponding learning model. Despite
tremendous progress being made, a theoretical foundation of optimal deep graph
learning is still missing, a gap that this project aims to fulfill. The outcomes
of this project have broader impacts on education and society. The results of
this project enrich the curriculum as well as summer outreach programs at
participating institutions, and are further disseminated to the community
through a variety of formats to create synergies and advance understandings of
different disciplines. This project benefits a variety of high-impact graph
learning based applications, including recommendation, power grid, neural
science, team science and management, and intelligent transportation
systems.&lt;br/&gt;&lt;br/&gt;This project examines the fundamental role of the
input data, including graph topology, attributes and optional labels, in graph
neural networks. There are three research thrusts in this project. The first
thrust seeks to understand how sensitive the GNNs model is with respect to the
input graph; how to quantify the uncertainty of the GNNs model; and how that
impacts the generalization performance of the GNNs model. The second thrust
develops algorithms to optimize the initially provided graph so as to maximally
boost the generalization performance of the given GNNs model. The third thrust
develops active learning methods based on deep reinforcement learning with
entropy regularization to optimally obtain the additional labels to further
improve the GNNs model. This project investigates new theoretic foundations in
terms of the sensitivity, the uncertainty and the generalization performance of
graph neural networks. It develops new algorithms for learning optimal graphs
and active GNNs with better efficacy whose fundamental limits, including sample
complexity, generalization error bound, optimality and convergence rate, are
well understood.&lt;br/&gt;&lt;br/&gt;This award reflects NSF's statutory
mission and has been deemed worthy of support through evaluation using the
Foundation's intellectual merit and broader impacts review criteria.