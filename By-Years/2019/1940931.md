* 1940931
* EAGER: Learning Language in Simulation for Real Robot Interaction
* CSE,IIS
* 12/01/2019,11/30/2021
* Donald Engel, University of Maryland Baltimore County
* Standard Grant
* Tatiana Korelsky
* 11/30/2021
* USD 219,516.00

While robots are rapidly becoming more capable and ubiquitous,
their&lt;br/&gt;utility is still severely limited by the inability of regular
users to&lt;br/&gt;customize their behaviors. This EArly Grant for Exploratory
Research (EAGER) &lt;br/&gt;will explore how examples of language, gaze, and
other communications can be collected from a&lt;br/&gt;virtual interaction with
a robot in order to learn how robots can&lt;br/&gt;interact better with end
users. Current robots' difficulty of use and&lt;br/&gt;inflexibility are major
factors preventing them from being more&lt;br/&gt;broadly available to
populations that might benefit, such as&lt;br/&gt;aging-in-place seniors. One
promising solution is to let users control&lt;br/&gt;and teach robots with
natural language, an intuitive and comfortable&lt;br/&gt;mechanism. This has led
to active research in the area of grounded&lt;br/&gt;language acquisition:
learning language that refers to and is informed&lt;br/&gt;by the physical
world. Given the complexity of robotic systems, there&lt;br/&gt;is growing
interest in approaches that take advantage of the latest in&lt;br/&gt;virtual
reality technology, which can lower the barrier of entry to&lt;br/&gt;this
research.&lt;br/&gt;&lt;br/&gt;This EAGER project develops infrastructure that
will lay the necessary&lt;br/&gt;groundwork for applying simulation-to-reality
approaches to natural&lt;br/&gt;language interactions with robots. This project
aims to bootstrap&lt;br/&gt;robots' learning to understand language, using a
combination of data&lt;br/&gt;collected in a high-fidelity virtual reality
environment with&lt;br/&gt;simulated robots and real-world testing on physical
robots. A person&lt;br/&gt;will interact with simulated robots in virtual
reality, and his or her&lt;br/&gt;actions and language will be recorded. By
integrating with existing&lt;br/&gt;robotics technology, this project will model
the connection between&lt;br/&gt;the language people use and the robot's
perceptions and actions.&lt;br/&gt;Natural language descriptions of what is
happening in simulation will&lt;br/&gt;be obtained and used to train a joint
model of language and simulated&lt;br/&gt;percepts as a way to learn grounded
language. The effectiveness of the&lt;br/&gt;framework and algorithms will be
measured on automatic&lt;br/&gt;prediction/generation tasks and transferability
of learned models to a&lt;br/&gt;real, physical robot. This work will serve as a
proof of concept for&lt;br/&gt;the value of combining robotics simulation with
human interaction, as&lt;br/&gt;well as providing interested researchers with
resources to bootstrap&lt;br/&gt;their own work.&lt;br/&gt;&lt;br/&gt;This award
reflects NSF's statutory mission and has been deemed worthy of support through
evaluation using the Foundation's intellectual merit and broader impacts review
criteria.