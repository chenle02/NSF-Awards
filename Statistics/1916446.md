
* 1916446
* Theory and algorithms for computational sufficiency
* DMS,STATISTICS
* 09/01/2019,07/30/2019
* Vincent Vu,OH,Ohio State University
* Standard Grant
* Pena Edsel
* 08/31/2022
* USD 120,000.00

The extraction of information from data is of fundamental importance. Central to
this problem is defining "information", determining what in the data is most
relevant and deciding which algorithms/procedures should be used. Technological
advances in data acquisition and storage have led to an over abundance of data,
which has only exacerbated the problem. The field of statistics has addressed
these issues since its inception; the traditional point of view relies on
stringent assumptions about the data and probabilistic models, but has ignored
computational aspects. This project will develop a new theory and algorithms
based on a concept called computational sufficiency. Rather than making
stringent assumptions about the data and models, this concept assumes that many
different and complementary procedures might be applied to the data. Then we
consider the relevant information in the data to be that which is sufficient for
obtaining the results of all the procedures under consideration. In this way,
the theory can (i) reveal hidden commonalities between procedures, (ii) identify
meaningful reductions of the data, (iii) provide results that are robust to
modeling assumptions, and (iv) allow us to exploit the reductions for efficient
computation.

Statistical sufficiency has been a foundational concept of mathematical
statistics since the early 20th century. Implicit in its definition is the
specification of a statistical model. However, actual data analysis does not
always begin with the specification of a model, and it may not even make
explicit use of a statistical model. The abundance of data and advances in
computing have made it easier for the data analyst to abandon a single
statistical model and instead consider multiple algorithmic models.
Computational sufficiency defines information in the context of a collection of
procedures that share a common input domain. The basic idea is very simple: we
wish to find functions of the data that contain sufficient information for
computing every procedure in the collection. In other words, what are the
reduced summaries of the data that are sufficient to produce the same answers as
would be obtained by applying the procedures to the whole data. This project
will broaden the applicability of this theory and its depth. The results will
lead to both theoretical insights and practical computational advances in data
analysis. In some cases, the theory will be able to provide a conceptual link
between seemingly unrelated methods and it will also provide a basis for
inferences that are less dependent on assumptions. The computational advances
provided by this research can lead to reduction of computational costs in
applying multiple, advanced procedures by an order of magnitude, thus
facilitating responsive and multifaceted analysis of large-scale data.

This award reflects NSF's statutory mission and has been deemed worthy of
support through evaluation using the Foundation's intellectual merit and broader
impacts review criteria.
