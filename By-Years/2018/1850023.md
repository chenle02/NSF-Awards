* 1850023
* CRII: RI: Bayesian Models for Fairness, and Fairness for Bayesian Models
* CSE,IIS
* 07/01/2019,06/30/2023
* James Foulds, University of Maryland Baltimore County
* Standard Grant
* Rebecca Hwa
* 06/30/2023
* USD 174,869.00

In our interconnected society, artificial intelligence (AI) and machine learning
(ML) systems have become ubiquitous. Every day, machine learning systems
influence our purchasing decisions, our navigation through virtual and physical
spaces, the friendships we make, and even the romantic relationships we form.
The decisions automated by these systems have increasingly important real-world
consequences, from credit scoring, to college admissions, to the prediction of
re-offending behavior in the criminal justice system, as is already being used
for bail and sentencing decisions across the United States of America. With the
growing impact of artificial intelligence and machine learning technologies on
our society, and their importance to the economic competitiveness and
technological leadership of the United States, it is imperative that we ensure
that these systems behave in a fair and trustworthy manner. Recent studies have
shown that data-driven AI and ML systems can in some cases exhibit unfair and
unjust behavior, for example due to biases hidden in the input data, or because
of flawed engineering decisions. This project develops a suite of tools for
modeling, measuring, and correcting unfair and discriminatory behavior in AI and
ML systems. The research focuses on simultaneously addressing algorithmic
discrimination that may occur across several overlapping dimensions, including
gender, race, national origin, sexual orientation, disability status, and
socioeconomic class. The novel AI techniques developed in this project address
the two main technical challenges which specifically arise in this context:
uncertainty in the measurement of fairness, and correlations in the
data.&lt;br/&gt;&lt;br/&gt;When ensuring AI fairness regarding multiple
protected dimensions such as gender and race, data sparsity rapidly becomes a
challenge as the number of dimensions, or the number of values per dimension,
increase. This data sparsity directly results in uncertainty in the measurement
of fairness. The project will leverage Bayesian inference, a branch of
statistics which specifically addresses uncertainty, to manage this issue.
Correlations between the protected (and other) attributes will be leveraged
using probabilistic graphical models, a class of machine learning models which
encode dependence relationships. Using a novel Bayesian definition of fairness
as a unifying framework, the project's contributions consist of three
interdependent tracks. The first track will focus on developing general modeling
techniques for the statistically efficient measurement of fairness, using latent
variable models to produce parsimonious representations, and hierarchical
modeling to achieve data efficiency. The second track develops adversarial
optimization algorithms to train machine learning algorithms to respect fairness
constraints when the data distribution is uncertain. In the third track, the
project will develop methods for ensuring fairness in Bayesian inference, which
can be used to prevent the inferences from reflecting negative stereotypes. The
methods will be validated with case studies on applications across a wide range
of data regimes, including modeling census income data, criminal justice
recidivism prediction, and social media analytics.&lt;br/&gt;&lt;br/&gt;This
award reflects NSF's statutory mission and has been deemed worthy of support
through evaluation using the Foundation's intellectual merit and broader impacts
review criteria.