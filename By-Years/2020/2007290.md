* 2007290
* III: Small: Collaborative Research: Modeling Pre- and Post- Conditions for Understanding Events
* CSE,IIS
* 09/01/2020,08/31/2024
* Niranjan Balasubramanian, SUNY at Stony Brook
* Continuing Grant
* Sylvia Spengler
* 08/31/2024
* USD 407,747.00

This project develops new algorithms for learning the typical pre-conditions and
post-conditions of real-world events. These logical conditions are crucial for
developing better language understanding applications that can reason precisely
about situations described in written text. There have been significant
technological advances in the automatic understanding of text, but event
reasoning requires knowledge that is often unstated and implicit. For example,
if a meeting is canceled, it would be unusual for the text to say that the
meeting was scheduled to happen (a pre-condition), and that it will now no
longer happen (a post-condition). While these are obvious to a human, these
conditions are unknown and crucial to building assistive technology. This kind
of reasoning can enable complete document understanding, support precise and
explainable question answering, and improve the output of language generation.
This project has broad applications to a variety of assistive technology for
information access and understanding for the general public. Learning pre-
conditions can further educational text exploration applications by explaining
how a certain situation came about. Better language understanding can also help
explain automated decisions, making technology more trustworthy in mission
critical domains. And finally, this project will help address the shortage of
talent in the critical areas of computer science and machine learning by
training graduate and undergraduate students.&lt;br/&gt; &lt;br/&gt;This project
focuses on developing both the models to learn pre- and post-condition
relations, but also the large datasets required to enable this learning. The
first thrust in the project plan is to develop new datasets of conditional
knowledge, and then to develop initial supervised learning algorithms to detect
them in text. The project will initially focus on todayâ€™s large-scale language
models to establish competitive baselines that the rest of the project will
improve upon. After creating these annotated datasets and baselines, the focus
will then turn to developing generative neural architectures like variational
autoencoders that are augmented with rich structured latent spaces. These spaces
will be augmented with entity networks that allow it to track generic event
knowledge, but also specific knowledge about the entities. The motivation for
generative models is to aggregate condition knowledge from large collections of
unlabeled text as well. Finally, in addition to developing large scale datasets
to learn this knowledge, the project will develop new reasoning tasks that could
spur the community to develop more precise language understanding models, and to
use these tasks to further research into richer models of event knowledge. All
scientific findings, datasets, and other artifacts of the research will be made
available for the scientific community and the broader
public.&lt;br/&gt;&lt;br/&gt;This award reflects NSF's statutory mission and has
been deemed worthy of support through evaluation using the Foundation's
intellectual merit and broader impacts review criteria.