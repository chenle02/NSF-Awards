* 0914868
* RI:Small Time-Based Language Modeling
* CSE,IIS
* 10/01/2009,09/30/2014
* David Novick, University of Texas at El Paso
* Continuing Grant
* Tatiana Korelsky
* 09/30/2014
* USD 505,999.00

Speech recognizers all include a component for predicting, based on the past
context, what words are likely to appear next. Today these components, known as
language models, operate at the symbol level, abstracted away from the details
of how and when the words are spoken. Spoken language, however, is not just a
symbolic or mathematical object, but is produced and understood by human brains,
with specific processing constraints, and these can directly affect what happens
when in dialog.&lt;br/&gt;&lt;br/&gt;This project is developing language models
and ``dialog models'' that explicitly use the information in the timings of
words. Inspired by psychological research suggesting that dialog and language
behaviors are the result of multiple simultaneously active cognitive processes,
the working assumption is that the words likely to be spoken at a given time
depend, probabilistically, on the elapsed time since various reference points:
for example since the speaker began talking, since the speaker's last
disfluency, since the listener's last back-channel, etc. Statistical analyses of
large corpora of human-human spoken dialogs, with machine learning methods, are
revealing patterns and regularities which are being used to build language
models with improved predictive power.&lt;br/&gt;&lt;br/&gt;These language
models implicitly represent some aspects of dialog dynamics, with the potential
to lead to an integrated understanding of the nature of dialog as a human
ability. These improved language models are also likely to improve speech
recognition accuracy, enabling the development of spoken language systems that
are more accurate, more efficient, and more useful.&lt;br/&gt;&lt;br/&gt;