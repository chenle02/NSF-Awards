* 0328431
* Collaborative Research:     Monitoring Student State in Tutorial Spoken Dialogue
* CSE,IIS
* 09/01/2003,08/31/2007
* Diane Litman, University of Pittsburgh
* Continuing Grant
* Tatiana Korelsky
* 08/31/2007
* USD 430,803.00

This research investigates the feasibility and utility of monitoring student
emotions in spoken dialogue tutorial systems. While human tutors respond to both
the content of student utterances and underlying perceived emotions, most
tutorial dialogue systems cannot detect student emotions, and furthermore are
text-based, which may limit their success at emotion prediction. While there has
been increasing interest in identifying problematic emotions (e.g. frustration,
anger) in spoken dialogue applications such as call centers, little work has
addressed the tutorial domain.&lt;br/&gt;&lt;br/&gt;The PIs are investigating
the use of lexical, syntactic, dialogue, prosodic and acoustic cues to enable a
computer tutor to automatically predict and respond to student emotions. The
research is being performed in the context of ITSPOKE, a speech-based tutoring
dialogue system for conceptual physics. The PIs are recording students
interacting with ITSPOKE, manually annotating student emotions in these as well
as in human-human dialogues, identifying linguistic and paralinguistic cues to
the annotations, and using machine learning to predict emotions from potential
cues. The PIs are then deriving strategies for adapting the system's tutoring
based upon emotion identification.&lt;br/&gt;&lt;br/&gt;The major scientific
contribution will be an understanding of whether cues available to spoken
dialogue systems can be used to predict emotion, and ultimately to improve
tutoring performance. The results will be of value to other applications that
can benefit from monitoring emotional speech. Progress towards closing the
performance gap between human tutors and current machine tutors will also expand
the usefulness of current computer tutors.&lt;br/&gt;