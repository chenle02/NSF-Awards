* 1412566
* Simultaneous Blind De-Convolution of Repeated Astronomical Exposures
* MPS,AST
* 09/15/2014,08/31/2017
* Tamas Budavari, Johns Hopkins University
* Standard Grant
* Nigel Sharp
* 08/31/2017
* USD 241,309.00

Images are ubiquitous throughout the scientific endeavor. Although many pictures
are perfectly adequate in their original form, there are many scientific fields
where single frames are not sufficient. Traditional methods of improving such
results to enable research with them include combining multiple frames, which
often reduces the final quality to around that of the lowest common denominator,
and taking long sequences from which only the best are selected and combined,
thus throwing away a lot of information. This is a project to find new ways
through statistics and image processing to combine repeated exposures to produce
images of superior quality, with less blur and higher resolution, and to carry
this out with automatic pipeline processing. The value of high quality images
that retain the maximum amount of information from all the painstakingly
assembled data is inestimable, and will impact pretty well every field of
science, including amateur activities and citizen science projects, and will be
generally useful in any situation where blurred or faint pictures are a limiting
factor.

Imaging detectors of increasing size and complexity are nowadays the primary
source of data in many scientific experiments. The images they produce, however,
are often blurred by distortions that can change rapidly, such as the atmosphere
between astronomical objects and telescopes. Eliminating such effects with
hardware is either extremely complex (e.g,. adaptive optics) or extremely
expensive (e.g., space-based observatories). To detect fainter signals requires
multiple exposures, which are traditionally combined by convolving to the lowest
acceptable quality, but doing that throws away a lot of the information in the
images. There has to be a better way. This project will develop new
methodologies in computational statistics and image processing for the optimal
combination of repeated exposures to produce images of superior quality, having
minimal blur and higher resolution than the originals. This requires developing
novel Bayesian algorithms and industrial-strength scalable software.

The study combines statistics, computer science and data-intensive science into
a focused, powerful research program that should lead to a significant leap in
image quality. The new idea is potentially game changing, is particularly well
suited to low signal-to-noise images, and should help in the design of new
experiments and observing strategies. The algorithms and tools developed will be
directly applicable to other research fields as diverse as meteorology and
genomics, and will be open-source and publicly available. These next-generation
data challenges will be especially valuable training for the graduate student
who will be doing much of the work.