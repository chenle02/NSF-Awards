* 2047418
* CAREER: Variational Inference for Resource-Efficient Learning
* CSE,IIS
* 09/01/2021,08/31/2026
* Stephan Mandt, University of California-Irvine
* Continuing Grant
* Rebecca Hwa
* 08/31/2026
* USD 446,455.00

The power of Deep Learning (DL) comes with enormous energy and storage costs due
to massive data needs and parameter-rich models. For example, recent models for
natural language generation contain more than a hundred billion parameters and
require huge amounts of training data. Training such a model can entail nearly
five times the lifetime carbon dioxide emissions of the average American car.
This project develops a holistic approach to resource-efficient DL based on a
common set of methodologies: DL models and algorithms are viewed through the
lens of information theory, making it possible to formally quantify and minimize
the required resources. Outcomes of this project include new methods for the
compression of both models (neural networks) and data (images and video), as
well as new training algorithms for DL that reduce data requirements and improve
runtime efficiency. These research activities will also inform summer teaching
activities for under-represented students, lead to new open-source software for
resource-efficient machine learning, as well as workshops and symposia on neural
compression and statistical machine learning. &lt;br/&gt;&lt;br/&gt;In more
detail, the project approaches resource-efficient machine learning from the
perspective of variational inference (VI) and contains three thrusts that focus
on different inefficiencies: (A) bandwidth inefficiency: a model's inefficient
representation of data or parameters, (B) data inefficiency: a model's extensive
need for training data, and (C) runtime inefficiency: a learning or inference
algorithm's inability to produce desired answers within a given computational
time budget. Thrust A draws on the connection between VI and rate-distortion
theory to derive new neural compression algorithms with improved compression
performance. Thrust B designs informative priors for effective learning with
limited data in non-stationary environments. Finally, thrust C develops highly
scalable training algorithms for Bayesian neural networks that hybridize Markov
Chain Monte Carlo and VI, trading-off precision for convergence speed. The
project contains applications from the domains of image and video compression as
well as climate science.&lt;br/&gt;&lt;br/&gt;This award reflects NSF's
statutory mission and has been deemed worthy of support through evaluation using
the Foundation's intellectual merit and broader impacts review criteria.