* 1514556
* CRCNS: Collaborative Research: Dynamic Models of Human Auditory Perceptual Switching Informed by Large-Scale ECoG Recordings
* CSE,IIS
* 09/01/2015,08/31/2019
* Bingni Brunton, University of Washington
* Continuing Grant
* Kenneth Whang
* 08/31/2019
* USD 145,556.00

Sounds in natural environments are complex mixtures from many different sources.
This project seeks to understand how humans organize mixtures of sounds into
meaningful objects. Perceptions of auditory objects arise not from any
particular part of the brain, but rather from coordinated activity across many
brain regions; further, binding of sounds to auditory objects may switch very
rapidly. Therefore, the study of how auditory objects are formed and how rapid
switching occurs requires analyzing recordings of brain activity in humans
across many brain areas and at very high speed. This project aims to develop new
theoretical methods for integrating and analyzing complex dynamic data sets of
brain recordings from large-scale electrode arrays. The modeling approach will
provide insight in the understanding of human auditory perception in both normal
and clinically impaired minds. &lt;br/&gt;&lt;br/&gt;Significant advances have
been made in the past three decades characterizing neural correlates of auditory
perceptions localized to the auditory cortex. Nevertheless, these neural
correlates are likely not restricted to the auditory cortex, or to any
particular part of the brain. To understand the neural mechanisms of auditory
perceptual representation and perceptual switching, the current project combines
advances in both experimental design and theory. Large-scale
electrocorticography (ECoG) recordings will be collected from human subjects as
they self-report their perceptions during a bistable auditory task involving
rapid perceptual switching. Next, spatial-temporal patterns of cortical
activation during the task will be extracted from these large time-series
datasets using a data-driven method novel to neuroscience known as dynamic mode
decomposition (DMD). Features extracted by DMD will then be used to build data-
driven, low-dimensional dynamic models that capture the temporal evolution of
multiple cortical areas, encoding both the auditory stimulus and the perceptual
state.