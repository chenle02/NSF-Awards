* 2211983
* Collaborative Research: SHF: MEDIUM: Smart Integrated Tuning of Parallel Code for Multicore and Manycore Systems
* CSE,CCF
* 10/01/2022,09/30/2025
* Barbara Chapman, SUNY at Stony Brook
* Continuing Grant
* Almadena Chtchelkanova
* 09/30/2025
* USD 81,301.00

High Performance Computing (HPC) entails executing code on multicore and
manycore architectures. To better utilize multicore/manycore architectures,
parallel programming models have emerged. But often using these parallel models
naively will not be able to scratch the surface of the potential performance
gains such systems can provide. A common technique for improving performance is
to add more hardware resources. However, this is expensive and system
integration is usually an onerous task. To this end, the investigators propose a
framework of improving performance by better utilization of the available
resource and identifying near-optimal configuration. These configurations can
take the form of code optimizations, as well as intelligent resource mapping and
utilization. Specifically, this project is concerned with identifying code
optimizations and runtime configurations that can potentially speed up
executions manifold. Faster executions can also implicitly lead to reduced power
consumption. Additionally, for situations where existing execution performance
is acceptable, the proposed approach can also be extended to optimize for other
performance metrics such as power. Power consumption is usually a huge
bottleneck for HPC systems, and is a source of concern for organizations that
deploy such systems; these concerns are both fiscal and environmental. The
investigators posit that the framework outlined in this project can also be
extended to optimize for power consumption without compromising execution
performance.&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;The investigators’ aim is to
provide such an AI-assisted framework that can automatically configure parallel
code considering the underlying hardware architecture. The steps necessary to
build such a framework lie at the convergence of compiler technologies,
performance analysis and modeling, and deep learning. A primary driver of this
project will be developing a program representation technique targeted towards
parallel code. Existing representations target mostly serial code and cannot
fully encapsulate the interactions and complexities of parallel code. Such a
code representation technique is highly suited to analyses using deep learning.
A means of representing parallel code in a machine learning friendly format will
be very beneficial to the overall program analysis community. The proposed code
representation will take the form of a graph, in order to correctly typify the
inherent structure present in code. The investigators propose modeling this code
representation using state-of-the-art Graph Neural Network (GNN) techniques. The
modeled embeddings will be used in conjunction with task specific features in
order to identify near optimum configurations for improved performance. The
overall scale of this project will span the entire “source code to execution”
pipeline that most HPC workloads follow. The aim of this project is to optimize
each optimizable step in the pipeline. A sample optimization pipeline can take
the following form: given a parallel code, our GNN-based code optimization model
will predict the best optimizations for the given code, followed by identifying
the best device (CPU, GPU, and others) for executing the optimized code. Further
downstream, our framework will identify the optimum runtime configurations
appropriate for the device under consideration. The ideas presented in this
project can have the potential effect of increased hardware utilization and
reduced future hardware commissioning.&lt;br/&gt;&lt;br/&gt;This award reflects
NSF's statutory mission and has been deemed worthy of support through evaluation
using the Foundation's intellectual merit and broader impacts review criteria.