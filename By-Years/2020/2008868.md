* 2008868
* AF: Small: Rehabilitating Constants in Sublinear Algorithms
* CSE,CCF
* 07/01/2020,06/30/2024
* Eric Price, University of Texas at Austin
* Standard Grant
* A. Funda Ergun
* 06/30/2024
* USD 500,000.00

The scale of data produced in the world is growing faster than
the&lt;br/&gt;ability to process it. One approach for dealing with this data
deluge&lt;br/&gt;involves sublinear algorithms, which are designed to estimate
some&lt;br/&gt;feature of data without needing to store, or sometimes even see,
the&lt;br/&gt;entire data set. Sublinear algorithms include distribution
testing&lt;br/&gt;(e.g., estimating if a lottery is fair or not), streaming
algorithms&lt;br/&gt;(e.g., finding the most common URLs on the web), and
property testing&lt;br/&gt;(e.g., estimating the maximum degree of a network).
For these&lt;br/&gt;problems, computer scientists have carefully studied how
the&lt;br/&gt;complexity of the solution grows with the problem parameters---
for&lt;br/&gt;example, estimating if a lottery is fair requires a number of
draws&lt;br/&gt;that scales with the square root of the number of possible
numbers&lt;br/&gt;drawn. But results so far have not been able to analyze the
solution&lt;br/&gt;complexity for concrete instances (e.g., for a birthday
lottery with&lt;br/&gt;366 possible numbers, how many samples are necessary to
verify&lt;br/&gt;fairness?). This project aims to change that, by finding
solutions&lt;br/&gt;with not only good asymptotic scaling, but good constant
factors.&lt;br/&gt;&lt;br/&gt;Developing sublinear algorithms with good constant
factors will&lt;br/&gt;require new algorithmic techniques. The sublinear-
algorithms&lt;br/&gt;literature is based on several widespread techniques like
probability&lt;br/&gt;amplification that are simple, general, and optimal up to
constant&lt;br/&gt;factors---and significantly suboptimal in their constant
factors. By&lt;br/&gt;replacing these techniques with more fine-grained ones,
this project&lt;br/&gt;aims to develop new algorithms with better performance in
practice.&lt;br/&gt;This project also aims to empirically measure the worst-
case&lt;br/&gt;performance of algorithms, by identifying which input
distributions&lt;br/&gt;are provably hardest to solve. By testing different
algorithms in&lt;br/&gt;practice, the project will discover and compare the
actual impact of&lt;br/&gt;different algorithmic
choices.&lt;br/&gt;&lt;br/&gt;This award reflects NSF's statutory mission and
has been deemed worthy of support through evaluation using the Foundation's
intellectual merit and broader impacts review criteria.