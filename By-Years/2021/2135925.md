* 2135925
* Distributionally Robust Adaptive Control: Enabling Safe and Robust Reinforcement Learning
* ENG,CMMI
* 07/01/2022,06/30/2025
* Naira Hovakimyan, University of Illinois at Urbana-Champaign
* Standard Grant
* Eva Kanso
* 06/30/2025
* USD 375,000.00

Data-driven algorithms can autonomously control complex systems like autonomous
cars and drones. However, the use of such powerful algorithms remains relegated
primarily to controlled laboratory environments. The main reason for the minimal
adoption of data-driven methods for safety-critical systems is the difficulty
one encounters when attempting to establish safety and predictability guarantees
as one would do with well-established control theoretical methods. This award
supports fundamental research to identify the best methodologies to consolidate
data-driven and control-theoretic tools so that the overall methodology is safe,
robust, and high-performing. The new approach lifts control tools to speak the
same language as the data-driven methods. In doing so, the performance of the
data-driven methods is not compromised, and yet, the safety guarantees of
control-theoretic tools can be constructed. Safe and predictable autonomous
operation of complex systems can bring immense socio-economic benefits through
its application in medical robotics, autonomous logistics, transportation, and
extra-terrestrial exploration, to name a few. This research involves multiple
disciplines, including robotics, control theory, statistical learning, and
mathematics. The cross-disciplinary nature will assist underrepresented groups'
broader participation in STEM and impact engineering education.
&lt;br/&gt;&lt;br/&gt;To adopt data-driven methods that rely on reinforcement
learning (RL) algorithms in safety-critical systems, we need guarantees on
safety and robustness. Robust and adaptive control methodologies developed for
classical systems with parametric uncertainties cannot be used directly in
conjunction with RL because the latter operates on data-driven models for which
identifying parametric and deterministic uncertainties is difficult, if not
impossible. This research will construct a new class of robust adaptive
controllers that are robust to errors in the learned distributions, thus
allowing RL algorithms to directly interact with these controllers without
further restrictions. Due to robustness at the level of distributions, notions
of risk-aware safety can be included in a straightforward manner. This research
will first aim to construct controllers that track temporally evolving state
distributions with uniform bounds. Then, the epistemic uncertainties will be
introduced with a novel adaptive control scheme to quantifiably control the
effect of the uncertainties in the space of distributions. The results produced
through this effort will bring the two distinct worlds of data-driven control
and classical control together at a natural intersection point where
trajectories of distributions, not of sample paths, are
considered.&lt;br/&gt;&lt;br/&gt;This award reflects NSF's statutory mission and
has been deemed worthy of support through evaluation using the Foundation's
intellectual merit and broader impacts review criteria.