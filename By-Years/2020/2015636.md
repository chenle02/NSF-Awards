* 2015636
* Leveraging Structural Information in Regression Tree Ensembles
* MPS,DMS
* 09/01/2019,08/31/2020
* Antonio Linero, University of Texas at Austin
* Continuing Grant
* Gabor Szekely
* 08/31/2020
* USD 25,854.00

A common task in statistics is prediction; for example, a practitioner may be
interested in predicting the presence of a disease given genetic information
about an individual. Due to recent advances in data collection, frequently one
has access to datasets which contain a massive number of predictors, but with
correspondingly few subjects. This setting is generally referred to as the "big
P, small n" scenario. Drawing meaningful conclusions under such circumstances is
generally impossible unless the underlying data satisfy certain structural
assumptions. The simplest such structural assumption is that only a small number
of the predictors are relevant; in this setting, finding the useful predictors
corresponds to finding a so-called "needle in a haystack." The goal of this
project is to construct procedures which adapt to this, and other, structural
assumptions. The project will focus on methods based on decision trees, which
are flowchart-like structures in which predictions are based on whether the
predictors satisfy various rules. Usually an ensemble of decision trees are
constructed, with the predictions for each individual tree averaged. While
decision tree ensembles are frequently used with high dimensional data, it is
unclear to what extent they adapt to the structural properties of the data. This
project will show that, in practice, off-the-shelf decision tree ensembling
methods do not adapt to common structural assumptions, and will develop new
methods which do. In addition to developing methods with strong theoretical
support, this project will support the development of an R package to give
practitioners easy access to our methodology.&lt;br/&gt;&lt;br/&gt;
&lt;br/&gt;The PI will develop Bayesian methods for incorporating structural
information into tree-based ensemble methods, and establish theoretically the
benefit of making use of this additional information. This forms a nonparametric
counterpart to the parametric approaches used in linear models, such as the
lasso, graphical lasso, or group lasso; Bayesian approaches in the parametric
setting include the use of variable selection priors, such as spike-and-slab
priors and global-local shrinkage priors. Structural information will be
incorporated by modifying the commonly used priors on decision tree ensembles so
that the prior is concentrated on models which satisfy the desired structure.
The PI will first investigate the theoretical properties of a sparsity inducing
prior which is designed to eliminate unnecessary predictors. Sparsity here is
obtained by applying a sparsity inducing Dirichlet prior to the a priori
probability that a given branch is associated to a given predictor. This prior
will be extended to allow for grouped variable selection in a similar manner to
the group lassoby considering the class of Dirichlet tree priors, and further to
accommodate graphical structures in the predictors through sparsity inducing
logistic normal priors. Additionally, the PI will develop computationally
efficient Markov chain Monte Carlo algorithms to fit the resulting models.
Compared to existing methods, these structural priors will be shown to lead to
substantial gains in predictive accuracy, and to more accurate scientific
discovery.