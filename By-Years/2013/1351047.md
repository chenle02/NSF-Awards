* 1351047
* CAREER: Towards a Big Data Application Server Stack
* CSE,CNS
* 02/01/2014,01/31/2019
* Tyson Condie, University of California-Los Angeles
* Continuing Grant
* Marilyn McClure
* 01/31/2019
* USD 464,715.00

Google's MapReduce inspired much of the Big Data Analytics work and has served
as a template for open source systems like Apache Hadoop. The MapReduce
programming model has wide applicability, but widespread adoption has exposed
some limitations, such as the lack of support for iteration (which is common in
machine learning algorithms), stream processing, graph analytics, real-time and
interactive queries. Beyond the programming framework, the underlying
implementation offers a template for how to scale-out massively distributed
computations: break them up into small tasks that can be carried out in parallel
by partitioning the underlying data, and save intermediate state to mitigate the
impact of partial failures (which must be planned for when running on large
clusters). The challenge then, is to build implementations of other programming
frameworks (e.g., SQL and machine learning) that share the same scale-out and
fault-tolerance runtime characteristics of MapReduce without imposing its
limitations.

Resource managers such as Apache Hadoop YARN, Google Omega and Berkeley Mesos
take a first step in this direction by separating resource allocation from the
details of higher-level programming models and languages. Resource managers
multiplex several jobs on the same underlying machine cluster, thereby
increasing utilization and fostering clean-slate software stacks. When the task
executing in a container a slice of a single machine's resources (CPU/GPU,
memory, disk) is finished, the container is returned to the resource manager,
where it is made available to other jobs. Unlike in higher-level stacks, a
container is a blank-slate process, designed to host arbitrary computations.
This project prescribes further reusable software layers that capture issues
like how many resources should I dedicate to a job?; what are the redundant
code-pathways and can I provide them in a reusable library?; what are the right
language and runtime abstractions? Exploring these questions in the context of
systems like MapReduce and related SQL implementations, ML toolkits, storage
systems, and messaging systems, on next generation resource managers, is the
primary focus of our work.

The goal is to unify a suite of large-scale data processing tasks on a single
runtime layer, built on modern resource managers (the cloud operating systems).
Our results will factor out commonalities in specialized systems and provide
them in a single underlying runtime system, shortening the time to ?market? for
the next ready-to-use Big Data toolkit, which in turn would increase the
availability of such tools to the broader community. Experience gained by
implementing and deploying applications at scale, over next generation resource
managers, could help inform critical design choices in the development of future
cloud computing platforms, and hence impact a broad range of scientific,
engineering, national security, healthcare and business applications. The
project offers enhanced opportunities for research-based advanced training of
graduate and undergraduate students, including members of groups that are
currently under-represented in computer science, in databases, machine learning,
and cloud computing.