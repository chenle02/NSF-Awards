* 1420897
* RI: Small: Dynamic Attractor Computing: A Novel Computational Approach Applied Towards Temporal Pattern and Speech Recognition
* CSE,IIS
* 10/01/2014,09/30/2017
* Dean Buonomano, University of California-Los Angeles
* Standard Grant
* Kenneth Whang
* 09/30/2017
* USD 399,698.00

Harnessing the brain's computational strategies has been a long sought objective
of computational neuroscience and machine learning. One reason this goal has
remained elusive is that most neurocomputational frameworks have not effectively
captured a fundamental computational feature of the brain: the ability to
seamlessly encode, represent, and processes temporal information. The current
project seeks to address this shortcoming by using the neural dynamics inherent
to recurrent neural networks to generate temporal patterns and process temporal
information.

The ability to generate the fine motor patterns necessary to play the piano or
parse the complex temporal structure of speech, are but two examples of the
human brain's sophisticated ability to generate and process complex temporal
patterns. Notably, both these examples also illustrate an additional feature of
the brain's computational abilities: "temporal warping." We can play the same
musical piece at different speeds, or understand speech spoken at slow or fast
rates. The mechanisms underlying the brain's ability to process temporal
information in a flexible and temporally invariant fashion are a key focus of
the current proposal. Recent theoretical and experimental studies have favored
the view that the brain does not have sampling rates, time bins or explicit
delay lines; but rather encodes time and the temporal features of stimuli
through the internal dynamics of recurrent neural networks. The computational
potential of these recurrent neural network models, however, has been limited
for two reasons: 1) while it is well established that the recurrent connections
of neural circuits are plastic, it has proven challenging to incorporate
plasticity into simulated recurrent neural network models; 2) the dynamic
regimes with the most computational potential are precisely those that also
exhibit chaos--voiding much of their computational potential because the
dynamics is not reproducible across trials. Building on a novel framework, this
project tunes the weights of the recurrent connections in a manner that "tames"
the chaotic dynamics of recurrent networks. The approach creates locally stable
trajectories (dynamic attractors) which provide a novel and potentially powerful
computational approach that can elegantly encode temporal information, and
retain internal memories of recent events. Of particular relevance to the
current project is to demonstrate that these networks can produce families of
similar neural trajectories that flow at different speeds, thus allowing the
network to generate the same complex motor pattern at different speeds. This
project will also determine if the principles of dynamic attractors and time
warping can be applied in the domain of sensory processing, using speech
recognition as a test bed for the brain's ability to discriminate complex
spatiotemporal stimuli.