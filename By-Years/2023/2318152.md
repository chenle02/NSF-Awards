* 2318152
* EFRI BRAID: Scalable-Learning Neuromorphics
* ENG,EFMA
* 09/01/2023,08/31/2027
* Dmitri Strukov, University of California-Santa Barbara
* Standard Grant
* Alias Smith
* 08/31/2027
* USD 1,999,968.00

Recent advances in the field of artificial intelligence suggest that the route
toward higher-intelligence brain-inspired systems is via significantly
increasing network size. However, such a rather evolutionary approach faces dire
challenges. Training the largest machine learning models for natural language
processing requires months of data-center-scale computations, in other words,
enormous energy, time, and cost. Reserves for improvements, e.g., due to the
refinement of algorithms and hardware, seem limited, and most importantly,
further advances could no longer be fueled by semiconductor technology scaling.
Additionally, state-of-the-art models rely on offline training with vast amount
of training data, i.e., are not capable of continual real-time learning. Such
challenges naturally bring more attention to the biological neural networks,
which are living proof of superior, agile, and adaptive intelligence running on
very energy-efficient “brain” hardware. Exciting opportunities are presented by
recent developments in the theory of spiking neural networks, the most
biologically plausible models, and neuromorphic circuits implemented with dense
emerging memory devices. The proposed project aims to capitalize on these
advances and address the most pressing challenges to develop scalable algorithms
and hardware for human-brain-scale neuromorphic systems with practically useful
(robust, fast, inexpensive) learning capabilities. The project will enable
neuromorphic systems of immediate importance for many practical applications,
including autonomous robots and vehicles, and biomedicine, including portable
and personal medical devices. Furthermore, the broad algorithm-to-system nature
of the proposed research provides attractive opportunities for high-school,
undergraduate, and graduate students to explore novel research and get exposed
to the emerging field of neuromorphic computing. The proposed project will
pursue outreach activities by leveraging programs at participating universities,
with a particular focus on attracting minority students.

The key features of our research are hardware-friendly local learning
algorithms, a framework for continual online “one-shot” learning, and variation-
tolerant in-memory computing hardware circuits. Specifically, on the algorithmic
front, we focus on recurrent spiking neural networks with biologically-plausible
spike frequency adaptation neurons. We will build on local learning algorithms
recently proposed by our team members that facilitate learning over longer time
scales via synaptic plasticity. These algorithms will be further extended to
support continual learning and co-optimized with the hardware using neural
architecture search techniques. On the hardware front, the focus is on hybrid
neuromorphic circuits that take advantage of analog in-memory computing.
Critical hardware challenges, such as the scaling of network complexity and
implementation of robust in-situ learning, will be addressed by utilizing ultra-
high-density crosspoint devices implementing fixed-value weights of the learning
algorithms and novel variation-tolerant memristive synapses featuring both
short-term and long-term plasticity. Algorithms and hardware circuits will be
holistically integrated into the learning-to-learn spiking neural network
framework. Such a framework enables two kinds of learning – a slow incremental
one mimicking developmental learning and fast “one-shot” learning utilizing
network dynamics.

This award reflects NSF's statutory mission and has been deemed worthy of
support through evaluation using the Foundation's intellectual merit and broader
impacts review criteria.