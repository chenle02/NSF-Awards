* 1513512
* III: Medium: Collaborative Research: Algorithms and Cyberinfrastructure for High-Precision Automated Quality Control of Hydro-Meteo Sensor Networks
* CSE,IIS
* 09/01/2015,08/31/2019
* Michael Piasecki, CUNY City College
* Continuing Grant
* Sylvia Spengler
* 08/31/2019
* USD 363,471.00

Advances in sensor technology are greatly expanding the range of quantities that
can be measured while simultaneously reducing the cost. However, deployed
sensors drift out of calibration and fail, so every sensor network requires
quality control (QC) procedures to promptly detect these failures. Existing QC
methods rely on human experts to carefully examine the data, which means that
when the number of sensors in a network doubles, the number of experts must
double too. This project will develop algorithms and software to increase the
level of automation in sensor QC so that a smaller number of experts can manage
a much larger network of sensors. The methods will be tested on weather data
from Oklahoma (the Oklahoma Mesonet), Oregon (the Andrews Long-Term Ecological
Network site), the US (the Earth Networks "WeatherBug" network), and sub-Saharan
Africa (the TAHMO project), and if the methods are found to work well, they will
be deployed in these networks at at the CUAHSI Water Data Center. Accurate
weather data could significantly increase the productivity of farms and improve
food security, particularly in Africa.

The project will develop an open-source standards-compliant system, SENSOR-DX,
that implements automated data QC. Existing probabilistic QC methods assume that
correct sensor readings are jointly Gaussian and readings from broken sensors
obey a uniform distribution. These assumptions lead to many QC mistakes. This
project will develop a new approach in which novel nonparametric anomaly
detection algorithms analyze the sensor data. Correct sensor readings have low
anomaly scores, while broken sensor readings have high scores; both follow
parametric distributions. Probabilistic methods can therefore model the
distribution of the resulting anomaly scores instead of the joint distribution
of the original sensor readings and infer (probabilistically) whether each
sensor is working correctly. To enhance the fault-detection capability of the
anomaly detection algorithms, the raw sensor data will be detrended and
assembled into multiple views that highlight various correlations among sensor
values. The project will develop a novel View-Anomaly-Diagnosis (VAD) framework
in which anomaly detection algorithms are applied to the tuples in each view,
and then the anomaly scores are combined via a probabilistic diagnostic model to
infer which sensors are broken and which are functioning correctly. The project
will study how good the detrending models need to be in order to enhance the
accuracy of anomaly detection. The new anomaly detection algorithms are based on
a new anomaly detection principle: "anomaly detection by overfitting". Existing
methods fit a statistical model to "normal" behavior and then identify data
points that do not fit well ("are underfit") and mark them as anomalies. The new
principle measures how easy it is to "overfit" a statistical model that
separates candidate anomalies from the rest of the data. The project will
develop new algorithms based on this principle and understand how they relate to
existing methods of anomaly detection by underfitting. The VAD framework will be
implemented in the SENSOR-DX system: a series of Kepler workflows that provide
support for connecting a new sensor network, training the detrending and anomaly
detection models, performing real-time anomaly detection, and repairing bad
sensor readings using predictive models. SENSOR-DX will also support semantic
matching of new sensor data streams by extending the EnvThs controlled
vocabulary thesaurus.

For further information see the project web site at http://tahmo.org/sensor-dx