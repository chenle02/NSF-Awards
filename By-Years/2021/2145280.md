* 2145280
* CAREER: Flexible and Robust Reasoning in Natural Language
* CSE,IIS
* 07/01/2022,06/30/2027
* Gregory Durrett, University of Texas at Austin
* Continuing Grant
* Tatiana Korelsky
* 06/30/2027
* USD 225,086.00

This award is funded in whole or in part under the American Rescue Plan Act of
2021 (Public Law 117-2).&lt;br/&gt;&lt;br/&gt;Modern question answering systems,
embedded in search engines and digital assistants, have improved dramatically
with the development of large neural network models. When a user asks a simple
question, these systems can typically return an answer directly rather than just
linking to a webpage. However, these systems still fail on more complex
questions, and when they fail, they may mislead their users. They lack an
important capability that humans have: the ability to reason about and
synthesize the information they see, retrieve and integrate additional
information, and arrive at a justified conclusion. This CAREER project aims to
address this shortcoming by developing systems that "think through" textual
evidence, leading to more reliable answers that can be explained to a user. Such
advances fit into a broader thread of building trustable AI systems that
explicitly show their work and are auditable before and during their
deployment.&lt;br/&gt;&lt;br/&gt;This project specifically addresses the
problems of question answering and fact-checking by developing a learning-based
system that reasons in natural language. The system takes text as input, then
applies pre-trained neural network models to reformulate that text, derive
conclusions from it, and eventually check a claim or verify an answer. This
process produces a series of logically connected statements understandable by a
human. This outcome is enabled by two modules. First, a deduction module
repeatedly combines two statements and generates a third that follows from the
inputs, encapsulating common logical rules. Second, a verifier determines
whether the final deduced evidence validates the original claim. Both systems
are built from pre-trained models like T5 that have demonstrated strong
generalization capabilities. Collecting training data for these models
constitutes a core challenge; the project's approach blends multiple strategies
including synthetic data generation and human-in-the-loop annotation. These
techniques are applied to the domains of question answering and fact checking,
problems where providing additional explanation and justification instead of
just giving a best-effort answer are essential to make usable systems. This
system paves the way for NLP tools that know what they don't know, provide
interpretability for end users, and enable system developers to better
understand and improve their models.&lt;br/&gt;&lt;br/&gt;This award reflects
NSF's statutory mission and has been deemed worthy of support through evaluation
using the Foundation's intellectual merit and broader impacts review criteria.