* 2119069
* Collaborative Research: PPoSS: LARGE: Unifying Software and Hardware to Achieve Performant and Scalable Frictionless Parallelism in the Heterogeneous Future
* CSE,CCF
* 10/01/2021,09/30/2025
* Peter Dinda, Northwestern University
* Continuing Grant
* Alexander Jones
* 09/30/2025
* USD 1,482,740.00

Exploiting parallelism is essential to making full use of computer systems, from
phones to supercomputers. It is thus intrinsic to most applications today, and
is becoming increasingly so with time, especially as hardware becomes more
heterogeneous. Programming effective and performant parallel applications
remains a serious challenge, however. Achieving both high productivity and high
performance currently requires multiple experts. The project seeks to reduce
this to an ordinary programmer. This problem is often approached along only one
of two lines, "theory down", focusing on high-level parallel languages and the
theory and practice of parallel algorithms, or "architecture up", focusing on
rethinking abstractions at multiple layers, starting with the hardware. The
projectâ€™s core novelties are (1) to unify these two approaches, combining their
strengths to reduce the expertise needed to write performant parallel programs,
and (2) to develop integrated techniques that can enable taking advantage of
heterogeneous hardware. Realizing these novelties will require designing a
"full-stack" approach to parallelism and innovation across the hardware/software
stack. The project's impacts are (1) the development of techniques that
dramatically simplify parallel programming, including for heterogeneous
machines, putting it into the purview of the ordinary programmer, and (2) the
development of systems and educational materials to teach this skill to broader
audiences including students at the researchers'
institutions.&lt;br/&gt;&lt;br/&gt;The technical strategy of the project is to
bridge high-level parallel languages, which allow clean expression and analysis
of program parallelism, to heterogeneous, extensible hardware (modeled using
FPGAs) through an integrated series of intermediate representations (IRs) of a
program and of the hardware/software capabilities of the target platform. The
design of these representations will be geared to avoid the information loss
(going both up and down the compiler/runtime/OS/hardware stack) that currently
hampers optimization at all levels. A new compilation model for high-level
parallel languages is being developed that extensively leverages modern compiler
technology, but also avoids "premature lowering" of parallel constructs, and
"premature abstraction" of hardware and low-level software features. Benchmarks
are beinge developed to measure the effectiveness of the
approach.&lt;br/&gt;&lt;br/&gt;This award reflects NSF's statutory mission and
has been deemed worthy of support through evaluation using the Foundation's
intellectual merit and broader impacts review criteria.