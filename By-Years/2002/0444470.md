* 0444470
* Extending the Limits of Large-Scale Shared Memory Multiprocessors
* CSE,CCF
* 11/01/2004,10/31/2007
* Mark Horowitz, Stanford University
* Standard Grant
* Almadena Chtchelkanova
* 10/31/2007
* USD 750,000.00

The objective of this research is to substantially improve the productivity of
programmers writing&lt;br/&gt;applications for petaflop-scale systems by using
programmer defined light-weight transactions as the&lt;br/&gt;single abstraction
for expressing parallelism, delineating communication, reasoning about
memory&lt;br/&gt;consistency, providing failure recovery, and allowing
performance optimization. Transactions as the&lt;br/&gt;central abstraction for
designing and programming parallel systems leads to a shared
memory&lt;br/&gt;programming and memory coherence model called Transaction
Coherence and Consistency (TCC).&lt;br/&gt;Transactions simplify parallel
programming by providing a way of writing correct shared-
memory&lt;br/&gt;programs without threads, locks and semaphores. TCC systems
provide high performance communication&lt;br/&gt;and synchronization with
support for hardware mechanisms that can keep memory coherent
and&lt;br/&gt;consistent based on programmer-defined transactions.&lt;br/&gt;To
achieve the research objective, this research program will focus on five
activities. First, the researchers&lt;br/&gt;will develop new abstractions that
use transactions to provide a shared memory programming model
that&lt;br/&gt;makes it much easier to analyze and optimize application
performance. Second, the researchers will&lt;br/&gt;develop performance
monitoring systems that make use of transactions to detect performance
bottlenecks&lt;br/&gt;and to provide intuitive feedback to programmers. Third,
the researchers will use the transaction based&lt;br/&gt;programming model to
implement compiler-based static and dynamic feedback-directed
optimizations&lt;br/&gt;that automatically detect and eliminate performance
bottlenecks and extend the scalability of transaction&lt;br/&gt;coherency to 105
processors. Fourth, the researchers will use transactions to optimize the
performance of&lt;br/&gt;parallel storage I/O. Finally, the researchers will
develop simulation and emulation technology that will&lt;br/&gt;enable us to
experiment with petaflop-scale systems that support light-weight transactions
before they are&lt;br/&gt;available.&lt;br/&gt;Broader Impacts&lt;br/&gt;The
broad impact of this research is to use transaction-based parallel programming
to educate and enable&lt;br/&gt;a new class of parallel software developers who
can implement parallel software with the same facility&lt;br/&gt;that sequential
software is written today. Enabling parallel software development will be
critical to&lt;br/&gt;advancing computing performance from desktop applications
to large-scale scientific and commercial&lt;br/&gt;applications. While parallel
processing has been essential for large-scale machines for a while,
recent&lt;br/&gt;announcements by Intel, AMD and IBM demonstrate that it will
soon be critical for desktop applications&lt;br/&gt;as well. To educate
students, other researchers, and industry about the benefits of transaction-
based&lt;br/&gt;parallel programming, we will incorporate transactional
programming concepts in the parallel&lt;br/&gt;programming curriculum and make
transaction-based applications available to the wider
scientific&lt;br/&gt;community. The researchers expect that releasing a suite of
optimized transaction-based applications&lt;br/&gt;along with simulation
technology will be instrumental in encouraging other researchers to
experiment&lt;br/&gt;with and explore the benefits of transactions. To further
promote the use of transaction-based parallel&lt;br/&gt;programming we will
organize a tutorial or workshop at a major scientific computing conference that
will&lt;br/&gt;cover the principles and experience of programming with
transactions.