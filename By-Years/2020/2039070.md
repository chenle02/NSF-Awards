* 2039070
* Collaborative Research: CPS: Medium: Closing the Teleoperation Gap: Integrating Scene and Network Understanding for Dexterous Control of Remote Robots
* CSE,CNS
* 02/15/2021,01/31/2024
* Keith Winstein, Stanford University
* Standard Grant
* Linda Bushnell
* 01/31/2024
* USD 400,000.00

The aim of this proposal is to enable people to control robots remotely using
virtual reality. Using cameras mounted on the robot and a virtual reality
headset, a person can see the environment around the robot. However, controlling
the robot using existing technologies is hard: there is a time delay because
it’s slow to send high quality video over the Internet. In addition, the
fidelity of the image is worse than looking through human eyes, with a fixed and
narrow view. This proposal will address these limitations by creating a new
system which understands the geometry and appearance of the robot’s environment.
Instead of sending high-quality video over the Internet, this new system will
only send a smaller amount of information about how the environment’s geometry
and appearance has changed over time. Further, understanding the geometry and
appearance will let us expand the view visible to the person. Overall, these
will improve a human’s ability to remotely control the robot by increasing
fidelity and responsiveness. We will demonstrate this technology on household
tasks, on assembly tasks, and by manipulating small
objects.&lt;br/&gt;&lt;br/&gt;The aim of this proposal is to test the hypothesis
that integrating scene and networking understanding can enable efficient
transmission and rendering for dexterous control of remote robots through
virtual reality interfaces. This system will result in dexterous teleoperation
that enables remote human operators to perform complex tasks with remote robot
manipulators, such as cleaning a room or repairing a machine. Such tasks have
not previously been demonstrated to be teleoperated for two reasons: 1) lack of
an intuitive awareness and understanding of the scene around the remote robot,
and 2) lack of an effective low-latency interface to control the robot. We will
address these problems by creating new scene- and network-aware algorithms which
tightly couple sensing, display, interaction and transmission, enabling the
operator to quickly and intuitively understand the environment around the robot.
This project will research new interfaces which allow the operator to use their
hand to directly specify the robot’s end effector pose in six degrees of
freedom, combined with spatial- and semantic-object-based models that allow safe
high-level commands. This project will evaluate the proposed system by assessing
the speed and accuracy of the remote operator’s ability to complete complex
tasks, including assembly tasks; the aim will be to complete unstructured
assembly tasks that have never been demonstrated to be remotely teleoperated
before.&lt;br/&gt;&lt;br/&gt;This project is in response to the NSF Cyber-
Physical Systems 20-563 solicitation.&lt;br/&gt;&lt;br/&gt;This award reflects
NSF's statutory mission and has been deemed worthy of support through evaluation
using the Foundation's intellectual merit and broader impacts review criteria.