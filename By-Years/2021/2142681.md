* 2142681
* EAGER: Collaborative Research: On the Theoretical Foundation of Recommendation System Evaluation
* CSE,IIS
* 09/15/2021,08/31/2023
* Bin Ren, College of William and Mary
* Standard Grant
* Hector Munoz-Avila
* 08/31/2023
* USD 19,999.00

This project develops a new theoretical foundation for evaluating the
performance of recommendation systems (RS), a crucial component guiding online
users and shoppers to navigate a sea of products and websites. Despite the
Covid-19 pandemic, online retail sales in the US totaled nearly $1 trillion
dollars in 2020. Since online purchasing is forecasted to increase, proper
design of RS will improve shopping/browsing, help small online businesses to
survive, and contribute to the nationâ€™s economy. Recent studies have noted the
sizeable improvements obtained from deep learning-based recommendations.
However, several studies suggest that these improvements may be spurious due to
poorly designed experiments with ill-chosen baselines, cherry-picked datasets,
inaccurate metrics of RS performance, and the use of ineffective evaluation
protocols that result in performance discrepancies between evaluation and
production environments. Recognizing that baseline and dataset problems can be
addressed by using standard benchmarks, this project focuses on designing
reliable new computation tools, metrics, and evaluation protocols for analyzing
recommendation systems. The tools will include new ways to score an RS based on
accurate statistical models of user behaviors and a suite of new algorithms that
use fewer samples and computational resources that produce more accurate
estimations of performance.&lt;br/&gt;&lt;br/&gt;From a technical standpoint,
this project will develop theoretical tools to analyze evaluation metrics and
protocols for RS based on statistical learning theory and stochastic processes.
The project focuses on three tasks. First, designing efficient metrics
estimation procedures that resolve the mismatch between sampling and top-K
evaluation metrics (e.g., normalized discounted cumulative gain (nDCG) and
Recall) by unifying two recently proposed ad hoc approaches for recovering the
top-K metrics based on sampling and searching for an overall best estimator.
Second, the develops methods to quantify the sensitivity and robustness of the
top-K metrics, and design new item sampling procedures that improve the
robustness of existing metrics, The finally, the project will analyze the
performance gap between offline evaluations and production environments (the
online settings), and proposing a new offline evaluation metrics that can better
mimic online performance.&lt;br/&gt;&lt;br/&gt;This award reflects NSF's
statutory mission and has been deemed worthy of support through evaluation using
the Foundation's intellectual merit and broader impacts review criteria.