* 2223282
* CAREER: Efficient Fine-grained Algorithms
* CSE,CCF
* 11/01/2021,01/31/2024
* Barna Saha, University of California-San Diego
* Continuing Grant
* A. Funda Ergun
* 01/31/2024
* USD 362,015.00

Ever since the inception of computation, the fundamental question has been- what
problems can be solved by computers? And which ones can be solved in a
reasonable time? This brought in the notion of polynomial time solvable problems
which are considered efficient vs NP-hard problems which take prohibitively long
time to solve. The field of approximation algorithms, along with the theory of
inapproximability, deals with which of the NP-hard problems can be solved
efficiently by allowing approximate solutions. However, this crude distinction
of algorithmic efficiency--polynomial vs NP-hard, is insufficient when handling
today's large scale of data. We need a finer-grained design and analysis of
algorithms that pinpoints the exact exponent of polynomial running time, and a
better understanding of when a speed-up is not possible. Unfortunately, except
for a few problem-specific innovations, the study of such algorithms is deeply
lacking in the literature.&lt;br/&gt;&lt;br/&gt;This project targets to build a
unified theory of fine-grained algorithm design to study fast approximation
algorithms, and their fine-grained hardness. Developing systematic techniques
that emphasize on the trade-offs between running time, approximation and
randomness, and aid in designing low-complexity parallel algorithms will
significantly improve the state of the art. Moreover, motivated by core machine
learning applications, the project proposes an alternate model of efficiency via
classical query complexity. Tools from classical query complexity have
previously inspired development of fast algorithms and vice versa; the PI
expects similar connections to happen in this project. Elements of this endeavor
will be integrated with new courses, many of the algorithms developed herein
will be implemented and the close connection of the PI with industry will result
in possible adaptation of methodologies.&lt;br/&gt;&lt;br/&gt;The project will
address a suite of important optimization problems, from long-standing open
questions to modern problems with diverse applications. It will introduce fresh
tools from additive combinatorics, fourier analysis, rate distortion theory, and
circuit complexity for analysis of algorithms, and establishing lower bounds.
Specifically, new generic techniques of amnesic dynamic programming, fast
matrix-product over semiring, and low-degree polynomial method will be developed
to design fine-grained approximation algorithms and their parallel counterparts.
Time complexity may not always be the primary measure of efficiency. There are
many core machine learning problems where query complexity, that quantifies the
amount of labeled data acquired via active querying, is more important. The
project will analyze for the first time the query complexity of basic learning
problems, and explore its connections to developing fast algorithms.