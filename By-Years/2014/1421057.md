* 1421057
* III: Small: Large-Scale Structured Sparse Learning
* CSE,IIS
* 08/01/2014,05/31/2015
* Jieping Ye, Arizona State University
* Continuing Grant
* Christopher Clifton
* 05/31/2015
* USD 333,360.00

Recent technological revolutions have lead to dramatically growing scale,
diversity, and complexity of data. Modern data analysis is facing new challenges
in handling this complexity. Although complex, the underlying representations of
many real-world data are often sparse. This sparseness often exhibits intrinsic
structure, e.g., spatial or temporal smoothness, graphs, trees, and groups.
Finding effective sparse representations is fundamentally important for
scientific discovery; the a-priori structure information may significantly
improve the sparse learning model. This project is developing algorithms and
tools (including open source software) to enable knowledge discovery from
massive high-dimensional and complex data, as well as a new curriculum that
incorporates the proposed research into the classroom.&lt;br/&gt;&lt;br/&gt;Most
sparse learning algorithms are based on the L1 norm due to its sparsity-inducing
property and strong theoretical guarantees, but this does not capture structure.
This project is advancing structured sparse learning by (1) analyzing the so-
called proximal operators associated with various feature structures, which
explains how and why they can induce the desired structured sparsity; (2)
developing efficient algorithms for computing the proximal operators, which
plays a key building block role in the proposed optimization algorithms; (3)
developing a structured sparse learning framework, which includes various sparse
learning models and algorithms developed in this project.