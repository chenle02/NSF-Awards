* 1226963
* Understanding the perception and recognition of spoken words: Effects of phonetics, phonological variation, and speech mode
* SBE,BCS
* 09/15/2012,02/28/2018
* Meghan Sumner, Stanford University
* Continuing Grant
* Tyler Kendall
* 02/28/2018
* USD 399,814.00

Everyday, people face variation in language. Readers see words printed in
different fonts and typefaces, typically static on a page. Listeners hear a
speech signal that is riddled with variation. Words are pronounced at different
speeds, with different emotions, by different speakers with various accents in
different situations, streaming by listeners at a rate of about 5 to 7 syllables
per second. No two utterances of a single word are identical, yet this variation
typically goes unnoticed. Despite this variation, listeners quickly and adeptly
understand these different pronunciations as one word and not another; this is
remarkable, considering that many words sound the same. An issue central to
linguistic theory is how listeners are able to take this variable speech string
and understand it. &lt;br/&gt;&lt;br/&gt;Explanations of how listeners
accomplish this task incorporate purely linguistic solutions to the problem. Dr.
Sumner's research program takes a new approach, grounded by the real-world
information conveyed by acoustic-phonetic values. From any window of speech,
listeners encounter cues about sound patterns, but they also encounter cues
about the social characteristics of a particular speech situation. The impetus
behind this work is that advancement in understanding the perception and
recognition of spoken words will come from examining the ways in which listeners
use these ever-present cues together. Dr. Sumner will examine this question by
examining the ways in which listeners respond to words that vary along different
linguistic and social dimensions. She will use experimental methods that inform
us about how a word pronunciation improves recognition to related words in the
mental lexicon, and about how listeners categorize different sounds depending on
the linguistic and social context in which they are
uttered.&lt;br/&gt;&lt;br/&gt;This work investigates the claims that listeners
rely on socially-informative acoustic values of nearby sounds and words when
understanding speech, and that linguistic information is stored with social
representations that influence speech perception at a level much lower than once
thought. This work adds a new link between linguistic and social experience to
theories of speech perception and should prompt the field to reconsider the role
of acoustic patterns and their place in theory more broadly. Integrating
linguistic and social information, this research may provide a path to improve
interactive spoken language technologies.