* 1955246
* Collaborative Research: Two-dimensional Synaptic Array for Advanced Hardware Acceleration of Deep Neural Networks
* ENG,ECCS
* 09/01/2020,08/31/2024
* Yiran Chen, Duke University
* Standard Grant
* Prem Chahal
* 08/31/2024
* USD 200,000.00

Nontechnical:&lt;br/&gt;&lt;br/&gt;The big data revolution has created a
critical need for new computing paradigms to efficiently extract valuable
information from large datasets. In existing computing systems, data is
constantly transferred between the computation and memory units. This so-called
memory bottleneck limits their energy efficiency and speed. In contrast,
computation and memory in the human brain (neurons and synapses) are closely and
densely interconnected. This gives rise to the brainâ€™s extremely low power
consumption at ~20W. Inspired by the brain, neuromorphic computing and
artificial neural networks have recently attracted immense interest. In
particular, deep neural networks (DNNs) can execute complex processing tasks
such as pattern recognition and image reconstruction. However, DNNs are
computationally intensive and power hungry. This makes it impractical for them
to be scaled up to the level of the complexity for true artificial intelligence
(AI). In this project, the team will develop a novel artificial synapse for deep
neural networks. This prototypical synapse will offer low power consumption,
high precision, good scalability, and great potential for large-scale
integration. This work can lead to significant improvement in energy efficiency,
bandwidth, and performance for deep learning algorithms. The research outcome
can lead to the wide use of AI for both high-performance computing and low-power
flexible electronics. This project can revolutionize society through advances in
healthcare, self-driving vehicles, and autonomous manufacturing. The team will
work closely with their local communities to attract students to pursue
engineering careers, especially those from underrepresented groups. Activities
will include laboratory demonstrations, design projects, summer internships, and
career workshops.&lt;br/&gt;&lt;br/&gt;Technical:&lt;br/&gt;&lt;br/&gt;The
objective of this project is to develop scalable electrochemical two-dimensional
(2D) synaptic arrays with high-precision and low-power for advanced hardware
acceleration of deep neural networks (DNNs) with orders of magnitude
improvements in energy and speed. While binary SRAM cells have shown promising
performance for DNN hardware acceleration, its inherent limitations in power and
area make it impractical to scale up to the complexity level required for large-
scale problems and/or datasets. In this project, the team will take a holistic
approach to develop scalable electrochemical 2D synaptic arrays with high
precision, lower-power, good linearity, low variations, and CMOS compatibility
for large-scale integration. The team will carry out the following three
research tasks: (1) device-level optimization in device precision, dynamic
range, and scaling; (2) array-level demonstration by building synaptic arrays,
lowering device variations, and designing peripheral circuits; (3) system-level
integration via building device models, implementing computing-in-memory (CIM),
and demonstrating on-chip learning for pixel-to-pixel applications. This work
will provide a low-power and scalable framework for the hardware acceleration of
DNNs, paving the ways towards the ubiquitous use of artificial intelligence (AI)
in both high-performance computers and low-power embedded
systems.&lt;br/&gt;&lt;br/&gt;This award reflects NSF's statutory mission and
has been deemed worthy of support through evaluation using the Foundation's
intellectual merit and broader impacts review criteria.