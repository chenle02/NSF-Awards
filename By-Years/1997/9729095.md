* 9729095
* Time Course of Spoken Word Recognition
* SBE,BCS
* 04/15/1998,03/31/2002
* Richard Aslin, University of Rochester
* Continuing Grant
* Guy Van Orden
* 03/31/2002
* USD 300,000.00

Understanding the mechanisms by which people recognize spoken words in
continuous speech is of central importance for theories of how language is
processed, how it develops, and how it is affected by brain injury. An
understanding of how people recognize words in continuous speech also provides
valuable informative for researchers developing speech recognition technology
and human-computer interaction systems. It is well-established that during
spoken word recognition listeners evaluate the unfolding input by activating a
set of potential lexical candidates which compete for recognition. However,
numerous questions remain about how the set of possibilities is established and
how it is evaluated during real-time processing. For example, little is known
about whether or not people are able to use fine-grained acoustic differences
during initial word recognition. Thus, it is not clear whether as the word
`carpet` is heard in continuous speech, the word recognition system considers
all words that begin with similar sequences of sounds, e.g., (car, card, etc.)
or whether subtle differences in the length of vowels in one-syllable and
polysyllabic words are used to restrict the set of alternatives. Questions like
these have important implications for how we understand and model the word
recognition system. However, our ability to answer these questions has been
limited because few of the experimental methods sensitive spoken word
recognition can be used with continuous speech in natural tasks. This is an
important limitation because natural speech often occurs in noisy conditions,
there is considerable speaker variability, and linguistic units, such as the
beginning and end of a word are not clearly marked in continuous speech. The
proposed research explores how candidate words are retrieved from memory and
evaluated during continuous speech using: (a) experimental studies in English
with digitized natural speech and synthesized speech; (b) computational
modeling; and (c) experimental and computational explorations with artificial
languages. The experiments measure eye-movements to objects in a circumscribed
visual world, extending the methodology pioneered by the PI and his
collaborators. Participants will follow spoken instructions to pick up and move
(with a mouse) line drawings of concrete objects on a computer monitor (e.g.,
`Pick up the candy. Now put it above the circle`.). Preliminary studies have
established that: (a) the pattern and timing of eye-movements are remarkably
sensitive to the uptake of information, allowing for a detailed mapping of the
nature of the candidate set and how it changes over time during continuous
speech; and (b) there is a simple quantitative mapping from hypothesized
underlying speech recognition processes to the probability of making an eye-
movement to a target object, allowing for precise testing of different theories
of word recognition. Moreover, the basic task, either with pictures or real
objects, can be naturally extended for use with infants, young children, and
neurologically-impaired populations. The project should result in both
methodological advances and in a body of empirical data important for scientists
studying normal and impaired language processing, as well as for scientists
developing speech recognition systems.