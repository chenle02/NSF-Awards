* 2313151
* Collaborative Research: RI: Medium: Lie group representation learning for vision
* CSE,IIS
* 10/01/2023,09/30/2026
* Stella Yu, Regents of the University of Michigan - Ann Arbor
* Continuing Grant
* Kenneth Whang
* 09/30/2026
* USD 100,000.00

The quest to build intelligent machines capable of sensing, understanding and
acting in their environment presents one of the great scientific challenges of
our time. Despite recent advances in artificial intelligence (AI), the
realization of robust, autonomous vision systems that understand and interact
with the physical world remains elusive. Mathematically, vision requires
understanding the relationships among an immense variety of object shapes, each
subject to an immense variety of geometric and lighting transformations, leading
to an explosion of possible visual scenes. This project aims to break through
this barrier by developing a mathematically grounded computational theory of
vision that will enable a new class of neural network learning algorithms to
parse visual scenes into their constituent objects and transformations, thereby
enabling computers to better represent the world around them. The results and
computational tools arising from this research will be disseminated to the
scientific community and general public through courses, seminars, hackathons,
and open-source software contributed to the Geomstats library.

The premise of this project is that the current limitations of AI and computer
vision can be addressed with an appropriate mathematical framework, Lie theory,
that models the hierarchical structure of natural transformations in the visual
world. The investigators will develop generalizations of foundational signal
processing transforms through explicit Lie group operations encoded in learnable
G-Modules (Group-Modules). These modules directly tackle the combinatoric
explosion in vision by factorizing images into shapes and their underlying
transformations. Specifically, the team will develop G-modules that learn group-
equivariant representations of the transformations contained in natural images
(Aim 1), robust representations of shape by collapsing group orbits only with
respect to specific transformations (Aim 2), and disentangling of transformation
and shape via factorization (Aim 3). The modules are assembled into hierarchical
architectures that can learn complex representations of transformations and
shapes (Aim 4). Together, these aims provide a new paradigm that grounds
existing models of vision and gives a set of guiding principles for the design
of future deep learning architectures with enhanced abilities to sense and
understand the world.

This award reflects NSF's statutory mission and has been deemed worthy of
support through evaluation using the Foundation's intellectual merit and broader
impacts review criteria.