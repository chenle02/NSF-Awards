* 1447721
* BIGDATA: F: DKA: Collaborative Research: Theory and Algorithms for Parallel Probabilistic Inference with Big Data, via Big Model, in Realistic Distributed Computing Environments
* CSE,IIS
* 09/01/2014,08/31/2018
* Sinead Williamson, University of Texas at Austin
* Standard Grant
* Jie Yang
* 08/31/2018
* USD 300,000.00

This project develops a new framework that enables machine learning (ML) systems
to automatically comprehend and mine massive and complex data via parallel
Bayesian inference on large computer clusters. The research has a profound
impact on the practice and direction of Big Learning. The developed technologies
have a catalytic effect on both ML research and applications: ML scientists are
able to rapidly experiment on novel, cutting-edge ML models with minimal
programming effort, unhindered by the limitations of single machines.
Researchers from other fields, like biology and social sciences, are able to run
contemporary advanced ML methods that transcend the capabilities of simple
models, yielding new scientific insights on data whose size would otherwise be
daunting. Data scientists at small start-ups are able to conduct ML analytics
with complex models, putting their capabilities on par with huge companies
possessing dedicated engineering and infrastructure teams. Students and
beginners are able to witness distributed ML in action with just a few lines of
code, driving ML education to new heights. &lt;br/&gt;&lt;br/&gt;Technically,
this research focuses on scaling up and parallelizing Bayesian machine learning,
which provides a powerful, elegant and theoretically justified framework for
modeling a wide variety of datasets. The research team develops a suite of
complementary distributed inference algorithms for hierarchical Bayesian models,
which cover most commonly used Bayesian ML methods. The project focuses on
combining speed and scalability with theoretical guarantees that allow us to
assess the accuracy of the resulting methods, and allow practitioners to make
trade-offs between speed and accuracy. Rather than focus on a few disconnected
models, the project develops techniques applicable to a broad spectrum of
hierarchical Bayesian models, resulting in a toolkit of building blocks that can
be combined as needed for arbitrary probabilistic models - be they parametric or
nonparametric, discriminative or generative. This is in contrast to much
existing work on parallel inference, which tends to focus on parallelization in
a specific model and cannot be easily extended. The project provides a solid
algorithmic foundation for learning on Big Data with powerful models. The
research contributes to democratizing advanced and large-scale ML methods for
broad applications, by offering the user and developer community a library of
general-purpose parallelizable algorithms for working on diverse problems using
computer clusters and the cloud, bridging the gap between practical needs from
data and basic research in ML.