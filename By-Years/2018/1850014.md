* 1850014
* CRII: SaTC: Moderating Effects of Automation on Information Transmission in Social Forums
* CSE,CNS
* 06/01/2019,05/31/2022
* Jake Williams, Drexel University
* Standard Grant
* Sara Kiesler
* 05/31/2022
* USD 190,770.00

This project aims to develop and deploy an information veracity evaluation
system to support online discourse moderation and human comprehension of online
information. Understanding information's nature can help users to identify
essential products and services, and even potentially help to inform democratic
participation. The project will help individual users navigate an environment
where the views of their online peers may be difficult to interpret, or where it
may not be clear whether other users are human or covert social bots (e.g., for
social engineering to obtain personal information). The research project
develops capacities for identifying social bots and evaluating their stances
towards claims on forums to underpin a system that will provide automated
veracity evaluation of web content and analysis to support forum moderators.
These projects will shed light on the use of social bots at affecting reader
perceptions of information, bringing awareness to dangers of automation in the
infosphere and ultimately, support for vigilance and anticipation of automation
attacks. &lt;br/&gt;&lt;br/&gt;This project aims to produce an open-source,
automated social information veracity evaluation system that can support
moderation of, and ultimately, user navigation in online discourse environments.
This system will be developed with auxiliary foci on detecting social bots and
evaluating social support, making it capable of filtering out covert autonomous
agents while assessing their roles in the support or denial of content claims. A
veracity-annotated dataset of unprecedented size that integrates online content
and associated reader commentary with social bot annotations will be extended
and enriched with reader support annotations. These will be used to develop
machine learning tools that can indicate 1) users' authenticity as human
commenters and 2) their stances towards claims, in order to 3) support the
evaluation of their discussed content's veracity. A platform and server-to-
server applications will be built to support implementation for forum
moderation, providing moderators live analytics, alerts, and an interactive
visual dashboard, in addition to a public display of summarized
results.&lt;br/&gt;&lt;br/&gt;This award reflects NSF's statutory mission and
has been deemed worthy of support through evaluation using the Foundation's
intellectual merit and broader impacts review criteria.