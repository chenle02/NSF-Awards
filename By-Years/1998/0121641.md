* 0121641
* ITR/IM: Capturing, Coordinating and Remembering Human Experience
* CSE,IIS
* 10/01/2001,09/30/2005
* Takeo Kanade, Carnegie-Mellon University
* Continuing Grant
* Stephen Griffin
* 09/30/2005
* USD 2,700,000.00

This work will develop algorithms and systems enabling people&lt;br/&gt;to query
and communicate a synthesized record of&lt;br/&gt;human experiences derived from
individual perspectives captured&lt;br/&gt;during selected personal and group
activities. For this research,&lt;br/&gt;an experience is defined through what
you see, what you hear,&lt;br/&gt;where you are, and associated sensor data and
electronic communications.&lt;br/&gt;The research will transform this record
into a meaningful, accessible&lt;br/&gt;information resource, available
contemporaneously and retrospectively.&lt;br/&gt;We will validate our vision
with two societally relevant applications:&lt;br/&gt;(1) providing memory aids
as a personal prosthetic or behavioral&lt;br/&gt;monitor for the elderly; and
(2) coordinating emergency response&lt;br/&gt;activity in disaster
scenarios.&lt;br/&gt;&lt;br/&gt;This project assumes that within ten years
technology will be capable&lt;br/&gt;of creating a continuously recorded,
digital, high fidelity record of&lt;br/&gt;a person's activities and
observations in video form. This research&lt;br/&gt;will prototype personal
experience capture units to record audio, video,&lt;br/&gt;location and sensory
data, and electronic communications. Each constituent&lt;br/&gt;unit captures,
manages, secures and associates information from&lt;br/&gt;its unique point of
view. Each operates as a portable, interoperable,&lt;br/&gt;information system,
allowing search and retrieval by both its&lt;br/&gt;human operator and remote
collaborating systems. An individual&lt;br/&gt;cannot see everything, nor
remember everything that was seen or&lt;br/&gt;heard. The integration of
multiple points of view provides more&lt;br/&gt;comprehensive coverage of an
event, especially when coupled with support&lt;br/&gt;for vastly improving the
memory from each perspective. The research thus&lt;br/&gt;enables the following
technological advances:&lt;br/&gt;&lt;br/&gt;* Enhanced memory for individuals
from an intelligent assistant using an&lt;br/&gt;automatically analyzed and
fully indexed archive of captured
personal&lt;br/&gt;experiences.&lt;br/&gt;&lt;br/&gt;* Coordination of
distributed group activity, such as management of an&lt;br/&gt;emergency
response team in a disaster relief situation, utilizing
multiple&lt;br/&gt;synchronized streams of incoming observation data to
construct a "collective&lt;br/&gt;experience."&lt;br/&gt;&lt;br/&gt;* Expertise
synthesized across individuals and maintained over
generations,&lt;br/&gt;retrieved and summarized on demand to enable example-
based training and&lt;br/&gt;retrospective analysis.&lt;br/&gt;&lt;br/&gt;*
Understanding of privacy, security and other societal implications
of&lt;br/&gt;ubiquitous experience collection.&lt;br/&gt;&lt;br/&gt;The
foundation for this work, the Informedia Digital Video Library,&lt;br/&gt;has
demonstrated the successful application of speech, image, and&lt;br/&gt;natural
language processing in automatically creating a rich,
indexed,&lt;br/&gt;searchable multimedia information resource for broadcast-
quality video.&lt;br/&gt;The proposed work builds from these technologies,
moving well beyond&lt;br/&gt;a digital video library into new information spaces
composed of&lt;br/&gt;unedited personal experience video augmented with
additional sensory&lt;br/&gt;and position data. Tools will be created to analyze
large amounts&lt;br/&gt;of continuously captured digital experience data in
order to extract&lt;br/&gt;salient features, describe scenes and characterize
events. The&lt;br/&gt;research will address summarization and collaboration of
multiple&lt;br/&gt;simultaneous experiences integrated across time, space and
people.