* 1815303
* SHF: Small: Using Software Defined Cache to Accelerate Index Search for In-memory Applications: Software and Hardware Approaches
* CSE,CCF
* 10/01/2018,09/30/2022
* Song Jiang, University of Texas at Arlington
* Standard Grant
* Danella Zhao
* 09/30/2022
* USD 369,000.00

Memory is one of the most important components in large-scale data centers. As
many software systems for big data processing keep their data sets entirely in
memory to enable high-performance in-memory computing, memory efficiency becomes
critical to application performance. While the applications, such as database
systems and big data analytics, often serve as an infrastructure for information
processing and providing IT services for millions of people in our society,
improvement of their performance via optimization of the memory access is of
great importance and impact. As memory access is slow compared with processor
and cache speeds, the project eliminates unnecessary memory accesses with a re-
designed cache architecture supporting flexible access and efficient management.
In addition, this project provides research training to both undergraduate and
graduate students, especially under-represented minority students, to prepare
them to be future information technology professionals with strong skills in
computer architecture and system areas. &lt;br/&gt;&lt;br/&gt;In memory-
intensive computing, a significant percentage of memory access is spent on
indices for translating user-defined keys into memory addresses for data
accessing. However, due to lack of temporal and spatial localities, it can be
very difficult to cache the indices and receive high cache-hit ratio.
Accordingly, searching of the indexes is often at the memory speed, and
searching for a data item may require multiple memory accesses. This project
designs a software-defined cache -- an informed use of processor cache where a
user program can explicitly specify data items for caching with their defined
keys. As a two-phase effort, the project adopts a software approach, in which it
is presented as a user-level library managing a look-aside buffer implicitly
mapped into the cache, and a hardware approach, in which keys are explicitly
hashed into the cache. Both approaches well exploit access locality and perform
index search at the cache speed with their respective unique advantages.
Accordingly, performance of memory system and memory-intensive applications can
be significantly improved.&lt;br/&gt;&lt;br/&gt;This award reflects NSF's
statutory mission and has been deemed worthy of support through evaluation using
the Foundation's intellectual merit and broader impacts review criteria.