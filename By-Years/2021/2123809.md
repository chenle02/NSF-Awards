* 2123809
* Collaborative Research: SCH: Trustworthy and Explainable AI for Neurodegenerative Diseases
* CSE,IIS
* 10/01/2021,09/30/2025
* My Thai, University of Florida
* Standard Grant
* Sylvia Spengler
* 09/30/2025
* USD 840,000.00

Driven by its performance accuracy, machine learning (ML) has been used
extensively for various applications in the healthcare domain. Despite its
promising performance, researchers and the public have grown alarmed by two
unsettling deficiencies of these otherwise useful and powerful models. First,
there is a lack of trustworthiness - ML models are prone to interference or
deception and exhibit erratic behaviors when in action dealing with unseen data,
despite good practice during the training phase. Second, there is a lack of
interpretability - ML models have been described as 'black-boxes' because there
is little explanation for why the models make the predictions they do. This has
called into question the applicability of ML to decision-making in critical
scenarios such as image-based disease diagnostics or medical treatment
recommendation. The ultimate goal of this project is to develop computational
foundation for trustworthy and explainable Artificial Intelligence (AI), and
offer a low-cost and non-invasive ML-based approach to early diagnosis of
neurodegenerative diseases. In particular, the project aims to develop
computational theories, ML algorithms, and prototype systems. The project
includes developing principled solutions to trustworthy ML and making the ML
prediction process transparent to end-users. The later will focus on explaining
how and why an ML model makes such a prediction, while dissecting its underlying
structure for deeper understanding. The proposed models are further extended to
a multi-modal and spatial-temporal framework, an important aspect of applying ML
models to healthcare. A verification framework with end-users is defined, which
will further enhance the trustworthiness of the prototype systems. This project
will benefit a variety of high-impact AI-based applications in terms of their
explainability, trustworthy, and verifiability. It not only advances the
research fronts of deep learning and AI, but also supports transformations in
diagnosing neurodegenerative diseases. &lt;br/&gt;&lt;br/&gt;This project will
develop the computational foundation for trustworthy and explainable AI with
several innovations. First, the project will systematically study the
trustworthiness of ML systems. This will be measured by novel metrics such as,
adversarial robustness and semantic saliency, and will be carried out to
establish the theoretical basis and practical limits of trustworthiness of ML
algorithms. Second, the project provides a paradigm shift for explainable AI,
explaining how and why a ML model makes its prediction, moving away from ad-hoc
explanations (i.e. what features are important to the prediction). A proof-based
approach, which probes all the hidden layers of a given model to identify
critical layers and neurons involved in a prediction from a local point of view,
will be devised. Third, a verification framework, where users can verify the
model's performance and explanations with proofs, will be designed to further
enhance the trustworthiness of the system. Finally, the project also advances
the frontier of neurodegenerative diseases early diagnosis from multimodal
imaging and longitudinal data by: (i) identifying retinal vasculature biomarkers
using proof-based probing in biomarker graph networks; (ii) connecting
biomarkers of the retina and the brain vasculature via cross- modality
explainable AI model; and, (iii) recognizing the longitudinal trajectory of
vasculature biomarkers via a spatio-temporal recurrent explainable model. This
synergistic effort between computer science and medicine will enable a wide
range of applications to trustworthy and explainable AI for healthcare. The
results of this project will be assimilated into the courses and summer programs
that the research team have developed with specially designed projects to train
students with trustworthy and explainable AI.&lt;br/&gt;&lt;br/&gt;This award
reflects NSF's statutory mission and has been deemed worthy of support through
evaluation using the Foundation's intellectual merit and broader impacts review
criteria.