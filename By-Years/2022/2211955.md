* 2211955
* Collaborative Research: RI: Medium: Expert-in-the-Loop Neural Summarization for Consequential Domains
* CSE,IIS
* 07/01/2022,06/30/2026
* Zachary Lipton, Carnegie-Mellon University
* Standard Grant
* Tatiana Korelsky
* 06/30/2026
* USD 583,746.00

Automatic summarization methods aim to create shortened versions of texts (for
example, news or scientific articles) that still accurately communicate their
main points. Summarization methods provide a potential means to counteract the
problem of "information overload" which is prevalent across many areas. But much
of the research on automatic summarization has focussed largely on just one type
of data: news articles. This is not because summarizing news articles is seen as
particularly important. Rather, it is a result of there being conveniently
available large datasets that can be used to "train" machine learning models to
perform summarization. However, the resultant focus on approaches that assume a
setting in which one has access to large volumes of "training data" to use to
train summarization models has warped research priorities; little work has been
done on investigating how automatic summarization methods might be used in
important but specialized domains such as medicine or law. In these kinds of
areas one is unlikely to have access to a massive dataset of manually written
summaries. Furthermore, domain experts in such areas are not likely to blindly
trust a system-generated summary (nor should they). This motivates a need for
transparency with respect to how the model generated a particular summary, and
for approaches that permit the expert to interact with the model more generally.
&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;This project aims to address these issues by
investigating and extending the capabilities of modern, pre-trained, neural
summarization models in the context of domains and tasks in which one has
limited explicit supervision, and where there is a heightened need for factually
accurate summaries. The project will involve critically evaluating state-of-the-
art models when fine-tuned for summarization in domains like medicine under
limited supervision; a specific aim is to characterize their behavior with
respect to the factuality of model outputs. The idea is then to extend these
models to permit interactive and efficient supervision, via active learning
methods, alternative types of supervision (e.g., expert "highlights"), and novel
pre-training objectives. Finally, the investigators will design architectures
that afford increased transparency and controllability; this will be
accomplished using latent variable summarization models, which will in turn
allow one to inspect which input segments informed particular outputs. This will
provide a natural means for the end-user (domain expert) to verify model
outputs, and it will also provide a means to "debug" summarization systems. The
hope is that these technical innovations will allow domain experts to benefit
from automated summarization technology.&lt;br/&gt;&lt;br/&gt;This award
reflects NSF's statutory mission and has been deemed worthy of support through
evaluation using the Foundation's intellectual merit and broader impacts review
criteria.