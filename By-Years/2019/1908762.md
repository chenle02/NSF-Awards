* 1908762
* SHF:Small: Mega-Transfer: On the Value of Learning from 10,000+ Software Projects
* CSE,CCF
* 10/01/2019,09/30/2024
* Timothy Menzies, North Carolina State University
* Standard Grant
* Sol Greenspan
* 09/30/2024
* USD 472,024.00

As more and more software and software-development artifacts have accumulated in
large repositories, Software Engineering has become a Big Data Science. Software
analytics is a subfield of Software Engineering in which the software-project
data is analyzed to gain actionable information that helps practitioners track
and improve software-development practices. Research in software analytics seeks
to develop better models, methods and tools to support, for example, software-
defect prediction, using software production and programmer activity data. The
first decade of software analytics has generated specific results about specific
projects, but it remains difficult to generalize across many projects so that
results on one project can be applied to other projects. As in any big-data
science, a central question is how much data, and of what quality, is needed to
draw valid and useful conclusions that can generalize across multiple projects.
A typical research study in software analytics looks at no more than a few dozen
projects, which limits the validity and usefulness of results in the field. The
project will explore the sufficiency and limits of data for transfer learning
across domains. The ultimate goal is to scale software analytics, particularly
transfer learning of nine extensively studied software-analytics tasks, to
10,000+ open-source projects and 1,500+ commercial projects. The result is
software-quality prediction models, backed by large-scale quantitative studies,
that can guide the development and maintenance of software, thus reducing the
overall cost of creating and maintaining software.&lt;br/&gt;&lt;br/&gt;The
technical challenges of this work include scalability of the algorithms and the
ability to transfer lessons learned between projects. To address these
challenges this project will develop innovative transfer-learning methods based
on fast linear-time clustering algorithms and fast stream-mining algorithms that
use incremental hyper-parameter optimization and clustering. The techniques will
find similar projects in instance space (within each cluster) in order to find
candidates for transfer. Further, where possible, this project will reduce the
transfer cost by pre-studies that perform feature selection on the source and
target data. Where it is found that software knowledge is highly localized, then
this project will also develop and deploy optimization methods for large scale
instance-based learning methods.&lt;br/&gt;&lt;br/&gt;This award reflects NSF's
statutory mission and has been deemed worthy of support through evaluation using
the Foundation's intellectual merit and broader impacts review criteria.