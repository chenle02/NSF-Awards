* 1650888
* Prediction during Processing of Repairs in Spoken Language
* SBE,BCS
* 06/15/2017,11/30/2021
* Fernanda Ferreira, University of California-Davis
* Standard Grant
* Tyler Kendall
* 11/30/2021
* USD 246,276.00

Everyone is familiar with the experience of being disfluent. Despite their best
efforts, on average speakers will produce a filler such as "uh" or an "um" every
twenty words, and they may also make a speech error which will need to be
repaired. Previous research has established some of the causes of disfluency and
has revealed that disfluencies of different types characterize different types
of speakers; for example, individuals with ADHD are more likely to produce
speech errors than to use fillers, and those without ADHD show the opposite
pattern. Far less is known, however, about how listeners are able to understand
speakers despite the presence of this noise in the linguistic signal. Early
proposals tested the hypothesis that listeners must somehow ignore disfluencies,
but more recent studies show that disfluencies are only partially suppressed,
indicating that disfluencies affect how listeners interpret the sentence they
hear and even how they evaluate the speaker. In addition, these newer
experiments show that when listeners hear a word spoken in error, they use the
error to predict what the speaker is likely to say instead. This prediction
mechanism is helpful for two reasons: first, because it allows the listener to
get a head start on processing the speaker's intended meaning; and second,
because it helps the listener come up with a more sensible interpretation of the
utterance should the speaker fail to detect and correct his or her error.
Understanding how these prediction mechanisms operate is especially relevant for
our understanding of language and aging; speakers are known to become more
disfluent as they age, making their speech harder to understand. This is a
pressing concern given the aging population of the United States. This work will
also help enhance speech recognition devices that must be robust to disfluency
if they are to operate on natural, spontaneous speech. Devices that respond to
voice commands are now in millions of Americans' homes and pockets, and as they
become more common, users will increasingly come to expect them to work smoothly
and reliably. &lt;br/&gt;&lt;br/&gt;The experiments that will be conducted for
this project use two complementary methods for assessing people's comprehension
of speech on a millisecond-by-millisecond basis: recording of brain
electrophysiological activity (EEG), and recording of eye movements to visual
displays presented during listening tasks. The experiments are designed to
answer three core questions about prediction during processing of disfluencies:
(1) When do listeners begin to predict? (2) What precisely is the content of the
prediction (a specific word, a general category)? (3) What is the fate of an
incorrect prediction? That is, given that listeners' expectations will not
always match the speaker's output, how do listeners reconcile their prediction
with any discrepant content? This project will involve students who will be
trained in experimental psycholinguistics, statistics, and computational
methods, allowing them to gain experience in designing and interpreting data, as
well as in preparing scientific reports for presentation and publication.