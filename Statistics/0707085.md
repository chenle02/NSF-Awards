
* 0707085
* New Methodology for Multiple Testing and Simultaneous Inference
* DMS,STATISTICS
* 07/01/2007,05/08/2009
* Joseph Romano,CA,Stanford University
* Continuing Grant
* Gabor Szekely
* 12/31/2011
* USD 262,402.00

The investigator develops new methods and theory for problems in multiple
testing and simultaneous inference. A classical approach to dealing with
multiplicity is to require that decision rules control the familywise error
rate. But, when the number of tests is large, this measure is so stringent that
alternative hypotheses have little chance of being detected. Thus, alternative
measures of error control are studied both in finite sample and asymptotically.
Such measures include: the false discovery rate; the probability of k or more
false rejections; tail probabilities of the false discovery proportion. In order
to develop methods which do not rely on unrealistic or unverifiable model
assumptions, the investigator makes extensive use of computer-intensive methods.
The power of resampling is that the joint dependence structure of the individual
tests can be captured so that methods need not be overly conservative. The
pursuit of such methodology is investigated from theoretical, computational and
theoretical points of view, with special emphasis on a large number of tests.

The goal of this research is to develop new theory and methods for problems of
multiple inference. Virtually any scientific experiment sets out to answer
questions about the process under investigation, which often can be translated
formally into a set of hypotheses to be tested. It is the exception that only a
single hypothesis or question is under study. In the "information age", the
statistician is faced with the challenge of accounting for all possible errors
resulting from a complex data analysis, so that any interesting conclusions can
reliably be viewed as real structure rather than the result of "data snooping",
i.e. finding artifacts of random data. For example, current methods in
biotechnology and genomics generate DNA microarray experiments, where gene
expression level in cells for thousands of genes are analyzed simultaneously on
a gene by gene basis. The goal then is to devise new techniques that are not
based on strong assumptions that effectively deal with problems of multiplicity
in the face of vast amounts of data. The resulting inferential tools can be
applied to such diverse fields as genetics, econometrics, finance, brain
imaging, clinical trials, education and astronomy.
