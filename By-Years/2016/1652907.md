* 1652907
* CAREER: A New Interaction Model for Eyes-Free Exploration of Touch Screens
* CSE,IIS
* 03/01/2017,02/28/2023
* Shaun Kane, University of Colorado at Boulder
* Continuing Grant
* Ephraim Glinert
* 02/28/2023
* USD 550,000.00

Touch-based user interfaces, which have become ubiquitous in personal computers
and mobile devices, typically offer only limited accessibility to blind and
visually impaired users because they rely heavily upon visual interaction and
provide little if any audio or haptic feedback. This creates an accessibility
barrier that affects millions of people in the United States. The problem is
compounded by the interaction model supported by most touch interfaces, which
assumes that a blind user will explore via a single finger and interact with
only one on-screen item at a time; as a consequence, blind users cannot easily
browse or skim through large amounts of information or interact with on-screen
content using gestures. These problems are further exacerbated by a lack of
coordination between the touch input framework and the speech output engine.
This research will substantially improve the accessibility and usability of
touch-based interfaces for blind users through the creation of a new model that
supports robust eyes-free interaction with touch screen applications and
tangible user interfaces, which will significantly improve touch exploration of
complex data by blind users and also by sighted people (e.g. to avoid
distractions while driving). Students with disabilities will be recruited as
members of the research team. Additional broad impact will be achieved through
educational activities that engage computing students in the design of
accessible user interfaces; to this end, modules on universal design of eyes-
free user interfaces for courses on user-centered design and tangible computing
will be created. Outreach activities will include a series of inclusive design
workshops in which persons with disabilities will be invited to
participate.&lt;br/&gt;&lt;br/&gt;The research will proceed in several stages.
First, to identify benchmarks and metrics for natural eyes-free exploration,
formative studies will be conducted with blind individuals and teachers of the
visually impaired. A new interaction model that supports eyes-free exploration
of, and interaction with, touch screen user interfaces will then be developed
that supports the hand poses, movements, gestures and navigation strategies
preferred by blind users, and will be implemented as an input framework that can
be embedded in existing touch-based applications. As part of this work, new
gesture recognizers and a new text-to-speech engine will be developed and
evaluated. Finally, authoring tools that enable scientists, artists, teachers,
and others to adapt their existing data to support non-visual touch interaction
with both touch screens and 3D-printed tactile graphics will be developed
through iterative testing with blind and sighted users, and their teachers, and
through the public deployment of reference applications including an interactive
map for learning public transit routes in a large urban area and a tool for
exploring interactive museum exhibits.