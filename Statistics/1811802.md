
* 1811802
* Collaborative Research: New Developments in Direct Probabilistic Inference on Interest Parameters
* DMS,STATISTICS
* 08/01/2018,05/11/2018
* Ryan Martin,NC,North Carolina State University
* Standard Grant
* Gabor Szekely
* 07/31/2021
* USD 199,464.00

The Bayesian approach to statistical learning relies on probabilistic models for
all observables and unknowns. The need to model all aspects of the problem can
restrict the scope of applications and, more generally, can be a burden to the
data analyst who is often only interested in certain features of the unknowns.
This project will develop a mathematically rigorous and computationally
efficient framework in which Bayesian learning can be carried out directly in
terms of only the features of interest. This reduces the modeling and
computational burden on the data analyst and provides new insights about
Bayesian learning more generally

A Bayesian approach is a powerful and rigorous framework for statistical
learning. The downside is that it requires a full model for the observables as
well as all unknown quantities, the specification of which can be a burden on
the data analyst. In addition to the familiar challenges of prior specification,
there are also risks of misspecification biases. A more subtle complication is
due to selection effects that result from considering several candidate models.
The data analyst's burden is further exaggerated in situations where only a
feature of the unknowns is of interest, i.e., when there is an interest
parameter and a (potentially high-dimensional) nuisance parameter and inference
is required only for the former. That is, the Bayesian approach still requires
that the data analyst make non-trivial efforts to specify prior distributions
and carry out posterior computations relevant only to the nuisance parameter,
which can be viewed as a waste. Yet having access to a posterior distribution
for inference on the interest parameter is still a desirable feature, and the
proposed research aims to develop a new framework for posterior inference
directly on interest parameters. These direct posteriors (DiPs) effectively
target the interest parameter, giving data analysts an opportunity to avoid the
seemingly wasteful modeling and computation efforts involving nuisance
parameters. This project will construct DiPs for finite- and infinite-
dimensional interest parameters with rigorous theoretical guarantees, and will
also develop efficient computational tools to facilitate DiP-based inference.

This award reflects NSF's statutory mission and has been deemed worthy of
support through evaluation using the Foundation's intellectual merit and broader
impacts review criteria.
