* 1320835
* AF:Small: Foundations of Transactional Memory Scheduling
* CSE,CCF
* 09/01/2013,08/31/2018
* Konstantin Busch, Louisiana State University
* Standard Grant
* Tracy Kimbrel
* 08/31/2018
* USD 365,897.00

Multi-core computer architectures offer unprecedented performance benefits and
present new challenges for the efficient synchronization of concurrent
computations. Transactional memory is a prominent programming model that
simplifies the synchronization of shared memory accesses, and avoids the
complications of fine-grained locking mechanisms. A memory transaction
represents a sequence of (read/write) shared memory operations that need to be
performed atomically by a computation thread. A transaction either commits, or
aborts in case of conflicts with other transactions that concurrently access the
same shared resources. This project aims to design, develop, and analyze
contention managers that schedule efficiently memory transactions in a variety
of systems. The goal is to provide schedulers that have provable formal
performance guarantees and at the same time are practically efficient; thus,
bridging the gap between theory and practice that currently appears in the
literature.&lt;br/&gt;&lt;br/&gt;The project considers a wide range of
distributed systems, including tightly-coupled systems such as multi-core
processors, and larger scale systems such as distributed networked processors.
One of the main objectives is to provide scheduling algorithms which scale
gracefully with the various system sizes and complexities. In order to fulfill
this objective, this work proposes new analytical techniques to obtain good
formal bounds with appropriate performance metrics, and also conducts
experimental evaluations in real world workloads to obtain good performance in
practical scenarios. The project establishes foundations for investigating the
performance of transactional memory systems, and also provides analytical tools
to the research community for exploring transactional memory to its full
potential. The proposed research impacts the larger computing community because
it affects the efficiency of distributed and parallel programs running on widely
used distributed and multi-core systems.