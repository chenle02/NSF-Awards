* 2241818
* CNS Core: Small: Transparent Network Acceleration
* CSE,CNS
* 05/15/2023,04/30/2026
* Eric Keller, University of Colorado at Boulder
* Standard Grant
* Darleen Fisher
* 04/30/2026
* USD 599,928.00

With the increase in connected devices comes an increase in network traffic.
Applications that we use every day, such as video conferencing, each need to
have traffic processed to secure and optimize the application. Current network
infrastructure is showing signs that in the near future it won't be able to keep
up with the growing demand or increased needs, such as having higher definition
video calls with more participants and imperceptible delays in the
communication. While some solutions have been proposed to help solve this
problem, they do so at the cost of compatibility, which would require rewriting
a large body of software that has been developed and tested over the course of
over a decade. This proposal introduces a new way of thinking about the way
network traffic is handled that will enable both forward and backwards
compatibility with modern software while meeting the needs of
applications.&lt;br/&gt;&lt;br/&gt;In particular, this proposal introduces
Transparent Network Acceleration (TNA), which is a novel architecture that
decomposes Linux network functionality into fast-path and slow-path functions
with explicitly optimized execution environments for each. TNA dynamically and
automatically builds a minimal fast path that is instantiated and adjusted at
run-time, leading to a transparently accelerated networking stack that fully
retains the Linux networking interfaces. Two core mechanisms are introduced.
First, to create a highly efficient fast path, TNA automatically and dynamically
instantiates only the part of the network stack that is used. New techniques are
introduced to introspect the Linux kernel, build a dependency graph of
functions, and assemble and deploy an optimal fast path. Second, it targets two
fast-path execution environments from a single source code description of
modules: (i) the eXpress Data Path (XDP), for in-kernel processing, and (ii) a
field programmable gate array (FPGA) based SmartNIC, for hardware accelerated
processing. As part of these targets, the design of modules, the compilation to
hardware or software, and synchronizing the state represent a novel and complete
design flow. In order to demonstrate the effectiveness of these two core
mechanisms, we complete the project with a case study of automatically
accelerating a selective forwarding unit (SFU) video conferencing network
function deployed with considerations such as firewalling and container
networking.&lt;br/&gt;&lt;br/&gt;This award reflects NSF's statutory mission and
has been deemed worthy of support through evaluation using the Foundation's
intellectual merit and broader impacts review criteria.