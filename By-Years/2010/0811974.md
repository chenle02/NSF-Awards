* 0811974
* RI-Small: Unsupervised Learning of Meaning
* CSE,IIS
* 09/01/2008,08/31/2012
* Christopher Manning, Stanford University
* Standard Grant
* Tatiana Korelsky
* 08/31/2012
* USD 450,000.00

The goal of this research to to develop a new generation of robust,
highly&lt;br/&gt;effective unsupervised and semi-supervised models of
meaning&lt;br/&gt;extraction. Our key methological insights include the use
of&lt;br/&gt;global methods of inference to simultaneously consider many
linguistic&lt;br/&gt;aspects of the task, the use of rich typed lexical
dependencies,&lt;br/&gt;and the semi-supervised use of structured data from the
web, such&lt;br/&gt;as dictionaries, thesauruses, encyclopedias, and so on. We
are&lt;br/&gt;extracting meaning at three levels: word meaning,
propositional&lt;br/&gt;meaning, and conceptual meaning. At the word level, we
are learning&lt;br/&gt;lexical relations like hyponymy (leptin is-a hormone),
synonymy,&lt;br/&gt;and others both from raw text and from structured sources
like on-line&lt;br/&gt;dictionaries. At the propositional level, we are
learning&lt;br/&gt;predicate-argument structure using a global unsupervised
clustering&lt;br/&gt;model as well as developing semi-supervised methods of
learning semantic frame&lt;br/&gt;extractors. At a larger structural level we
are inducing scripts&lt;br/&gt;and structured narrative relations between verbs.
These rich models&lt;br/&gt;of meaning will have a broader impact by providing a
critical step&lt;br/&gt;towards the creation of systems with true language
understanding&lt;br/&gt;capabilities. The results could thus impact the creation
of&lt;br/&gt;natural-language applications in every field, from educational
or&lt;br/&gt;tutorial applications, to information extraction tasks like
legal&lt;br/&gt;discovery, to conversational agents. All deliverables of
this&lt;br/&gt;project will be available on the web: WordNet expansions,
induced&lt;br/&gt;frames and scripts, our temporal event classifier, and
semantic&lt;br/&gt;role, frame, and script
inducers.&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;