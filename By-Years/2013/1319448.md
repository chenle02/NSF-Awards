* 1319448
* SHF: Small: Embedded Graph Software-Hardware Models and Maps for Scalable Sparse Computations
* CSE,CCF
* 08/01/2013,01/31/2017
* Padma Raghavan, Pennsylvania State Univ University Park
* Standard Grant
* Almadena Chtchelkanova
* 01/31/2017
* USD 424,999.00

A large number of "big data" and "big simulation" applications, such as those
for determining network models or simulations of partial differential equation
models, concern high dimensional data that are sparse. Sparse data structures
and algorithms present significant advantages in terms of storage and
computational costs. However, with only a few operations per data element,
efficient and scalable implementations are difficult to achieve on current and
emerging high performance computing systems with very high degrees of core level
parallelism, complex node interconnect topology and multicore/manycore nodes
with non-uniform memory architectures (NUMA). This proposal develops and
evaluates 치-embedded graph hardware-software models and attendant data locality-
preserving and NUMA-aware application to core/thread mappings to enhance
performance and parallel scalability. &lt;br/&gt;Consider an application task
graph A, weighted with measures of work and data sharing that is approximately
embedded in two or three dimensions, to obtain an 치-embedded graph A.
Additionally, consider a weighted graph of a HPC system that is naturally
assigned coordinates to obtain an 치-embedded host graph model H. This proposal
develops parallel algorithms to compute interconnect topology-aware mappings of
A to H in order to optimize performance measures such as congestion and dilation
while preserving load balance. Additionally, at a multicore node in H that is
assigned a subgraph of A, (i) sparse data are reordered to enhance parallelism
and locality, and (ii) a dynamic fine-grain NUMA-aware task scheduling is
applied to respond through work-stealing to core variations in performance from
resource conflicts, throttling etc. Finally, through insights gained from
치-embedded graph models, sparse matrix algorithms are reformulated to enhance
communication avoidance, soft error resilience and data preconditioning.
Outcomes include enabling weak scaling to a very large number of cores by
extracting parallelism at fine, medium and large-grains, and significantly
enhanced fixed and scaled problem efficiencies through locality preservation.
&lt;br/&gt;The interconnect topology-aware models and maps hold the potential
for impact on very large scale HPC workloads through potential incorporation
into the Message Passing Interface for enhanced sparse communications.
Additionally, the proposed locality-aware mappings and NUMA-aware scheduling can
potentially benefit the very large base of modeling and simulation applications
that run on small multicore clusters. Graduate student training is enhanced
through a "scale-up" challenge component in an interdisciplinary course on
computational science and engineering. High school students are introduced to
parallel computing through summer in-residence programs seeking to broaden
participation in science and engineering from underrepresented communities.