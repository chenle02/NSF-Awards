* 2104709
* CDSE: Collaborative: Cyber Infrastructure to Enable Computer Vision Applications at the Edge Using Automated Contextual Analysis
* CSE,OAC
* 09/01/2021,08/31/2024
* Yung-Hsiang Lu, Purdue University
* Standard Grant
* Seung-Jong Park
* 08/31/2024
* USD 269,999.00

Digital cameras are deployed as network edge devices, gathering visual data for
such tasks as autonomous driving, traffic analysis, and wildlife observation.
Analyzing the vast amount of visual data is a challenge. Existing computer
vision methods require fast computers that are beyond the computational
capabilities of many edge devices. This project aims to improve the efficiency
of computer vision methods so that they can run on battery-powered edge devices.
Based on the visual data and complementary metadata (e.g., geographical
location, local time), the project first extracts contextual information (such
as a city street is expected to be busy at rush hour). The contextual
information can help assist determine whether analysis results are correct. For
example, a wild animal is not expected on a city street. Moreover, contextual
information can improve efficiency. Only certain pixels need to be analyzed
(pixels on the road are useful for detecting cars, while pixels in the sky are
not) and this can significantly reduce the amount of computation, thus enabling
analysis on edge devices. This project constructs a cyberinfrastructure for
three services: (1) understand contextual information to reduce the search space
of analysis methods, (2) reduce computation by considering only necessary
pixels, and (3) automate evaluation of analysis results based on the contextual
information without human effort.&lt;br/&gt;&lt;br/&gt;Understanding contextual
information is achieved by using background segmentation, GPS-location-dependent
logic, and image depth maps. Background analysis leverages semantic segmentation
and analysis over time to identify the background pixels and then generate
inference rules via a background-implies-foreground relationship. If a pixel is
consistently marked by the same semantic label across a long period of time,
this pixel is classified as a background pixel. The background information can
infer certain types of foreground objects. For example, if the background is
city streets, the foreground objects can be vehicles or pedestrians; if a bison
is detected, this is likely a mistake. This project processes only the
foreground pixels by adding masks to the neural network layers. Masking
convolution can substantially reduce the amount of computation with no loss of
accuracy and no additional training is needed. Meanwhile, hierarchical neural
networks can skip sections of a model based on context. For example, pixels in
the sky only need to be processed by the hierarchy nodes that classify
airplanes. The project provides an online service that can accept input data and
analysis programs for automatic evaluation of the programs, without human
created labels. The evaluation is based on the correlations of background and
foreground objects.&lt;br/&gt;&lt;br/&gt;This award reflects NSF's statutory
mission and has been deemed worthy of support through evaluation using the
Foundation's intellectual merit and broader impacts review criteria.