* 0208289
* Time Complexity Limits for Shared-Memory Synchronization
* CSE,CCF
* 09/01/2002,08/31/2005
* James Anderson, University of North Carolina at Chapel Hill
* Standard Grant
* Kathleen M. O'Hara
* 08/31/2005
* USD 107,497.00

Through the years, much work has been done on synchronization algorithms
for&lt;br/&gt;shared-memory multiprocessors. In contrast, very little work has
been done on&lt;br/&gt;time-complexity lower bounds that express fundamental
limits to which such&lt;br/&gt;algorithms are subject. Given the vastness of the
literature on&lt;br/&gt;synchronization, this may seem somewhat surprising.
However, there is a&lt;br/&gt;simple explanation: while devising a useful time
complexity measure for&lt;br/&gt;sequential algorithms is straightforward, it is
not altogether obvious how to&lt;br/&gt;do this in a meaningful way for
synchronization algorithms. Indeed, in most&lt;br/&gt;synchronization
algorithms, processes may wait unboundedly; thus, if one
merely&lt;br/&gt;applies the standard sequential measure of counting all
operations to such an&lt;br/&gt;algorithm, then its time complexity is
unbounded. This is not a very
useful&lt;br/&gt;statistic.&lt;br/&gt;&lt;br/&gt;Recently, some progress has
been made towards defining useful time complexity&lt;br/&gt;measures. One such
measure is the the remote memory references (RMR) measure.&lt;br/&gt;Under the
RMR measure, a distinction is made between local and remote
accesses&lt;br/&gt;of shared memory. An access is local if it does not require a
traversal of&lt;br/&gt;the global interconnect between processors and shared
memory, and is&lt;br/&gt;remote otherwise. The RMR measure was motived by
research on "local-spin"&lt;br/&gt;synchronization algorithms. In such
algorithms, processes are structured so&lt;br/&gt;that all busy waiting is on
variables cached locally or stored in a local&lt;br/&gt;memory
module.&lt;br/&gt;&lt;br/&gt;When studying synchronization problems, the
following key question arises:&lt;br/&gt;Using some class C of synchronization
primitives, what is the most&lt;br/&gt;efficient possible algorithm for solving
a given synchronization problem?&lt;br/&gt;It is this basic question to which
this research project is directed, where&lt;br/&gt;"efficiency" is defined as
time complexity under the RMR measure. The&lt;br/&gt;research agenda includes
work on both algorithms and lower bounds. Based&lt;br/&gt;on such work, rankings
of synchronization primitives are being developed&lt;br/&gt;that order
synchronization primitives according to the time complexity&lt;br/&gt;(worst-
case, average-case, amortized) with which various
synchronization&lt;br/&gt;problems can be solved. Such rankings should be of
value to computer&lt;br/&gt;architects. Indeed, preliminary research has shown
that a variety of&lt;br/&gt;fetch-and-phi primitives are more powerful than
comparison primitives for&lt;br/&gt;implementing blocking synchronization
mechanisms. This stands in contrast&lt;br/&gt;to the fact that compare-and-swap
and related comparison primitives are&lt;br/&gt;commonly regarded to be among
the most powerful and useful of primitives,&lt;br/&gt;and are widely supported
in modern machines.&lt;br/&gt;&lt;br/&gt;