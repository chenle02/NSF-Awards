* 1830421
* NRI: FND: Robust Inverse Learning for Human-Robot Collaboration
* CSE,IIS
* 09/01/2018,08/31/2022
* Prashant Doshi, University of Georgia Research Foundation Inc
* Standard Grant
* James Donlon
* 08/31/2022
* USD 660,182.00

Collaborative robots are robots that share with humans their work and personal
spaces. These robots are expected to work with humans on a variety of tasks in
various situations with few changes to their software and hardware. To do this,
the robot must understand what is it that the human or other robot is doing, how
is the human or robot performing the task, and then personalize its interaction.
Currently, robots are programmed with much manual effort to perform specific
tasks in controlled environments. This research is studying ways that will
substantially advance a robot's capabilities in all these aspects, to enable a
collaboration that is as automatic and seamless as possible. It is building
methods, which allow the robot to observe the human or robot perform the task,
understand the human's preferences and intent in the task, and then
spontaneously collaborate with the human on the task. This approach relies on
the insight that observing a human or robot perform the task provides
information and facilitates learning the task. An application considered in this
project is an agricultural robot that will observe and autonomously collaborate
with a human in grading and packing onions in postharvest processing sheds. This
has the potential to augment scarce human labor in our nation's farms in
performing this repetitive task. &lt;br/&gt;&lt;br/&gt;Inverse reinforcement
learning (IRL) refers to both the problem and method by which an agent learns
the goals and preferences of another agent that explain the latter's observed
behavior. The technical approach to this research is first identifying the
challenges that IRL is facing in its use toward inferring the goals and
preferences of the observed agent, human or robot, in real-world contexts. The
research is tractably generalizing IRL to meet key unmet challenges. It is
developing new methods that will make IRL robust to real-world uncertainties
involving hidden variables, occlusions, and imperfect observations by the robot.
Typically, IRL is one sided and the reward is learned with the aim of imitating
the observed behavior. This research will go a step further and investigate how
the dynamics and learned preferences can be revised and incorporated in the
robot's collaborative decision making and planning, to enable the robot to
spontaneously collaborate with the previously observed agent on the task.
Consequently, the focus is on domains where the subject robot can observe an
agent performing well-defined tasks that benefit from teamwork. The research
plan is expected to yield a portfolio of algorithms that take key steps toward
enabling robots to autonomously learn how to perform tasks and deploy this
knowledge toward optimally collaborating with others on the task. Being able to
learn tasks simply from passive demonstrations provides greater appeal to this
research as it minimizes costly human interventions.&lt;br/&gt;&lt;br/&gt;This
award reflects NSF's statutory mission and has been deemed worthy of support
through evaluation using the Foundation's intellectual merit and broader impacts
review criteria.