* 2239448
* CAREER: Statistical Learning with Recursive Partitioning: Algorithms, Accuracy, and Applications
* MPS,DMS
* 06/01/2023,05/31/2028
* Jason Klusowski, Princeton University
* Continuing Grant
* Yulia Gel
* 05/31/2028
* USD 84,472.00

As data-driven technologies continue to be adopted and deployed in high-stakes
decision-making environments, the need for fast, interpretable algorithms has
never been more important. As one such candidate, it has become increasingly
common to use decision trees, a hierarchically organized data structure, for
building a predictive or causal model. This trend is spurred by the appealing
connection between decision trees and rule-based decision-making, particularly
in clinical, legal, or business contexts, as the tree structure mimics the
sequential way a human user may think and reason, thereby facilitating human-
machine interaction. To make them fast to compute, decision trees are popularly
constructed with an algorithm called recursive partitioning, in which the
decision nodes of the tree are learned from the data in a greedy, top-down
manner. The overarching goal of this project is to develop a precise
understanding of the strengths and limitations of decision trees based on
recursive partitioning, and, in doing so, gain insights on how to improve their
performance in practice. In addition to this impact, high-school, undergraduate,
and graduate research assistants will be vertically integrated and benefit both
academically and professionally. Innovative curricula, workshops, and data and
methods competitions involving students, academics, and industry professionals
will facilitate outreach and encourage participation from a broad audience.
&lt;br/&gt;&lt;br/&gt;This proposal aims to provide a comprehensive study of the
statistical properties of greedy recursive partitioning algorithms for training
decision trees, as is demonstrated in two fundamental contexts. The first thrust
of the project will develop a theoretical framework for the analysis of oblique
decision trees, where, in contrast to conventional axis-aligned splits involving
only a single covariate, the splits at each decision node occur at linear
combinations of the covariates. While this methodology has garnered significant
attention from the computer science and optimization communities since the
mid-80s, the advantages they offer over their axis-aligned counterparts remain
only empirically justified, and explanations for their success are largely based
on heuristics. Filling this long-standing gap between theory and practice, the
PI will investigate how oblique regression trees, constructed by recursively
minimizing squared error, can adapt to a rich class of regression models
consisting of linear combinations of ridge functions. This provides a
quantitative baseline for a statistician to compare and contrast decision trees
with other less interpretable methods, such as projection pursuit regression and
neural networks, that target similar model forms. Crucially, to address the
combinatorial complexity of finding the optimal splitting hyperplane at each
decision node, the PIâ€™s framework can accommodate many existing computational
tools in the literature. A major component of the research is derived from
connections between recursive partitioning and sequential greedy approximation
algorithms for convex optimization problems (e.g., orthogonal greedy
algorithms). The second thrust focuses on the delicate pointwise properties of
axis-aligned recursive partitioning, with implications for heterogeneous causal
effect estimation, where accurate pointwise estimates over the entire support of
the covariates are essential for valid inference (e.g., testing hypotheses and
constructing confidence intervals). Motivated by simple setting where decision
trees provably fail to achieve optimal performance, the PI will investigate how
the signal-to-noise ratio affects the quality of pointwise estimation. While the
focus is on causal effect estimation directly using decision trees, the PI will
also investigate implications for multi-step semi-parametric settings, where
preliminary unknown functions (e.g., propensity scores) are estimated with
machine learning tools, as well as conditional quantile regression, both of
which require estimators with high pointwise accuracy.&lt;br/&gt;&lt;br/&gt;This
award reflects NSF's statutory mission and has been deemed worthy of support
through evaluation using the Foundation's intellectual merit and broader impacts
review criteria.