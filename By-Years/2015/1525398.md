* 1525398
* CIF: Small: Bypassing the L1 Norm: Non-Convex Regularization, Convex Optimization, and Sparse Signal Processing
* CSE,CCF
* 08/01/2015,07/31/2019
* Ivan Selesnick, New York University
* Standard Grant
* Phillip Regalia
* 07/31/2019
* USD 315,175.00

Numerous problems in nonlinear signal processing and data science are
successfully tackled through their formulation as ill-conditioned inverse
problems. Such problems arise in medical imaging, speech and audio processing,
biomedical time-series analysis, manufacturing, and remote sensing. Many ongoing
advances in these fields are based on sparse regularization (i.e., the modeling
of data as highly compressible when appropriately transformed). This research
program aims to advance mathematical and computational tools to pose and solve
ill-conditioned inverse problems by developing new techniques for sparse
regularization.

Convex formulations of problems arising in science and engineering are
attractive because one may leverage a wealth of algorithms that are globally
convergent and computationally efficient, even for large-scale non-smooth
problems. While the L1 norm is a cornerstone of convex sparse regularization, it
tends to under-estimate signal values. Non-convex sparse regularization is
therefore a popular and valuable alternative; however, it is hampered by several
complications: the optimal solution is generally a discontinuous function of the
data and the objective function to be minimized generally possesses many sub-
optimal local minima which can entrap optimization algorithms.

This research aims to exploit the effectiveness of non-convex sparse
regularization without forgoing the principles of convex optimization, by
applying the principle of convex relaxation to the objective function as a
whole, rather than to the regularizer alone. In particular, this program
undertakes the development of new non-separable non-convex penalty functions
that ensure the convexity of the objective function (comprising data fidelity
and sparse regularization terms) to be minimized.