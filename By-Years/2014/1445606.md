* 1445606
* Bridges: From Communities and Data to Workflows and Insight
* CSE,OAC
* 12/01/2014,11/30/2021
* Michael Levine, Carnegie-Mellon University
* Cooperative Agreement
* Robert Chadduck
* 11/30/2021
* USD 20,895,167.00

1. Abstract: Nontechnical Description&lt;br/&gt;&lt;br/&gt;The Pittsburgh
Supercomputing Center (PSC) will provide an innovative and groundbreaking high-
performance computing (HPC) and data-analytic system, Bridges, which will
integrate advanced memory technologies to empower new communities, bring desktop
convenience to HPC, connect to campuses, and intuitively express data-intensive
workflows.&lt;br/&gt;&lt;br/&gt;To meet the requirements of nontraditional HPC
communities, Bridges will emphasize memory, usability, Title and effective data
management, leveraging innovative new technologies to transparently benefit
applications and lower the barrier to entry.&lt;br/&gt;Three tiers of processing
nodes with shared memory ranging from 128GB to 12TB will address an extremely
broad range of user needs including interactivity, workflows, long-running jobs,
virtualization, and high capacity. Flexible node allocation will enable
interactive use for debugging, analytics, and visualization. Bridges will also
include a shared flash memory device to accelerate Hadoop and databases.
&lt;br/&gt;&lt;br/&gt;Bridges will host a variety of popular gateways and
portals through which users will easily be able to access its resources. Its
many nodes will allow long-running jobs, flexible access to interactive use (for
example, for debugging, analytics, and visualization, and access to nodes with
more memory. Bridges will host a broad spectrum of application software, and its
familiar operating system and programming environment will support high-
productivity programming languages and development
tools.&lt;br/&gt;&lt;br/&gt;Bridges will address data management at all levels.
Its shared Project File System, connected to processing nodes by a very capable,
appropriately scaled fabric, will provide high-bandwidth, low-latency access to
large datasets. Storage on each node will provide local filesystem space that is
frequently requested by users and will prevent congestion to the shared
filesystem. A set of nodes will be optimized for and dedicated to running
databases to support gateways, workflows, and applications. Dedicated web server
nodes will enable distributed workflows.&lt;br/&gt;&lt;br/&gt;Bridges will
introduce powerful new CPUs and GPUs, and a new interconnection fabric to
connect them. These new technologies will be supported by extremely broad set of
applications, libraries, and easy-to-use programming languages and
tools.&lt;br/&gt;Bridges will interoperate with and complement other NSF
Advanced Cyberinfrastructure resources and large scientific instruments such as
telescopes and high-throughput genome sequencers, and it will provide convenient
bridging to campuses.&lt;br/&gt;&lt;br/&gt;Bridges will enable important
advances for science and society. By supporting pioneers who set examples in
fields not traditionally users of HPC, and by lowering the barrier of entry,
this project will spur others to recognize the power of the technology and
transform their fields, as has happened in traditional HPC fields such as
physics and chemistry. The project will engage students in research and systems
internships, develop and offer training to novices and experts, extend the
impact of the new system to minority schools and EPSCoR states, impact the
undergraduate and graduate curriculum at many universities, raise the level of
computational awareness at four-year colleges, and support the introduction of
computational thinking into high schools.&lt;br/&gt;&lt;br/&gt;2. Abstract:
Technical Description&lt;br/&gt;&lt;br/&gt;The Pittsburgh Supercomputing Center
will substantially increase the scientific output of a large community of
scientific and engineering researchers who have not traditionally used high-
performance computing (HPC) resources. This will be accomplished by the
acquisition, deployment, and management of Bridges, a HPC system designed for
extreme flexibility, functionality, and usability. Bridges will be supported by
operations, user service, and networking staff attuned to the needs of these new
user communities, and it will offer a wide range of software appropriate for
nontraditional HPC research communities. Users will be able to access Bridges
through a variety of popular gateways and portals, and the system will provide
development tools for gateway building.&lt;br/&gt; &lt;br/&gt;Innovative
capabilities to be introduced by Bridges are:&lt;br/&gt;&lt;br/&gt; 1. Three
tiers of processing nodes will offer 128GB, 3TB, and 12TB of hardware-supported,
coherent shared memory per node to address an extremely broad range of user
needs including interactivity, workflows, long-running jobs, virtualization, and
high capacity. The 12TB nodes, featuring a proprietary, high-bandwidth internal
communication fabric, will be particularly valuable for genome sequence
assembly, graph analytics, and machine learning. Bridges will also include a
shared flash memory device to accelerate Hadoop and databases. Flexible node
allocation will enable interactive use for debugging, analytics, and
visualization.&lt;br/&gt;&lt;br/&gt; 2. Bridges will provide integrated, full-
time relational and NoSQL databases to support metadata, data management and
efficient organization, gateways, and workflows. Database nodes will include
SSDs for high IOPs and will be allocated through an extension to the XRAC
process. Dedicated web server nodes with high-bandwidth connections to the
national cyberinfrastructure will enable distributed workflows. The system
topology will provide balanced bandwidth for nontraditional HPC workloads and
data-intensive computing.&lt;br/&gt;&lt;br/&gt; 3. Bridges will introduce
powerful new CPUs (Intel Haswell and Broadwell), GPUs (NVIDIA GK210 and GP100),
the innovative, high-performance Intel Omni Scale Fabric to support increasingly
productive development of advanced applications, supported by an extremely broad
set of applications, libraries, and easy-to-use programming languages and tools
such as OpenACC, parallel MATLAB, Python, and R.&lt;br/&gt;&lt;br/&gt; 4. A
shared Project File System (PFS) will provide high-bandwidth, low-latency access
to large datasets. Each node will also provide distributed, high-performance
storage to support many emerging applications, intermediate and temporary
storage, and reduce congestion on the shared PFS.&lt;br/&gt;&lt;br/&gt;Bridges
will enable important advances for science and society. By supporting pioneers
who set examples in fields not traditionally users of HPC, and by lowering the
barrier of entry, this project will spur others to recognize the power of the
technology and transform their fields, as has happened in traditional HPC fields
such as physics and chemistry. The project will engage students in research and
systems internships, develop and offer training to novices and experts, extend
the impact of the new system to minority schools and EPSCoR states, impact the
undergraduate and graduate curriculum at many universities, raise the level of
computational awareness at four-year colleges, and support the introduction of
computational thinking into high schools.