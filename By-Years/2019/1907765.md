* 1907765
* SHF: Small: Revamping I/O Architectures Using Machine Learning Techniques on Big Compute Machines
* CSE,CCF
* 10/01/2019,09/30/2024
* Jun Wang, The University of Central Florida Board of Trustees
* Standard Grant
* Danella Zhao
* 09/30/2024
* USD 599,827.00

In modern computer systems, the growing disparity in rapidly increasing
computational speeds and slowly improving data transfer rates to/from off-chip
memory and disk drives is a long-standing research challenge, often referred to
as the Input/Output wall problem. This problem has become severe in today's big
data and big compute era. Despite the rapid evolution in data storage
technologies in recent years, the increasing heterogeneity and diversity in
machines and workloads, coupled with the continued data explosion, exacerbate
the speed gap between computing and disk-drive storage. There is an increasing
need to develop a high-performance, and cost-effective architecture for emerging
large-scale and diverse applications that is not affected by the Input/Output
wall problem. The goal of this project is to leverage existing big compute
resources such as graphic processing units (GPUs) and deep learning techniques
to speed up the secondary storage system performance without adding new
hardware. This project will also contribute to society through engaging under-
represented groups from a Hispanic Serving Institution and research
dissemination for computer science and engineering education and
training.&lt;br/&gt; &lt;br/&gt;This project proposes to develop new GPU-enabled
online learned I/O architecture using artificial intelligence. This project
entails three research thrusts: First, it will develop custom machine learning
and deep learning algorithms and models for the storage system. For example, it
will design a temporal-aware classification technique to attack the extreme-
scale learning problem. Second, it will develop new learning solutions for core
storage system modules such as prefetching, log management and garbage
collection. Third, it will integrate these proposed interwoven modules into a
heterogeneous GPU machine and a GPU cluster at scale. It will develop optimal
parallelism solutions for internal solid-state disk (SSD) devices and Non-
Volatile Memory Express (NVMe) and Peripheral Connection Interface Express
(PCIe) protocol to construct an express channel, moving data directly between
storage and GPUs.&lt;br/&gt;&lt;br/&gt;This award reflects NSF's statutory
mission and has been deemed worthy of support through evaluation using the
Foundation's intellectual merit and broader impacts review criteria.