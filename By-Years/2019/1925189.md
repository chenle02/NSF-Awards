* 1925189
* NRI:FND: Unifying standard physics-based control with learning-based perception and action to enable safe and agile object manipulation using unmanned aerial vehicles
* ENG,ECCS
* 09/15/2019,08/31/2023
* Marin Kobilarov, Johns Hopkins University
* Standard Grant
* Richard Nash
* 08/31/2023
* USD 749,639.00

Flying robots capable of object manipulation will enable new applications such
as load pickup and delivery, infrastructure inspection and repair, agricultural
crop management and harvesting. Currently though, aerial vehicles are limited in
their agility and robustness when in close contact with their surroundings. More
specifically, controlling aerial robots to interact with the natural environment
requires complex models for inferring object dynamics in real-time through shape
and appearance, dealing with contact and compliance, relying on complex
perceptual cues such as occlusions or shadows, while at the same time ensuring
safety and reliability. Currently, standard algorithms for robotic perception
and control are not sufficient for such tasks. While machine learning techniques
have proven powerful for vision-based perception and more recently for control
in simple environments, current learning techniques are not directly suitable
for agile autonomous vehicles where safety is critical and failed actions can be
fatal for the robot and humans around it. To overcome these challenges, this
project proposes a framework that combines standard control methods with
learning-based perception and action in an integrated framework equipped with
formal high-confidence guarantees on performance. The proposed methodology aims
to enable autonomous vehicles to accomplish tasks that are currently impossible
or infeasible to achieve with standard methods. &lt;br/&gt;&lt;br/&gt;The
project will develop computational theory and algorithms that combine standard,
i.e. physics and logic-based, control methods with learning-based control,
implement a software framework and apply it to aerial manipulation tasks. More
specifically, a fully differentiable framework will be developed that integrates
components with known dynamics based on classical physical state representation
and components that adapt to a given task through a learned implicit state
representation that captures rich inertial and visual sensing. Then, a
methodology for robust policy optimization with safety certificates will be
developed based on high-fidelity stochastic models learned from robot data and
then used to compute action policies in simulation using learned synthetic
sensor models. The policies can be equipped with high-confidence formal bounds
on performance and safety, which are validated and adapted in the real world. As
a result, the robotic system can operate efficiently with guarantees on
performance and safety. Finally, a fault-tolerant autonomy software framework
will be implemented and the algorithms validated using three applications of
aerial manipulation: object pick-up and transport in cluttered environments;
remote sensor placement and infrastructure inspection; agricultural crop
sampling and management. The proposed theory and methods are generally
applicable to any robotic system operating in challenging environments, beyond
aerial vehicles.&lt;br/&gt;&lt;br/&gt;This award reflects NSF's statutory
mission and has been deemed worthy of support through evaluation using the
Foundation's intellectual merit and broader impacts review criteria.