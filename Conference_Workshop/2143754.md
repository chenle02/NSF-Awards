
* 2143754
* CAREER: Random Neural Nets and Random Matrix Products
* DMS,PROBABILITY, Networking Technology and Syst
* 06/01/2022,08/10/2022
* Boris Hanin,NJ,Princeton University
* Continuing Grant
* Tomek Bartoszynski
* 05/31/2027
* USD 210,614.00

We live in an era of big data and inexpensive computation. Vast stores of
information can efficiently be analyzed for underlying patterns by machine
learning algorithms, leading to remarkable progress in applications ranging from
self-driving cars to automatic drug discovery and machine translation.
Underpinning many of these exciting practical developments is a class of
computational models called neural networks. Originally developed in the 1940's
and 1950's, the neural nets used today are as complex as they are powerful. The
purpose of this project is to develop a range of principled techniques for
understanding key aspects of how neural networks work in practice and how to
make them better. The approach taken by this project is probabilistic and
statistical in nature. Just as the ideal gas law accurately describes the large-
scale properties of a gas directly through pressure, volume, and temperature
without the need specify the state of each individual gas molecule, this project
will explore and identify emergent statistical behaviors of large neural
networks that provably explain many of their key properties observed in
practice. The project will also provide research training and educational
opportunities through organization of summer schools in machine learning for
graduate students. At a high level, a neural network is a family of functions
given by composing affine transformations with elementary non-linear operations.
The simplest important kind of neural networks are roughly described by two
parameters called depth and width. The former is the dimension of the spaces on
which the affine transformations act and the latter is the number of
compositions. The technical heart of this project is to understand the
statistical behavior of such networks when the affine transformations are chosen
at random. The starting point is an analytically tractable regime in which the
network width is sent to infinity at fixed depth. In this infinite width limit,
random networks converge to Gaussian processes and optimization of network
parameters from their randomly chosen starting points reduces to a kernel
method. Unfortunately, this concise description cannot capture what is perhaps
the most important empirical property of neural networks, namely their ability
to learn data-dependent features. Understanding how feature learning occurs is
at the core of this project and requires new probabilistic and analytic tools
for studying random neural networks at finite width. The basic idea is to
perform perturbation theory around the infinite width limit, treating the
reciprocal of the network width as a small parameter. The goal is then to
obtain, to all orders in this reciprocal, the expressions for joint distribution
of the values and derivatives (with respect to both model inputs and model
parameters) of a random neural network. Such formulas have practical
consequences for understanding the numerical stability of neural network
training, suggesting principled settings for optimization hyper-parameters, and
quantifying feature learning.

This award reflects NSF's statutory mission and has been deemed worthy of
support through evaluation using the Foundation's intellectual merit and broader
impacts review criteria.
