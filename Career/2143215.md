
* 2143215
* CAREER: Statistical Learning from a Modern Perspective: Over-parameterization, Regularization, and Generalization
* DMS,STATISTICS
* 09/01/2022,01/21/2022
* Yuting Wei,PA,University of Pennsylvania
* Continuing Grant
* Pena Edsel
* 08/31/2027
* USD 13,866.00

Statistical methods have been a major driving force towards interpretable,
actionable, and trustworthy machine learning. However, the existing statistical
theory remains highly inadequate in explaining many new phenomena that emerge,
and become pervasive in modern machine learning applications. For instance, the
prevalence of over-parameterized models (i.e., the ones that have more model
parameters than samples) challenges our classical statistical insights about the
bias-variance tradeoff; the fact that many learning algorithms exhibit favorable
algorithmic regularization to alleviate overfitting is largely beyond the reach
of previous statistical literature, and the unconventional shapes of the risk
curves in modern applications puzzle many statisticians. Compared to the rich
theory developed for classical settings, however, the statistical underpinnings
for these curious yet mysterious phenomena remain far from sufficient. Motivated
by this, the overarching goal of the project is to enrich the statistical
foundation of machine learning by adapting it to contemporary settings, thereby
bridging classical statistics and cutting-edge machine learning. In addition,
the project will provide valuable opportunities for training students
(particularly underrepresented groups) at all levels across multiple disciplines
in the STEM field, and will exert scientific and societal impacts on several
domains beyond the tasks described herein, including but not limited to
neuroscience, online education, and equitable machine learning.

Striving for interpretability and actionable insights, this project plans to
revisit multiple classical statistical problems---ranging from minimum-norm
interpolation, risk estimation, cross validation, kernel boosting, data-
imbalanced classification, to transfer learning---with an emphasis on unveiling
new insights for modern yet under-explored regimes. Several recurring themes
include: (i) characterizing precise risk behavior in the face of large model
complexity; (ii) reconciling the seemingly conflicting goals of over-
parameterization and regularization; (iii) developing algorithm-specific
statistical reasoning tools; and (iv) exploring the interplay between
regularization and generalization. The project comprises three distinct yet
related thrusts: (1) statistical insights for over-parameterization: which
explores the prolific interplay between model complexity and out-of-sample
performance; (2) algorithmic regularization via early stopping: which aims to
develop statistical principles that underlie early stopping; (3) risk
(non)-monotonicity with imbalanced data: which is motivated by the non-
monotonicity of generalization errors in the sample size and pursues principled
debiasing methods to rectify it. The project will develop a suite of statistical
insights that can inform cutting-edge machine learning practice, as well as an
array of statistical methodologies that will be practically appealing for modern
data-driven applications.

This award reflects NSF's statutory mission and has been deemed worthy of
support through evaluation using the Foundation's intellectual merit and broader
impacts review criteria.
