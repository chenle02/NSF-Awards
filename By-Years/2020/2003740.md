* 2003740
* RUI: Collaborative Research:  CDS&amp;E:  A Modular Multilayer Framework for Real-Time Hyperspectral Image Segmentation
* CSE,OAC
* 08/01/2020,07/31/2024
* OLCAY KURSUN, Auburn University at Montgomery
* Standard Grant
* Seung-Jong Park
* 07/31/2024
* USD 207,664.00

The analysis of images has been used by the scientific community to solve
challenging problems and to get insight into diverse natural, social, and
technical phenomena. Different types of images have been employed in various
areas of study. One example is the hyperspectral images, which have higher
resolution when compared to conventional camera images. Analyzing such images
has its challenges. For instance, it is computationally demanding, and
traditional methods have some limitations. This project provides an efficient
solution to analyze such images, by exploiting high-performance computing tools
and machine learning techniques. The resulting methods are applied to image-
based atmospheric cloud detection.&lt;br/&gt;&lt;br/&gt;The project develops a
real time, multi-layer, and modular segmentation framework for hyperspectral
images. The developed framework automatically identifies various regions within
a hyperspectral image by classifying each pixel of the image and associating
them to class segments. The developed system is multi-layer, where each layerâ€™s
responsibility is to perform an operation on its input, generate region
classification data, and pass the resultant output to the next layer.
Importantly, each layer analyzes its input from distinct viewpoints, utilizing
spectral and spatial data, resulting in a multi-layer framework where the layers
complement each other. Also, this project aims to provide an optimized high-
performance (speed-up and accuracy) computational tool for real-time
hyperspectral image analysis. This is achieved by adapting the algorithms used
in the different parts of the model for parallel
processing.&lt;br/&gt;&lt;br/&gt;This award reflects NSF's statutory mission and
has been deemed worthy of support through evaluation using the Foundation's
intellectual merit and broader impacts review criteria.