
* 0706732
* Computer-intensive methods for nonparametric time series analysis
* DMS,STATISTICS
* 08/01/2007,07/24/2007
* Dimitris Politis,CA,University of California-San Diego
* Standard Grant
* Gabor Szekely
* 07/31/2010
* USD 140,001.00

The statistical analysis of time series and random fields is vital in many
diverse scientific disciplines. This project continues the development of
computer-intensive statistical methods of inference for the analysis of
dependent data without relying on unrealistic or unverifiable model assumptions.
In particular: (a) General estimators are constructed based on nested subsample
values of converging/diverging statistics with general applications including
tail index and rate estimation. (b) Limit theorems are proven for the
distribution of self-normalized statistics from marked point processes with
(possibly) heavy tails; it is shown how subsampling can be used for inference
purposes without explicit knowledge and/or estimation of the heavy tail index.
(c) It is demonstrated that the use of special `flat-top' kernels is advised
both in the context of residual bootstrap, as well as in the problems of
functional estimation in nonparametric autoregression, estimation of conditional
moments, and spectral density and large-sample covariance matrix estimation. (d)
Two different block bootstrap schemes, one based on a local blocking technique
and the other on residuals, are devised to address data from locally (but not
globally) stationary series. (e) The way to conduct a most powerful bootstrap
hypothesis test in linear/nonlinear (auto)regression set-ups is identified and
powerful bootstrap unit root tests are devised as a result.

This project falls in the realm of nonparametric statistics where inferences
(estimation, confidence intervals, hypothesis tests, etc.) are carried out
without relying on ad hoc model assumptions. In some sense, the nonparametric
viewpoint allows the data to ``speak for itself'', and is particularly
appropriate in a `large-sample' situation where data are abundant; in our
information-explosion age, this is progressively a typical situation. For
example, in a daily series of exchange rates or stock returns spanning a decade,
or a series of (average) annual temperatures over the last 100 years, there may
be evidence that the stochastic structure of the series has not remained
invariant over such a long stretch of time. Part of this project deals with
devising appropriate computer-intensive methods for inference in such
nonstationary environments (e.g., trend detection and estimation) that would be
most helpful in economic applications as well as the problem of climate change.
As another example, consider meteorological data gathered from weather stations
scattered all around the country; since the spatial locations of the
measurements are highly irregular, this type of data constitutes a so-called
`marked point process'. The work under this project provides powerful
methodology for the analysis of data under such practically important and
difficult settings.
