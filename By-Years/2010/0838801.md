* 0838801
* SGER:   Exploiting Alternative Packagings of Source Meaning in Statistical Machine Translation
* CSE,IIS
* 09/01/2008,08/31/2010
* Philip Resnik, University of Maryland, College Park
* Continuing Grant
* Tatiana Korelsky
* 08/31/2010
* USD 160,763.00

SGER: Exploiting Alternative Packagings of Source Meaning in Statistical Machine
Translation&lt;br/&gt;&lt;br/&gt;Current approaches in statistical machine
translation (MT) miss a key&lt;br/&gt;fact: the source language sentence is not
the only way the author's meaning could have been expressed. The idea that the
source sentence is just one of various ``packagings'' of underlying meaning was,
of course, one familiar motivation for interlingual approaches to translation;
however, interlingual semantic representations have generally been abandoned as
notoriously difficult to define, and equally difficult to obtain accurately with
broad coverage once defined. In this project, we are revisiting the idea of
"packagings" of meaning, but exploring it in practical ways consistent with
current practice in statistical MT. Unlike semantic transfer or
interlingual&lt;br/&gt;approaches, we encode alternatives as source paraphrase
lattices, a representation that allows us to exploit generalizations about the
source language while still maintaining the surface-to-surface orientation that
characterizes the statistical state of the art. Our exploratory work focuses on
capturing syntactic and semantic variation using Lexicalized Well Founded
Grammars (LWFG), a recent formalism that balances expressiveness with practical
and provable learnability results. We are quantifying and characterizing the
information available in source paraphrase lattices, assessing the value of
shallow paraphrasing, and exploring the relative promise of deeper techniques
for source paraphase generation using LWFG and other constraint-based
grammatical frameworks. The ability to capture&lt;br/&gt;generalizations via
source paraphrase may open new possibilities in the translation of minority and
endangered languages, which lack training corpora on the scale necessary to
support standard statistical MT techniques.