* 1450916
* EAGER: How does deep learning improve speech recognition accuracy?
* CSE,IIS
* 09/01/2014,08/31/2016
* Steven Wegmann, International Computer Science Institute
* Standard Grant
* Tatiana Korelsky
* 08/31/2016
* USD 150,000.00

Pervasive and accurate automatic speech recognition has the potential to
transform society in many positive ways, not the least of which is providing
better access to information for those who find it difficult or even impossible
to interact with computers using a keyboard: e.g. the elderly, the physically
disabled, or the vision impaired. Every day millions of people use applications
based on this technology to solve problems that are most naturally accomplished
by interacting with machines via voice. However, the most successful of these
applications have always been rather limited in scope, because, although useful,
speech recognition can be frustratingly unreliable. For example, human beings
are easily able to understand one another despite loud background noise in a
crowded room, severe distortion over a telephone channel, or wide variation in
accents within their common language, but even much milder examples of these
problems will completely derail a speech recognition system. This EArly Grant
for Exploratory Research (EAGER) supports a project whose short term goal is to
understand in a deep, quantitative way why methodology used in nearly all speech
recognizers is so brittle. The long term goal is to leverage this understanding
by developing less brittle methodology that will enable more accurate speech
recognition with a wider scope of applicability. &lt;br/&gt;&lt;br/&gt;This
exploratory study will, first, discover why multilayer perceptrons (MLPs) can
sometimes improve speech recognition accuracy, second, use these diagnostic
insights to select better MLP architectures, and, third, release software so
that others can leverage the developed methods. The use of MLPs has staged a
remarkable resurgence in the last decade, in particular the "deep" architectures
developed recently. In the field of speech recognition, there are two
applications of MLPs that have significantly improved large vocabulary speech
recognition accuracy. Each of these applications work within the standard speech
recognition machinery, which uses hidden Markov models (HMMs) to model the
acoustics, mel-frequency cepstral coefficients (MFCCs) for the models' inputs
(features), and multivariate normals for the hidden states' marginal
distributions. The first application makes a relatively minor adjustment to the
standard machinery by augmenting the standard model inputs with new features
learned from data using a MLP. The second application makes a more substantial
change to the standard machinery by replacing the collection of hidden states'
marginal distributions with a single MLP that models the marginal state
posteriors. The research in the exploratory study will discover the basic
mechanisms that the MLP-based features use to substantially improve HMM-based
speech recognition accuracy. This research builds upon the previous work in the
ICSI lab that used simulation and a novel sampling process to quantify the
impacts that the major HMM assumptions have on speech recognition
accuracy.&lt;br/&gt;&lt;br/&gt;Other recent research on MLPs for speech
recognition has either concentrated on implementation, i.e., how to actually
improve speech recognition accuracy, or on theoretical asymptotic results. While
this research is obviously important, it has proceeded largely by trial and
error and, in particular, it has not addressed the interesting scientific
questions surrounding how these applications of MLPs actually improve speech
recognition accuracy. A deeper understanding of this latter question should, in
the short term, lead to further improvements in speech recognition accuracy and,
in the long term, enable the development of more suitable and successful models
for speech recognition than the HMM, which would be a transformative advance in
the field.