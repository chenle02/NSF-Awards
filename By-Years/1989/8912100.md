* 8912100
* Linear Speedup Architectures
* CSE,CCF
* 11/15/1989,04/30/1992
* Kenneth Steiglitz, Princeton University
* Continuing Grant
*   name not available
* 04/30/1992
* USD 207,052.00

Limiting factors in highly parallel computation that stem from timing,
communication, and reliability requirements are studied. Particular emphasis is
placed on large regular structures, which are especially well suited for signal
processing and iterative scientific computation, such as solving differential
equations and simulating cellular automata. Computers with a million or more
processors will be needed in some of these applications and mathematical results
are needed to guide designer's decisions on synchronization method, memory
organization and use of redundancy. The over-riding concern is whether the cost
of systems can be made to grow slowly enough with the amount of parallelism.
Ideally, the goal is to ensure linear speedup - a proportionate return on
hardware investment. In the area of timing, work continues in statistical
modeling of clock skew in synchronous systems, the analysis of throughput in
self-timed arrays, and an ongoing comparison between these two synchronization
paradigms. In synchronous systems all processing in different computational
elements is simultaneously triggered by a signal called clock. Clock skew arises
when the clock signal must travel different distances in order to reach all
parts of a system. This may cause the system to work incorrectly. In self-timed
systems, different elements do not assume their computations are synchronized
and exchange "hand- shaking" signals everytime they need to synchronize. This
may render the system slower due to overhead. In the area of communication, work
continues on bounds based on pebbling arguments. These bounds on throughput
depend only on memory bandwidth, and reveal the constraining effect of that
resource. A major problem in parallel processing with large number of processors
is to ensure that data is available for all processors to operate on. This data
must be present in the memory of the different elements and/or be input/output
to the machine. The availability of data ultimately determines computational
speed. Analogous questions about the reliability of large parallel arrays are
addressed. The question of whether or not linear speedup is attainable with
fixed reliability is a central problem here, and is the subject of theoretical
work.