* 0218852
* ITR:    Bayesian Learning at the Syntax-Semantics Interface
* CSE,IIS
* 09/01/2002,08/31/2005
* Jesse Snedeker, Massachusetts Institute of Technology
* Continuing Grant
* Tatiana Korelsky
* 08/31/2005
* USD 406,000.00

Bayesian Learning at the Syntax-Semantics Interface Abstract

Children easily learn features of novel verbs from small numbers of scene-
utterance pairs. For example, after encountering a few examples of "breaking" an
object, they infer that break might require an object, e.g., John broke the
glass. They also learn semantic properties. Children and adults can then
generalize to other scene instances representing break. This project
hypothesizes that children combine syntactic and semantic evidence to learn verb
features, using a probabilistic method called Bayesian inference.

The project's first goal is to implement a computational model that can induce
probability distributions on features from a very small number of scene-
utterance pairs. This model will make explicit all the information sources used.
Second, the project will confirm which cues are actually used by human learners
in certain settings. The experimental method matches the computer model's
predictions empirically, by presenting adult and child learners with training
sequences of novel verbs used across varying syntactic and semantic feature
situations. This project's results will advance adaptable computer systems and
information-filtering, both in terms of robustness to noise and an ability to
learn from a small number of examples. These results will improve the
construction of a key component of natural language processing engines: the
dictionary.