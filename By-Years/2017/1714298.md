* 1714298
* Decision Making in Autonomous Machines
* SBE,SMA
* 09/01/2017,08/31/2019
* Yochanan Bigman, Bigman                  Yochanan       E
* Fellowship Award
* Josie S. Welkom
* 08/31/2019
* USD 138,000.00

This award was provided as part of NSF's Social, Behavioral and Economic
Sciences Postdoctoral Research Fellowships (SPRF) program. The goal of the SPRF
program is to prepare promising, early career doctoral-level scientists for
scientific careers in academia, industry or private sector, and government. SPRF
awards involve two years of training under the sponsorship of established
scientists and encourage Postdoctoral Fellows to perform independent research.
NSF seeks to promote the participation of scientists from all segments of the
scientific community, including those from underrepresented groups, in its
research programs and activities; the postdoctoral period is considered to be an
important level of professional development in attaining this goal. Each
Postdoctoral Fellow must address important scientific questions that advance
their respective disciplinary fields. The Directorate of Social, Behavioral and
Economic Sciences offers postdoctoral research fellowships to provide
opportunities for recent doctoral graduates to obtain additional training, to
gain research experience under the sponsorship of established scientists, and to
broaden their scientific horizons beyond their undergraduate and graduate
training. This postdoctoral fellowship award supports a rising scholar in the
field of psychology investigating how people perceive the morality of autonomous
machines (AM), and whether people are willing to delegate moral responsibility
to AM. For thousands of years, morality was believed to be unique to humans.
However, the rise of AM challenges this uniqueness. As autonomous machines
become more sophisticated, they are able to make decisions with moral importance
in medicine, military and driving. This research investigates whether people are
willing to extend moral agency to machines and allow them to make decisions when
lives are at stake. This research advances social science to keep pace with
technological advancement, investigates the roots of moral judgment, and also
helps reveal whether morality can be shared amongst humans and autonomous
machines.

This research project has three objectives. The first objective is assessing the
perceived moral status of autonomous machines. Are robots seen as morally
responsible in the same way as people, and if not, in what ways are they morally
deficient? The second objective is assessing the role of perceived mind in the
moral status of autonomous machines. What kind of minds are autonomous robots
seen to have, and does this mind determine their moral status? This objective is
relevant for human agents as well as for autonomous machines. Doing so, we
investigate for the first time which mental capacities people want moral agents
to have. The third objective is assessing whether acceptance of robots as moral
deciders depends on their advantage as deciders and on whether or not they are
fully autonomous. The research investigates these objectives, using both large-
scale surveys in which participants rate their reaction to humans and AM making
moral decisions, and laboratory studies providing further validation and using
measures other than self-report. Finally, the research investigates how
professionals, such as medical practitioners, respond to the possibility of AM
making some of the hard decisions they usually make.