* 1161962
* RI: Medium: Collaborative Research: Multilingual Gestural Models for Robust Language-Independent Speech Recognition
* CSE,IIS
* 10/01/2012,09/30/2016
* Hosung Nam, Haskins Laboratories, Inc.
* Standard Grant
* Tatiana Korelsky
* 09/30/2016
* USD 203,791.00

Current state-of-the-art automatic speech recognition (ASR) systems typically
model speech as a string of acoustically-defined phones and use contextualized
phone units, such as tri-phones or quin-phones to model contextual influences
due to coarticulation. Such acoustic models may suffer from data sparsity and
may fail to capture coarticulation appropriately because the span of a tri- or
quin-phone's contextual influence is not flexible. In a small vocabulary
context, however, research has shown that ASR systems which estimate
articulatory gestures from the acoustics and incorporate these gestures in the
ASR process can better model coarticulation and are more robust to noise. The
current project investigates the use of estimated articulatory gestures in large
vocabulary automatic speech recognition. Gestural representations of the speech
signal are initially created from the acoustic waveform using the Task Dynamic
model of speech production. These data are then used to train automatic models
for articulatory gesture recognition where the articulatory gestures serve as
subword units in the gesture-based ASR system. The main goal of the proposed
work is to evaluate the performance of a large-vocabulary gesture-based ASR
system using American English (AE). The gesture-based system will be compared to
a set of competitive state-of-the-art recognition systems in term of word and
phone recognition accuracies, both under clean and noisy acoustic background
conditions.

The broad impact of this research is threefold: (1) the creation of a large
vocabulary American English (AE) speech database containing acoustic waveforms
and their articulatory representations, (2) the introduction of novel machine
learning techniques to model articulatory representations from acoustic
waveforms, and (3) the development of a large vocabulary ASR system that uses
articulatory representation as subword units. The robust and accurate ASR system
for AE resulting from the proposed project will deal effectively with speech
variability, thereby significantly enhancing communication and collaboration
between people and machines in AE, and with the promise to generalize the method
to multiple languages. The knowledge gained and the systems developed will
contribute to the broad application of articulatory features in speech
processing, and will have the potential to transform the fields of ASR, speech-
mediated person-machine interaction, and automatic translation among languages.
The interdisciplinary collaboration will facilitate a cross-disciplinary
learning environment for the participating faculty, researchers, graduate
students and undergraduate students Thus, this collaboration will result in the
broader impact of enhanced training in speech modeling and algorithm
development. Finally, the proposed work will result in a set of databases and
tools that will be disseminated to serve the research and education community at
large.