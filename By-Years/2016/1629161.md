* 1629161
* EXP: Assessing 'Complex Epistemic Performance' in Online Learning Environments
* CSE,IIS
* 09/01/2016,08/31/2019
* ChengXiang Zhai, University of Illinois at Urbana-Champaign
* Standard Grant
* John Cherniavsky
* 08/31/2019
* USD 549,811.00

The Cyberlearning and Future Learning Technologies Program funds efforts that
support envisioning the future of learning technologies and advance what we know
about how people learn in technology-rich environments. Cyberlearning
Exploration (EXP) Projects design and build new kinds of learning technologies
in order to explore their viability, to understand the challenges to using them
effectively, and to study their potential for fostering learning. This project
will develop online software tools to assess and offer feedback to learners
communicating complex scientific or technical information. "Complex epistemic
performance" here refers to knowledge representations in reports or case studies
which involve not only facts and theoretical concepts that might produce correct
answers but also arguments, interpretations and conclusions that are matters of
disciplinary or professional judgment. Development and testing of these tools
will take place using clinical case studies in medicine and veterinary medicine.
Medical students will write analyses of specific cases of sick people and
animals, marshaling evidence and making diagnoses based on this evidence. Peers
will offer "second opinions", followed by revision. Students will receive a
combination of human feedback and machine feedback.&lt;br/&gt;&lt;br/&gt;The
principal technical innovation in this project will be the development and
testing of machine learning algorithms that offer useful feedback to learners
and support instructor assessment. The software will analyze student-created
cases in relation to the case objectives and a clinical evaluation rubric. It
will compare review text item by item to the rubric criteria. The rubric-
criterion ratings and overall ratings assigned by users (students and
instructors) will train the software via a process of supervised machine
learning. In this way, the software will be able to make progressively more
accurate assessments of new texts, as well as evaluate the quality of peer
reviews. The software will also use unsupervised machine learning techniques,
highlighting as-yet unclassified patterns that may warrant investigation -- in
other words, it will ask students and instructors to interpret patterns of
response that may be of interest but which they may not have noticed. One key
research outcome of this project will be whether and how machine-supported and
machine-mediated formative assessment processes improve learning outcomes in
science and related professions, as exemplified in university-level medical
education. The algorithms developed in this project could also have broader
applicability in a wide range of areas of scientific and technical endeavor.