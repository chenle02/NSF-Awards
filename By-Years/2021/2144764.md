* 2144764
* CAREER: Machine-centered Cyberinfrastructure for Panoramic Video Analytics in Science and Engineering Monitoring
* CSE,OAC
* 06/15/2022,05/31/2027
* Zhisheng Yan, George Mason University
* Continuing Grant
* Juan Li
* 05/31/2027
* USD 293,426.00

Video analytics plays a pivotal role in science and engineering monitoring.
Monitoring videos captured by remote cameras are typically live streamed to
servers for analysis because of the limited computational capabilities of camera
devices. From wildlife tracking and coastline event detection to airport suspect
recognition and victim search in disaster response, such automated video
analytics systems have been deployed widely to assist human operators. The
recent advancement of 360 degree cameras enables a new paradigm of panoramic
video analytics that can cover the 360 degree surroundings of a monitoring site
and can address the errors in and missing analysis abilities of traditional 2D
video analytics. However, realizing this vision requires live streaming massive
panoramic video data to servers for online analytics, which cannot be supported
by the current cyberinfrastructure (CI). The mismatch between the 360 degree
video bit rate and available network bandwidth can cause lagging or failed
analysis, diminishing the benefits of panoramic video analytics. This project
will create a framework of video compression, streaming, and recovery for
achieving the vision of panoramic video analytics in science and engineering
monitoring. The new CI will allow scientists and engineers to conduct online
panoramic video analytics and enable innovative applications that are otherwise
unattainable. The research outcomes will support the development of a remote
learning tool for imaging analytics, course curriculum and undergraduate
research in media computing, and educational videos for public
outreach.&lt;br/&gt;&lt;br/&gt;This project investigates a machine centered
video computing framework in order to enable online panoramic video analytics.
Unlike traditional human centered video frameworks where pixels are processed to
preserve extensive aesthetic details for human viewing, the proposed CI
compresses, streams, and recovers feature points for machine analytics. Because
of this fundamental change, the proposed framework is able to greatly outperform
legacy video CIs and support panoramic video analytics. To this end, a deep
learning based 360 degree video codec will be built to distill the
spatiotemporal characteristics of video features and optimize both compression
ratio and analytics accuracy. Second, an adaptive 360 degree video bitrate
streaming system will be designed to ensure continuous delivery of full 360
degree video frames by prioritizing regions of interest preferred by machines.
Third, a 360 degree video recovery scheme will be developed to restore noisy and
delayed video data while considering the time constraints in the online
analytics models. Finally, interdisciplinary collaboration will be done with
application area scientists and engineers to carry out the project plans for
evaluation and validation of the panoramic video framework on real world
problems.&lt;br/&gt;&lt;br/&gt;This award reflects NSF's statutory mission and
has been deemed worthy of support through evaluation using the Foundation's
intellectual merit and broader impacts review criteria.