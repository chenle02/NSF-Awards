* 2308530
* CSR: Small: Processing-in-Memory enabled Manycore Systems to Accelerate Graph Neural Network-based Data Analytics
* CSE,CNS
* 08/15/2023,07/31/2026
* Partha Pande, Washington State University
* Standard Grant
* Karen Karavanic
* 07/31/2026
* USD 599,951.00

Many science and engineering applications are enabled by processing large and
heterogeneous graph structured data. For example, data from sensor feeds,
information databases about events, supply chains, and web traffic are
relational by nature and require graph-based representations. Graph Neural
Networks (GNNs) will allow the analysis to uncover hidden patterns from the data
and can enable these applications for making predictions and decision support.
Training machine learning (ML) models at the edge (training on-chip or on
embedded systems) can address many pressing challenges, including data
privacy/security, increase the accessibility of ML applications to different
parts of the world by reducing the dependence on the communication fabric and
the cloud infrastructure, and meet the real-time requirements of
augmented/virtual reality (AR/VR) applications. Many applications including
AR/VR require GNN training on embedded systems. However, existing edge platforms
do not have sufficient capabilities to support on-device training of GNNs.
Moreover, it is estimated that training a single unpruned neural network on
conventional compute platforms, such as GPUs, can cost over $10,000 and emit as
much carbon as five cars over their lifetimes. Resistive random-access memory
(ReRAM) based processing-in-memory (PIM) architectures are a promising solution
to address this problem. The crossbar structure of ReRAM-based architectures
enables efficient Matrix-Vector Multiplication (MVM) operations, which are
ubiquitous in modern ML tasks including GNN training/inference. We use ReRAM as
an example, but the proposed computing framework will work equally well for any
other crossbar-based PIM configuration. The educational contribution of this
work lies in the establishment of an interdisciplinary research-based curriculum
integrating PIM, machine learning, and data-driven design optimization. The
proposed research will enhance the education of students by enabling them to
apply classroom knowledge to research problems that require hardware, software,
and theoretical expertise. The PIs have many years of accumulated experience in
involving underrepresented groups in research. This experience will be leveraged
to motivate and engage students from underrepresented groups, including women,
African Americans, and Hispanics.

In this project, we lay the foundations for a novel and reliable computing
framework for GNN computation using PIM-based manycore systems. With the rising
needs of GNN-based applications from the edge to the cloud, we need computing
systems to meet the stringent size, weight, and power (SWaP) constraints. The
key contribution of this research will be the conceptual development,
optimization, and evaluation of high-performance, energy-efficient, and reliable
PIM-based architectures for GNN computing. Despite the exponential growth in
interest and extensive research and application studies on GNN-based data
analytics, the importance of hardware-assisted execution efficiency and
hardware-aware algorithm efficiency has not received adequate attention. This
research will reduce the dependency on data centers and high-performance
computing (HPC) clusters for executing GNN-based applications. There is a lack
of holistic solutions that allow us to quickly design and optimize PIM-enabled
computing platforms for GNNs. Hence, machine learning enabled hardware and
software co-design optimization strategies proposed in this work will have
profound impacts on computing platforms where GNNs are increasingly deployed.
There is a growing set of edge ML applications that are enabled by deploying
GNNs on mobile platforms or embedded systems. Since mobile/embedded platforms
are constrained by both compute and storage, there is a great need for PIM-based
solutions to deploy GNNs for edge ML applications. More computational power at
the edge will reduce both the internet traffic and power-hungry processing at
data centers.

This award reflects NSF's statutory mission and has been deemed worthy of
support through evaluation using the Foundation's intellectual merit and broader
impacts review criteria.