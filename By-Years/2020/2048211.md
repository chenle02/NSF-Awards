* 2048211
* CAREER: Teaching Machines through Human Explanation for Information Extraction
* CSE,IIS
* 05/15/2021,04/30/2026
* Xiang Ren, University of Southern California
* Continuing Grant
* Hector Munoz-Avila
* 04/30/2026
* USD 513,058.00

The majority of data being generated by society is free-form textual data. As
the volume of text data continues to grow, humans alone cannot hope to be able
to understand every piece of textual information published. Hence, a need for
machine-based methods to extract salient entities along with their relationships
from massive textual data is needed. While efforts to develop such methods have
proven successful in academia, that success rarely translates over to
practitioners employing extraction systems to solve real-world problems. A
significant cause of this failure in translation is the requirement of copious
amounts of training examples for a machine to learn extraction models. Even when
a sufficient number of examples exist, machines learn very rigid methods, such
that even a slight misspelling can cause a failure. Therefore, a re-think of how
we develop, refine and maintain such machine-based extraction methods is
required. This project proposes a new methodology based around the idea of
providing explanations for both correct and incorrect decisions made by a
machine, with the intention of requiring far fewer examples for machine
training, as well as providing a process of softening the rigidity of current
extraction methods. &lt;br/&gt;&lt;br/&gt;To achieve this goal, this project
will solicit (from humans) natural language explanations on how a machine should
reason about their task, as well as explanations correcting erroneous reasoning
and an alerting system for when a machine’s rationale is possibly going wrong.
Rather than treating humans as merely a “source of labels”, this project aims at
developing a new learning framework that directly models a human’s natural
language explanations to either provide a machine with labeling rationale or
correct an observed erroneous rationale. The project will develop explanation-
based learning methods that can capture the compositional nature of human
natural language explanations, and study explanation-guided model refinement
methods to update model parameters based on the provided human explanations
regarding undesirable behaviors. To adapt to changing data distribution, this
project will formulate a human-in-the-loop continual model refinement framework
where problematic model behavioral patterns are automatically identified, and
human feedback is solicited to correct the model. With these advancements, the
project looks to fundamentally change the way models are trained, refined and
updated, and look to do it by exploiting the expert knowledge contained within
human explanations.&lt;br/&gt;&lt;br/&gt;This award reflects NSF's statutory
mission and has been deemed worthy of support through evaluation using the
Foundation's intellectual merit and broader impacts review criteria.