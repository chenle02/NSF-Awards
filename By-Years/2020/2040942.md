* 2040942
* FAI: Organizing Crowd Audits to Detect Bias in Machine Learning
* CSE,IIS
* 02/01/2021,01/31/2024
* Jason Hong, Carnegie-Mellon University
* Standard Grant
* Sylvia Spengler
* 01/31/2024
* USD 665,000.00

Machine learning development teams often struggle to detect and mitigate harmful
stereotypes due to their own blind spots, particularly when ML systems are
deployed globally. These kinds of representation harms cannot be easily
quantified using today’s automated techniques or fairness metrics, and require
knowledge of specific social, cultural, and historical contexts. The researchers
team will develop a crowd audit service that harnesses the power of volunteers
and crowd workers to identify specific cases of bias and unfairness in machine
learning systems, generalize those to systematic failures, and synthesize and
prioritize these findings in a form that is readily actionable by development
teams. Success in the research team’s work will lead to new ways to identify
bias and unfairness in machine learning systems, thus improving trust and
reliability in these systems. The research team’s work will be shared through a
public web site that will make it easy for journalists, policy makers,
researchers, and the public at large to engage in understanding algorithmic bias
as well as participating in finding unfair behaviors in machine learning
systems. &lt;br/&gt;&lt;br/&gt;This project will explore three major research
questions. The first is investigating new techniques for recruiting and
incentivizing participation from a diverse crowd. The second is developing new
and effective forms of guidance for crowd workers for finding instances and
generalizing instances of bias. The third is designing new ways of synthesizing
findings from the crowd so that development teams can understand and
productively act on. The outputs of this research will include developing a
taxonomy of harms; designing and evaluating new kinds of tools to help the crowd
tag, discuss, and generalize representation harms; synthesizing new design
practices in algorithmic socio-technical platforms in which these platforms can
provide users with the opportunity to identify and report observed unfair system
behaviors via the platform itself; and gathering new data sets consisting of
unfair ML system behaviors identified by the crowd. These datasets will support
future research into the design of crowd auditing systems, the nature of
representation harms in ML systems, and for future ML teams working on similar
kinds of systems.&lt;br/&gt;&lt;br/&gt;This award reflects NSF's statutory
mission and has been deemed worthy of support through evaluation using the
Foundation's intellectual merit and broader impacts review criteria.