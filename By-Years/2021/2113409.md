* 2113409
* Collaborative Research: Inference and Decentralized Computing for Quantile Regression and Other Non-Smooth Methods
* MPS,DMS
* 07/01/2021,06/30/2024
* Wenxin Zhou, University of California-San Diego
* Standard Grant
* Yong Zeng
* 06/30/2024
* USD 173,126.00

Recent years have witnessed the transition of statistical analysis from a small-
or moderate-scale data environment to a world involving massive data on parallel
and distributed computing platforms. However, such a transition poses
significant statistical and computational challenges for many important methods
with non-smooth loss functions. As a representative example, quantile regression
methods are building blocks for many advanced methods in statistics and
econometrics and are frequently used to model financial data and medical data.
The computational inflexibility makes quantile regression less favorable among
various branches of the statistical learning tool kit. The project aims to
develop a unified framework for large-scale learning with non-smooth loss
functions to address the aforementioned problems. The developed methods will be
applied to analyze complex biomedical data subject to censoring or privacy
protocol and large-scale public health data. Both graduate and undergraduate
students will receive training through research involvement in the project,
ranging from developing new methods and theory to open-source software under
different platforms. &lt;br/&gt; &lt;br/&gt;The principal investigators will use
a combination of tools from statistics, optimization, and probability to develop
a unified convolution smoothing framework and establish rigorous theoretical and
algorithmic foundations for a class of statistical methods with non-
differentiable loss, typified by quantile regression and support vector machine.
The former is indispensable for understanding pathways of dependence and
heterogeneous effects irretrievable through standard conditional mean regression
analysis. However, most existing computational methods for quantile regression
are based on generic algorithms, which are not scalable in large-scale machine
learning applications when the number of variables is large. Convolution
smoothing admits fast calibrated gradient-based algorithms without compromising
the estimates' quality, therefore offering a balanced trade-off between
statistical accuracy and computational precision. It also extends the
applicability of quantile regression, from low to high dimensions, fully to
partially observed samples, and linear to nonlinear structures, in modern big
data analytics. The first part of the project will focus on three statistical
problems: (a) high-dimensional sparse quantile regression, (b) large-scale
censored quantile regression, and (c) robust regression with redescending
M-estimation. The second part of the research focuses on developing efficient
decentralized algorithms for methods with non-smooth loss functions under two
modern data types: (i) parallel and distributed data, and (ii) online streaming
data.&lt;br/&gt;&lt;br/&gt;This award reflects NSF's statutory mission and has
been deemed worthy of support through evaluation using the Foundation's
intellectual merit and broader impacts review criteria.