* 1942444
* CAREER: Fast Foveation: Bringing Active Vision into the Camera
* CSE,IIS
* 09/01/2020,08/31/2025
* Sanjeev Koppal, University of Florida
* Continuing Grant
* Jie Yang
* 08/31/2025
* USD 411,696.00

The prevalence of foveation, and the wide variety of it in the living world,
makes it very clear that this is an effective visual design strategy. This
project is about copying foveation, by building fast cameras that can optically
concentrate sensing resources onto areas of interest in the world around them.
Doing this can improve sensing performance for computer vision-enabled
intelligent systems. On resource-constrained platforms, such as robots or
spacecraft, adaptively sensing only on areas of interest improves efficiency.
This project will create capability that enables a variety of sensing
applications. Throughout the project timeline, research outcomes will be
integrated in the investigator's hardware/software bridging courses, focused on
fundamental procedures such as camera calibration. In addition, a program called
LensLearning will be started, to spread foveating camera concepts beyond the
lab. LensLearning includes impacting high-school students through special
University of Florida programs with hands-on projects. It also enables the
training of one high-school student and one undergraduate senior every summer
through this project's timeline, by working with the University of Florida's
associated programs, with the goal of giving opportunities to underrepresented
minorities in foveated camera research.&lt;br/&gt;&lt;br/&gt;Although the idea
of artificial foveation has been explored with slow, mechanical means of motion,
in this project the foveating cameras and accompanying algorithms will be much
faster because they exploit newly available, next generation micro-mechanical
optics that can quickly and adaptively change the camera resolution. The first
phase of this project involves building the fast foveating camera test-bed and
characterizing the fundamental limits of fast foveation for dynamic scenes
through an optical model that considers modulation speed, camera field-of-view,
noise, motion and long-range effects. The second phase involves demonstrating
tracking advantages in dynamic scenes with variants of the fast foveation setup,
such as co-located systems and arrays of foveating cameras. Evaluations in
simulation will be done using widely available datasets by comparing processing
power and imaging efficiency. Real evaluation will also be done on the test bed,
resulting in the release of a novel foveated dataset of dynamic scenes of
everyday objects. In the last phase, the developed systems and algorithms will
be used to demonstrate extreme imaging applications by combining both large
baselines and co-located multimodal systems, showing capabilities such as
glasses-free eye-tracking, imaging in dark environments and fast face imaging
for robotics.&lt;br/&gt;&lt;br/&gt;This award reflects NSF's statutory mission
and has been deemed worthy of support through evaluation using the Foundation's
intellectual merit and broader impacts review criteria.