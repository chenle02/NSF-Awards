* 9212225
* Audio-Visual Kinematic Specification of Articulatory Dynamics
* SBE,BCS
* 09/15/1992,08/31/1996
* Lawrence Rosenblum, University of California-Riverside
* Continuing Grant
* Paul G. Chapin
* 08/31/1996
* USD 155,263.00

ABSTRACT Recently, it has become apparent that a thorough understanding of
speech perception must involve an understanding of how audio-visual speech is
perceived (Summerfield, 1987). However, relatively little work has been
conducted to determine the informational metric for audio-visual speech
integration. This research has been designed to determine what this metric is
and whether the form of the information for auditory and visual perception is
similar. The first series of experiments tests if the perceptual primitives of
audio-visual speech perception are articulatory or low-level energy properties.
Computer-animated visual displays coupled with discrepant auditory information
will be used. The second series of experiments tests the feasibility of
modality-independent speech information (Summerfield, 1987). A visual display
technique which allows for efficient kinematic analyses will be implemented
(Johansson, 1974). The final series of experiments have been designed to
determine if kinematic information for speech can specify articulatory dynamics.
Visual kinematic properties which are specified to dynamic articulatory states
will be determined and then tested in perceptual experiments. These experiments
should add to our understanding of speech perception and, specifically, speech
(lip) reading. The proposed kinematic analyses and computer animation techniques
should help determine the salient kinematic parameters which underlie speech
reading.