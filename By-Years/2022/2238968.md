* 2238968
* CAREER: Robust and Collaborative Perception and Navigation for Construction Robots
* CSE,IIS
* 09/01/2023,08/31/2028
* Chen Feng, New York University
* Continuing Grant
* Cang
* 08/31/2028
* USD 475,000.00

The construction industry is responsible for maintaining aging civil
infrastructure and build new facilities that can accommodate the social needs of
the 21st century. This is in addition to the ongoing critical need to address
long-standing problems in occupational safety, labor productivity, costs, and
labor shortage. A promising technical solution is to introduce mobile robots on
construction jobsites. It is possible to leverage recent discoveries in robotics
and artificial intelligence (AI) to tackle those aforementioned challenges.
However, unlike manufacturing automation or self-driving cars, construction
robots face unique challenges due to the need to navigate dynamic environments.
Such robots are also required to work closely with humans in a variaty of tasks
and often handle heavy payloads. This award supports fundamental robotics
research to allow better perception and navigation for construction jobsite
monitoring robots. It will produce an intelligent mobile robot team equipped
with cameras to autonomously monitor construction progress and operations to
improve jobsite efficiency and safety. The results of this research will be
widely applicable to scenarios beyond construction, ranging from connected and
autonomous vehicles to service robotics in smart and accessible cities. The
project will facilitate collaboration between robotics, artificial intelligence,
and civil and mechanical engineering. Furthermore, it aims to broaden
participation of underrepresented groups in engineering via educational games,
multi-disciplinary robotics curriculum, and workforce training
workshops.&lt;br/&gt;&lt;br/&gt;Mobile robotics in construction jobsites are
often limited by perception challenges due to occlusion and limited field of
view. In dynamic jobsites, limited perception leads to navigation and
inefficient assistance. To improve the robustness, reliability, and scalability
of the vision system in mobile robots, novel self-supervised and graph-based
representation learning will be used to extract, organize, and reason about
places and objects from high-dimensional sensory inputs. This research will
advance the state of the art along three directions: (1) robust navigation from
topological representations for monitoring in dynamic and ever-changing
jobsites, (2) collaborative perception for providing safer operation monitoring
and collision warnings on busy jobsites, and (3) integrated perception and
navigation at both the algorithm, system, and dataset levels. The research will
be validated in real construction jobsites through industry partners, and the
resulting software, hardware design, and dataset will be open source to
stimulate future research.&lt;br/&gt;&lt;br/&gt;This award reflects NSF's
statutory mission and has been deemed worthy of support through evaluation using
the Foundation's intellectual merit and broader impacts review criteria.