* 1925192
* CC* Compute: Building a state-of-the-art campus compute resource at Franklin &amp; Marshall College
* CSE,OAC
* 07/01/2019,06/30/2022
* Peter Fields, Franklin and Marshall College
* Standard Grant
* Kevin Thompson
* 06/30/2022
* USD 400,000.00

Franklin &amp; Marshall College (F&amp;M) is building and deploying a campus
cluster resource to better meet the needs of our researchers and their students
who need greater access to high performance compute resources to support
intensive data analysis and computation. This project provides much needed local
compute nodes for F&amp;M's researchers and students while also contributing to
the growing fabric of shared computing clusters across the country. This project
contributes these new resources to the Open Science Grid (OSG) which is a
national, distributed computing partnership that allows participants to share
their resources with other researchers to maximize the impact these investments
have on scientific research and discovery. As an institution, F&amp;M has many
top-tier scientific researchers who also partner with students in research that
is regularly funded by public and private agencies. Providing substantially
improved infrastructure to support this research will advance and expand the
institution's capacity to support important investigations, from the search for
pulsars to brain science. F&amp;M has a demonstrated commitment to recruiting
and supporting STEM students and this infrastructure improvement and investment
allows the institution to continue to be a leader in this arena, providing
access to the best resources and opportunities for future scientists. This
project, similar to other recent initiatives, demonstrates how it is possible to
design and implement significant research infrastructure, even at a smaller
institution, that advances scientific discovery both on and beyond our
campus.&lt;br/&gt;&lt;br/&gt;The compute cluster maximizes available resources
for research that requires both HPC and HTC solutions. It consists of a head
node and a login node, both running dual 20 core Xeon Gold 6230 2.10GHz CPUs
with 192GB of 2666 MHz ECC memory, and 38TB of SSD NVMe storage. There are 36
standard compute nodes, each using dual 20 core Xeon Gold 6230 2.10 GHz CPUs
with 192GB of 2666 MHz ECC memory and 1.8TB NVMe SSD for OS and local scratch.
There is one GPU node for use with software that takes advantage of cuda
compiled software and GPU co-processing. It mirrors the same specs as the
standard compute node, but includes four Nvidia V100 GPU cards featuring 32 GB
HBM2 memory and 5120 stream processors.&lt;br/&gt;&lt;br/&gt;This award reflects
NSF's statutory mission and has been deemed worthy of support through evaluation
using the Foundation's intellectual merit and broader impacts review criteria.