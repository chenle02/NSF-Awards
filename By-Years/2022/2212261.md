* 2212261
* Collaborative Research: CIF: Medium: MoDL:Toward a Mathematical Foundation of Deep Reinforcement Learning
* CSE,CCF
* 10/01/2022,09/30/2026
* Simon Du, University of Washington
* Standard Grant
* Phillip Regalia
* 09/30/2026
* USD 600,000.00

Deep Reinforcement Learning (DRL), which uses neural networks to solve
sequential decision-making problems, has made breakthroughs in real-world
applications, such as robotics, gaming, healthcare, and transportation systems.
However, current theoretical work on reinforcement learning is restricted to
problems with a small number of states; as these results do not cover neural
networks, they cannot be used to satisfactorily explain the empirical successes
of DRL. This project seeks to bridge this gap by building a mathematical
foundation for DRL that leverages ideas from approximation theory, control
theory, and optimization theory. This will allow the computational and
statistical complexity of DRL to be systematically characterized, and will help
with designing more efficient and reliable empirical methods. Education and
outreach plans are integrated into this project. Specifically, the investigators
will mentor graduate and undergraduate students (some through the STARS program
for underrepresented groups at the University of washington), develop new
courses and monographs, organize research workshops, and develop course
materials for a high school data science and artificial intelligence curriculum.
&lt;br/&gt;&lt;br/&gt;This project has three major components. The first thrust
identifies which types of guarantees are achievable by policies for different
reinforcement learning problem instances. Concretely, this requires
investigating how increasingly structured problem instances enable stronger
guarantees for policies; this will be done by using, and further developing,
tools from non-convex optimization to describe policies that achieve stationary
points, local maxima, and global maxima of the reward function. The second
thrust takes the perspective of approximation theory and capacity control to
investigate how the neural network complexity can be gradually increased to
eventually find the most complex sub-family of neural networks that permit
sample-efficient algorithms. The third thrust builds upon the knowledge gained
in the first two thrusts, and is devoted to the design of computationally
efficient algorithms; this will be done by leveraging tools from optimization
theory and by making connections with control theory.&lt;br/&gt;&lt;br/&gt;This
award reflects NSF's statutory mission and has been deemed worthy of support
through evaluation using the Foundation's intellectual merit and broader impacts
review criteria.