* 1151358
* Gaze Control during Scene Viewing: Behavioral and Computational Approaches
* SBE,BCS
* 02/01/2012,05/31/2016
* John Henderson, University South Carolina Research Foundation
* Standard Grant
* catherine arrington
* 05/31/2016
* USD 414,422.00

When we view the visual world, our eyes flit from one location to another about
three times per second, in movements called saccades. Useful visual information
is acquired only during fixations, brief periods of time when gaze rests on an
object or scene feature. The cognitive and neural processes that direct saccades
and fixations through a scene in real time fall under the term 'gaze control'.
This project focuses on unraveling how human gaze control operates during active
real-world scene perception.&lt;br/&gt; &lt;br/&gt;This project approaches human
gaze control by starting with the insight that understanding eye movement timing
will provide key insight into the underlying cognitive and neural systems that
control gaze. The research combines innovative eye-tracking methods with a
working computational model that simulates eye movement control. In this
research program, the empirical and computational threads are complementary and
synergistic. On the one hand, we can test our understanding of gaze control by
determining whether the model can produce eye movements that look like those
produced by people. On the other hand, insights from the model can be used as a
tool to enhance our theoretical understanding of gaze control, and these
insights can be further tested with new experiments.&lt;br/&gt; &lt;br/&gt;The
results from this project will enhance basic scientific understanding of how
humans perceive and understand the visual world. The project also has wide-
ranging implications for the creation of new display technologies and machine
interfaces that can be controlled by eye movements. And the results are relevant
for the design of new artificial vision systems that actively track and focus on
relevant information in the environment.