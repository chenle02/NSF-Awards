* 2053414
* Collaborative Research: Fusing Massive Disparate Data and Fast Surrogate Models for Probabilistic Quantification of Uncertain Hazards
* MPS,DMS
* 07/01/2021,06/30/2024
* Paul Segall, Stanford University
* Standard Grant
* Pena Edsel
* 06/30/2024
* USD 50,000.00

Mitigating the impact of natural hazards, such as volcanic eruptions,
earthquakes, or infectious diseases, rests on our ability to accurately quantify
hazard risks in advance of their occurrence. This project will tackle this
challenge and develop a new computationally feasible framework to integrate
disparate field observations and computer simulations. The new framework will
deliver substantial upgrades in computational efficiency for natural hazard
quantification. One testbed will be the 2018 eruption of the Kilauea Volcano in
Hawaii, which injured 23 people and destroyed more than 700 dwellings. For this
event, extensive field observations from disparate sources, such as radar
satellites, global navigation satellite system receivers, borehole tiltmeters,
and seismometers, as well as large-scale computer simulations, will be used to
analyze methods for volcanic hazard quantification. The methods developed in the
project will be implemented in open-source software available to a wide
community of scientists and engineers. The project is complemented by training
for both graduate and undergraduate students.&lt;br/&gt; &lt;br/&gt;The first
major roadblock for precisely quantifying uncertain natural hazards is the
computational scalability of computer simulations, as they often require the
numerical solution of partial differential equations on massive spatio-temporal
domains with multi-dimensional input. This challenge will be overcome by
developing Gaussian process (GP) emulators as a computationally feasible
surrogate model to approximate outcomes of computer experiments. This approach
is appealing because it not only includes parallel predictions with linear
computational order with respect to the number of coordinates, but it also
leverages the correlation between coordinates to enable fast predictive
sampling. The second computational challenge is in fusing disparate data from
multiple sources to calibrate physical models. The project will address this
challenge by quantifying uncertainty in data processing and estimating the
discrepancy between the physical model and reality to allow for data
integration. While this project focuses on applications in natural hazard
quantification, the new GP emulator, computational tools for model calibration,
and data integration methods will more generally extend the applicability of
data science and machine learning algorithms.&lt;br/&gt;&lt;br/&gt;This award
reflects NSF's statutory mission and has been deemed worthy of support through
evaluation using the Foundation's intellectual merit and broader impacts review
criteria.