* 1844481
* CAREER: Valid and Scalable Inference for High-dimensional Statistical Models
* MPS,DMS
* 03/15/2019,02/29/2024
* Adel Javanmard, University of Southern California
* Continuing Grant
* Pena Edsel
* 02/29/2024
* USD 402,189.00

Due to the advent of "big data" technologies, fine-grained data sets can be
collected at unprecedented scales, bringing transformative changes to modern
life ranging from healthcare, and social networks, to recommendations systems
and commerce. As a result, data-driven methods are becoming de rigueur nowadays,
driving the need for increasingly sophisticated algorithms that find subtle
statistical patterns in massive amount of data. This trend however is a double-
edged sword: on the one hand, modern statistical learning methods help
researchers in various fields to discover unexpected patterns from data and to
make better decisions impacting everyday life. On the other hand, the rapid
growth in the size and scope of data sets as well as the complexity of the
methods used has made statistical models less transparent. Employing the derived
models without a proper understanding of their validity can lead to many false
discoveries, incorrect predictions and massive costs. For example, suppose the
medical records of patients are used to develop a model for providing
personalized risk score for a chronic disease. A high-risk score can trigger an
intervention, such as incentive for healthy behavior, additional tests, and
medical follow-ups which are all costly. This raises the concern about the
validity of the outcomes retuned by this model. Should one interpret them at an
average level or an individual level? Are the model predictions biased and, if
so, how much? The overarching goal of this project is to develop novel
foundational perspectives on the emerging inferential and computational
challenges in statistics and data science. In addition, the PI plans to develop
software packages to implement the proposed methods and make them publicly
available. The proposed work will benefit a broad range of researchers from
various areas ranging from bioinformatics and machine learning to finance and
engineering. The PI will also integrate components from this project into an
advanced graduate class and use selected results to motivate undergraduate
students to pursue careers in STEM (Science, Technology, Engineering and Math).
&lt;br/&gt;&lt;br/&gt;This project aims at developing statistical methods that
are 1) scalable and 2) valid in the sense that in addition point estimation,
they also quantify the statistical uncertainty that is intrinsic in the
estimation and predications. These issues are among the central topics in modern
statistics and it is imperative to develop solid theory and powerful methodology
to address them. This project focuses on three interrelated prongs that develop
fundamental insights for these ubiquitous challenges: (1) Uncertainty assessment
and high-dimensional inference: the PI will develop a flexible framework for
general hypothesis testing problems in high-dimensional setting using the so-
called debiasing approach, and further study inference for high-dimensional
models with adaptively collected samples, such as time series; (2) Online
hypotheses testing: the PI will formulate the decentralized false discovery rate
(FDR) control where the number of hypotheses to be tested is unknown (possibly
infinite) and the decision maker should take an action on each before the next
p-value is received; and (3) Optimal iterative estimation for non-linear
decision regions.&lt;br/&gt;&lt;br/&gt;This award reflects NSF's statutory
mission and has been deemed worthy of support through evaluation using the
Foundation's intellectual merit and broader impacts review criteria.