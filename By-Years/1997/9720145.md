* 9720145
* Complexity of Neural Networks for Applications
* MPS,DMS
* 08/15/1997,03/31/2002
* NONE NONE, NONE
* Standard Grant
* Michael Steuerwalt
* 03/31/2002
* USD 75,000.00

Kon 9720145 The investigator studies neural network architectures and
applications of wavelet techniques to investigate neural networks' complexity.
Wavelets have been established as a rich and useful family of expansion
functions. The investigator studies further the recovery of functions (in
particular representations of visual images) from their wavelet transforms.
Issues of stability and complexity, which have not up to now been addressed in
his proof of the Marr conjecture and related analysis of the Mallat conjecture,
are studied. An important current question regards the complexity of such
networks (i.e., their essential size) for the completion of desired tasks. The
investigator studies two types, so-called functional and logical networks.
Functional networks have received a good deal of attention, and a coherent
theory has established that they are essentially orthogonal (or more general)
expansion engines. The homology between the structure of networks and expansion
tasks has allowed establishing the connection of wavelet convergence results
with network complexity issues. The investigator examines the class of so-called
logical networks as a needed completion of available network architectures for
the execution of intelligent tasks. In addition he works to show that wavelet-
based neural networks achieve lower bounds on complexities of neural nets for
given tasks. These results move toward a general complexity theory for neural
nets on the order of current computational complexity theory for serial and
parallel computer architectures. Such a complexity theory is expected to be a
hybrid of current discrete and continuous computational complexity theories. The
global purpose of this project is a mathematical study of neural network
architectures that implement some of the theoretical complexity results that the
investigator obtains. Neural networks as models of parallel distributed
computing are currently the leading architectures holdi ng a promise of
artificially emulating intelligent systems, as has been indicated in many of
their current applications (including mortgage decisions, commercial stock
market analysis applications, chemical and thermal homeostasis control systems,
satellite image analysis, etc.). A major unanswered question in the development
of such systems is the fundamental issue of how large a network needs to be in
order to perform specific intelligent functions. One type of task that current
so-called "functional" neural architectures have difficulty in dealing with is
artificial visual recognition and related tasks involved in the general area of
robotics. This difficulty seems to be an inherent part of the functional neural
architectures under current study, and the investigator develops architectures
involving so-called "logical" components, which act essentially as algorithmic
engines. In particular such network architectures are necessary for artificial
vision tasks, and prototypes of such tasks are simulated computationally with
the aid of graduate students working on the project. Wavelets are currently
considered to be one of the most useful tools for representing the types of
input-output functions implemented in neural networks. A more general question
regarding the complexity and size of neural networks accomplishing real-world
tasks is addressed through application of wavelet techniques to network
construction. In particular, functional neural networks may achieve their
optimal performance using wavelets as activation functions. There is a larger
question here regarding whether wavelet techniques are the best possible for the
implementation of functional neural network architectures, which is a conjecture
the investigator has made and investigates. The computational aspects of the
project are aided by associated groups at Howard University and Bryn Mawr
College, the Howard group involving a number of graduate students.