* 2008906
* SHF: Small: Development of Differentiable Memory Augmented Neural CPU Architecture for Cognitive Computing
* CSE,CCF
* 10/01/2020,09/30/2023
* Jie Gu, Northwestern University
* Standard Grant
* Sankar Basu
* 09/30/2023
* USD 508,000.00

The past half-decade has seen unprecedented growth in machine learning with deep
neural networks (DNNs), which now represent the state-of-the-art in many AI
applications. However, existing DNN models require substantial memory and
computing power, which greatly limit their use in resource-constrained systems
such as mobile and IoT devices. This project will develop new algorithms and
hardware to significantly improve the efficiency of DNNs, and represents an
important step towards enabling fast and adaptive DNN executions even in
resource-limited environments. In that sense, this project has the potential to
enable a wider deployment of machine learning, which will play a critical role
in many aspects of the future smart society. The research project will provide
research training opportunities to the students as well as new curriculum
development by leveraging existing resources at Cornell, e.g., summer camps as
well as an outreach programs for high-school students including
women.&lt;br/&gt;&lt;br/&gt;This project aims to significantly improve the
efficiency of DNNs while maintaining high accuracy, by co-developing algorithm
optimizations and efficient hardware accelerator architecture. While there exist
many lines of work on reducing DNN execution costs, the majority of these
techniques are designed primarily to improve inference, and perform static
optimizations that reduce computation uniformly for all inputs or only exploit a
limited form of dynamic sparsity, namely zeros. This project aims to enable new
performance-accuracy trade-off points for DNNs that are not possible today by
exploiting general forms of dynamic sparsity that are specific to each input at
run-time. More specifically, the project plans to investigate input-specific
gating techniques that can remove redundant computations for both training and
inference, develop dynamic quantization techniques that do not require training
data, and design an efficient and unified hardware accelerator architecture that
provides both real-world performance and energy
improvements.&lt;br/&gt;&lt;br/&gt;This award reflects NSF's statutory mission
and has been deemed worthy of support through evaluation using the Foundation's
intellectual merit and broader impacts review criteria.