* 2106216
* CIF: Small: Self-Adaptive Optimization Algorithms with Fast Convergence via  Geometry-Adapted Hyper-Parameter Scheduling
* CSE,CCF
* 07/01/2021,06/30/2024
* Yi Zhou, University of Utah
* Standard Grant
* Alfred Hero
* 06/30/2024
* USD 411,248.00

Machine-learning and artificial-intelligence techniques have been widely applied
in modern society to enhance quality of lifr. In these applications, machine-
learning models such as neural networks are trained on a large dataset using
various optimization algorithms, which iteratively adjust the model parameters
and converge to a good model. In particular, the convergence of these
optimization algorithms often relies on choosing a good set of hyper-parameters.
For example, one important algorithm hyper-parameter is the step size, which
controls the scale of the update applied to the model parameters in every
iteration, and it must be carefully chosen to avoid slow convergence and
possible divergence. In practice, these algorithm hyper-parameters either are
guided by optimization theory or are set through manual fine-tuning. While
theory-guided algorithm hyper-parameters often rely on certain unknown
geometrical information of the model and are often too conservative, resulting
in result in slow convergence, manually fine-tuned algorithm hyper-parameters
critically depend on the specific application and algorithm, and often introduce
much computation overhead. This project aims to address these issues by
developing a principled, computation-light and effective hyper-parameter
scheduling scheme for different types of optimization algorithms to achieve fast
and stable convergence. The developed adapted hyper-parameter scheduling scheme
is intended to facilitate machine-learning practitioners tuning the algorithm
hyper-parameters and dynamically adapt them to the ongoing optimization process.
This has further positive impact on implementation of large-scale machine
learning applications such as autonomous driving, training adversary-robust
models, robust decision making in finance and control, etc.
&lt;br/&gt;&lt;br/&gt;In this project, the researchers are developing a
principled and efficient algorithm hyper-parameter scheduling framework that
jointly adapts different algorithm hyper-parameters to the local geometry of the
nonconvex objective function for a variety of popular optimization algorithms,
and corroborate them with strong theoretical convergence guarantees in nonconvex
machine learning. Specifically, the researchers are developing such geometry-
adapted hyper-parameter scheduling scheme for deterministic optimization
algorithms, including first-order gradient-based algorithms, accelerated
gradient algorithms and second-order Newton-type algorithms. The researchers are
developing new analysis tools that advance the understanding of the relation
between hyper-parameters and the dynamic optimization process. Iteration and
computation complexities of these algorithms is being established in nonconvex
optimization. Based on this development, the researchers are extending the
adapted hyper-parameter scheduling scheme to stochastic optimization algorithms,
which use mini-batch random sampling and therefore necessitate a joint
scheduling of step-size and batch size. Analysis of sample complexity and high
probability convergence guarantee is being established for these algorithms.
Furthermore, these developments are guiding the design of adapted hyper-
parameter scheduling scheme for gradient-based minimax optimization
algorithms.&lt;br/&gt;&lt;br/&gt;This award reflects NSF's statutory mission and
has been deemed worthy of support through evaluation using the Foundation's
intellectual merit and broader impacts review criteria.