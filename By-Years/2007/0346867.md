* 0346867
* CAREER: A Unified Framework for Multilevel Parallelization on Deep Computing Systems
* CSE,CCF
* 02/15/2004,05/31/2007
* Dimitrios Nikolopoulos, College of William and Mary
* Continuing Grant
* Eun K. Park
* 05/31/2007
* USD 286,982.00

Nikolopoulos, Dimitrios&lt;br/&gt;College of William and
Mary&lt;br/&gt;CCF-0346867&lt;br/&gt;&lt;br/&gt;This research activity involves
the design, development and deployment&lt;br/&gt;of a programming framework for
explicit multilevel parallelization using a &lt;br/&gt;global address space
model. The framework targets the upcoming generation of &lt;br/&gt;Petaflop-
class supercomputers, which are based on architectural substrates
&lt;br/&gt;with multiple levels of on-chip and off-chip parallelism and deep
memory &lt;br/&gt;hierarchies. The research addresses the need for increased
productivity and &lt;br/&gt;utilization of the National supercomputing power, by
attempting to close the &lt;br/&gt;gap between computer architecture innovation
and parallel programming &lt;br/&gt;practices. The main goal of this activity is
to reduce the &lt;br/&gt;programming effort required for mapping algorithmic
parallelism to &lt;br/&gt;hierarchical hardware components with heterogeneous
means for parallel &lt;br/&gt;execution and assist programmers in deriving
balanced designs of layered parallel applications. The programming framework
&lt;br/&gt;investigated in this research unifies parallel programming models and
&lt;br/&gt;methodologies and enables faster adaptation of parallel code to new
hardware &lt;br/&gt;platforms. Concurrently, it forms a basis for education and
training of &lt;br/&gt;interdisciplinary student audiences in high performance
programming. &lt;br/&gt;&lt;br/&gt;The parallel programming component of this
research is designed around &lt;br/&gt;standard C++ templates with notation for
nested threads and iterators. &lt;br/&gt;The notations for parallelism are
coupled with a templated representation of &lt;br/&gt;data, which allows for
arbitrary partitioning, sharing, and coherence control &lt;br/&gt;at multiple
levels of parallel execution constructs. While the programmer
&lt;br/&gt;highlights nested parallelism, the orchestration and management of
multigrain &lt;br/&gt;threads and data are delegated to the compiler and the
runtime system. The &lt;br/&gt;research investigates novel methods for
controlling the &lt;br/&gt;granularity of multilevel parallelism via vertical
analysis of the program. &lt;br/&gt;Periodicity analysis and selective runtime
tracing are used as means to &lt;br/&gt;derive effective data distribution and
layout schemes without user&lt;br/&gt;intervention. Alongside runtime analysis,
new resource-driven scheduling &lt;br/&gt;strategies and novel microprocessor
features, including on-chip &lt;br/&gt;multithreading, on-chip SIMD parallelism
and speculative execution, are &lt;br/&gt;incorporated into the parallelization
and program optimization processes. &lt;br/&gt;