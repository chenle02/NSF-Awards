* 2211784
* Collaborative Research: HCC: Medium: HCI in Motion -- Using EEG, Eye Tracking, and Body Sensing for Attention-Aware Mobile Mixed Reality
* CSE,IIS
* 09/01/2022,08/31/2025
* Tobias Hollerer, University of California-Santa Barbara
* Standard Grant
* Todd Leen
* 08/31/2025
* USD 738,652.00

Mobile, wireless, headsets for virtual and augmented reality, such as the Meta
Quest 2 and Microsoft HoloLens-2, are becoming more widely used in many
applications beyond video games, such as training, construction, and medicine.
However, wearing these head-worn goggles while walking can make some people feel
sick or distracted, which has even led to injury in some cases. This effect is
similar to texting while walking, but potentially worse because a person's
entire periphery can be filled with distracting media elements. While previous
research has investigated these issues when users are standing still or seated,
it is unclear how problems unfold and how they can be prevented while users are
in motion. Specifically, this project will investigate how and why virtual and
augmented reality headsets affect attention and feelings of sickness. First,
this work will record data, such as heart rate, brain waves, and the direction
users are turning their eyes to, while they are wearing virtual and augmented
reality headsets and walking. Secondly, this project will develop ways to reduce
sickness and distraction while walking with virtual and augmented reality
headsets. This work will improve the safety of mobile virtual and augmented
reality headsets, products that virtually all big technology companies today
heavily invest in as possible companions or replacements to smartphones. This
project will be introduced in courses and research mentorship projects at The
University of Texas at San Antonio and the University of California at Santa
Barbara, to advance research training of both undergraduate and graduate
students. Considering that both universities and research teams have a history
of supporting many underrepresented minority students, it is expected that the
educational value of this project will be high, especially in terms of
recruiting and mentoring women and underrepresented minority
students.&lt;br/&gt;&lt;br/&gt;There is an increasing prevalence of mobile,
immersive interfaces (e.g., mobile Virtual Reality(VR) / Augmented Reality (AR))
that may affect users' cognitive capacities and situational awareness,
potentially leading to physical harm (e.g., impaired task performance, tripping
over physical obstacles in VR, unsafe street crossings while seeing
advertisements in AR). The landscape of human-computer interaction has expanded
from fairly well standardized stationary office configurations to more varied
mobile and immersive settings involving active body movements (mobile and
situated computing, AR, mobile VR) and simulated first-person perspective
changes and motion experiences (immersive computing). To make matters worse,
compared to more standardized platforms such as desktop and laptop UIs, tablet
and smartphone interfaces, individual differences among users have a much bigger
usability impact in context-driven surround-focus usage scenarios found in
mobile AR/VR. For example, motion sickness (i.e., cybersickness) in VR is known
to inflict symptoms of widely varying severity, depending on the individual
user. One serious consequence is that interaction designers have difficulties
providing engaging general experiences that are universally usable by a wide
variety of users. Despite the increasing prevalence of immersive technologies
and their pitfalls, the precise cognitive and physiological mechanisms at play
when 'computing in motion' are not well understood. This work is aimed at
filling this knowledge gap. The specific objectives are: 1) to assess the
cognitive effects of interacting with mobile AR/VR while users are walking, 2)
to provide automated tools to effectively reduce the cognitive demand of mobile
AR/VR, and 3) to make mobile AR/VR safer and more usable. Based on preliminary
data, the central hypothesis is that through multi-modal sensing combined with
machine learning approaches, mobile AR/VR applications can learn the
characteristics of user behavior and provide real time adaptations that will
reduce user error, increase ease of use, improve task performance, and reduce
the impact of physical hazards. This work will improve the safety of mobile AR
and mobile VR, paradigms that virtually all big technology companies today
heavily invest in as a possible follow-up paradigm to the smartphone platform.
Educational impact will occur through incorporation of research outcomes into
undergraduate and graduate courses offered at The University of Texas at San
Antonio and the University of California at Santa Barbara, and research training
and mentorship opportunities for both undergraduate and graduate students. The
courses include Machine Learning, Deep Learning for Visual Computing, Human-
Computer Interaction, and Mobile Application Programming. Because our project
integrates a topic of high social impact with cutting edge machine learning and
human-computer interaction research along with proven successful mentorship
strategies, the educational impacts of the project will be high, especially in
terms of recruiting and mentoring women and underrepresented minority
students.&lt;br/&gt;&lt;br/&gt;This award reflects NSF's statutory mission and
has been deemed worthy of support through evaluation using the Foundation's
intellectual merit and broader impacts review criteria.