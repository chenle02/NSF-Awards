* 2114642
* SHF: Small: Explanation Logic
* CSE,CCF
* 10/01/2021,09/30/2024
* Martin Erwig, Oregon State University
* Standard Grant
* Anindya Banerjee
* 09/30/2024
* USD 499,971.00

Computing technology has a rapidly growing impact on people's lives. Therefore,
it becomes ever more important for computing systems to explain their behavior
and to reassure users why they should trust them. Since the complexity of
automated tasks is also steadily increasing, explanations of computation results
and decisions will be non-trivial and potentially contain a large number of
details, which poses the challenge of delivering the right information at the
right level of detail. To be effective, explanations must dynamically adapt to
specific users and situations. This research explores the nature of dynamic,
adaptable explanations and how they can effectively help users to gain an
understanding of and trust in the behavior of complex computing systems. The
projectâ€™s novelty is the focus on the dynamic nature of explanations to
facilitate flexible, adaptive responses to users, and the project's impacts are
more transparent future computing systems, more widely accepted by more people,
brought about by methods and tools that affect the design of future
software.&lt;br/&gt;&lt;br/&gt;The overall goal of this research project is to
develop a logical basis for reasoning about understanding in support of
explanation systems. Based on the definition of a new modality of understanding
(which is different from knowledge and belief), the investigator is developing a
dynamic two-agent explanation logic (with user and system as agents), which
allows the description of the evolution of understanding. The research project
is producing a dynamic model for explanation logic that is to be the basis for a
generic explanation inference algorithm that mirrors the dynamically shrinking
set of possible worlds with a correspondingly shrinking set of tailored
explanations. In addition, the research is developing concepts for tailorable
explanation structures to support the effective communication of explanations
that are customized to specific user needs.&lt;br/&gt;&lt;br/&gt;This award
reflects NSF's statutory mission and has been deemed worthy of support through
evaluation using the Foundation's intellectual merit and broader impacts review
criteria.