* 2046873
* CAREER: Detecting, Understanding, and Fixing Vulnerabilities in Natural Language Processing Models
* CSE,IIS
* 07/01/2021,06/30/2026
* Sameer Singh, University of California-Irvine
* Continuing Grant
* Tatiana Korelsky
* 06/30/2026
* USD 182,393.00

With recent advances in machine learning, models have achieved high accuracy on
many challenging tasks in natural language processing (NLP) such as question
answering, machine translation, and dialog agents, sometimes coming close to or
beating human performance on these benchmarks. However, these NLP models often
suffer from brittleness in many different ways: they latch onto erroneous
artifacts, do not support natural variations in language, are not robust to
adversarial attacks, and only work on a few domains. Existing pipelines for
developing NLP models lack support for useful insights, and identifying bugs
requires considerable effort from experts both in machine learning and the
domain. This CAREER project develops several techniques to support this need for
more robust training and evaluation pipelines for NLP, providing easy-to-use,
scalable, and accurate mechanisms for identifying, understanding, and addressing
NLP models' vulnerabilities. The developed methods will support diverse
application areas such as conversational agents, sentiment classifiers, and
abuse/hate speech detection. Further, the team engages with the developers of
NLP models in academia and industry to develop a data science curriculum for
K-12 education, particularly for students from underrepresented
communities.&lt;br/&gt;&lt;br/&gt;Based on the notion of vulnerability as
unexpected behavior on certain input transformations, the team will contribute
across the following three thrusts. The first thrust identifies vulnerabilities
by testing user-defined behaviors and searching over many possible
vulnerabilities. In the second thrust, the investigators develop methods to
understand the model's vulnerabilities by tracing the causes of errors to
individual training data points and data artifacts. The last thrust will develop
approaches to address vulnerabilities in models by directly injecting the
vulnerability definitions into the model during training and using explanation-
based annotations to supervise the models. These thrusts build upon the goals of
behavioral testing, explanation-based interactions, and architecture agnosticism
to support most current and future NLP models and
applications.&lt;br/&gt;&lt;br/&gt;This award reflects NSF's statutory mission
and has been deemed worthy of support through evaluation using the Foundation's
intellectual merit and broader impacts review criteria.