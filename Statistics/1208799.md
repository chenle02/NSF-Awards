
* 1208799
* Building a theoretical and methodological framework for collaborative statistical inference and learning: multi-party and multiphase paradigms
* DMS,STATISTICS
* 09/01/2012,09/02/2014
* Xiao-Li Meng,MA,Harvard University
* Continuing Grant
* Gabor Szekely
* 08/31/2016
* USD 360,000.00

Scientific data almost always undergo filtering, imputation, and other forms of
preprocessing before they are analyzed. When such steps are taken, the data
analysis becomes a collaborative endeavor by all parties involved in data
collection, preprocessing, and inference. This research terms such settings as
falling within the multiparty and multiphase paradigms for statistical inference
and learning. These settings are rife with subtleties and pitfalls. Each party
does not and often cannot have a perfect understanding of the entire phenomenon
at hand; the final results will inevitably contain some combination of their
judgments, and some preprocessing can irreversibly destroy information from the
raw data. Building upon his previous theory and methods for dealing with
uncongeniality with multiple imputation, the PI and his students aim to develop
a set of statistical theory and methods to understand such problems and to
provide better preprocessing, inferences, and learning. Their ultimate goals
include providing methods for assessing the validity of such collaborative
analyses, guidance on statistically-principled preprocessing, and a rich new
theory of statistical learning and inference with multiple parties. The
theoretical framework they develop can shed light on principles and methods for
constructing more useful scientific databases, handling complex measurement
processes, and analyzing massive datasets.

With the dramatic increases in the size, diversity, and complexity of data
available for scientific discoveries, medical advances, education reforms and
evidence-based policy making, the entire enterprise of quantitative scientific
inquiry has been presented with unprecedented challenges and opportunities. The
vast majority of current inquiries are not made by a single individual or even a
single team. In particular, the analysis of scientific data depends heavily on
preprocessing in practice. Following data collection, raw data is typically
transformed into a more easily handled form. Such transformations range from
innocuous to highly destructive. When poorly executed, they can destroy huge
scientific investments by rendering them useless for future analyses. These
dangers are rising in importance as scientists and funding agencies emphasize
the construction of scientific repositories and "big data". Despite its
importance, preprocessing is poorly understood from a theoretical perspective.
Even among statisticians, conventional wisdom and informal guidance are the
norm. The PI and his students will work to close this gap, building a theory of
preprocessing and collaborative inference. This theory aims to guide the
construction of scientific repositories and the analysis of massive datasets
generated by the latest technologies. It can also open the doors to greater
collaboration and access to high-quality scientific data, broadening the
scientific enterprise.
