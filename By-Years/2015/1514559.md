* 1514559
* Smoothing Methods in Optimization
* MPS,DMS
* 09/01/2015,06/30/2020
* James Burke, University of Washington
* Standard Grant
* Pedro Embid
* 06/30/2020
* USD 186,999.00

This research project concerns mathematical optimization, a field that has
experienced explosive growth of over the past thirty years due to its wide
applicability in science, engineering, business, and medicine. Contributing
factors include the advent of the internet, advances in computational power and
computing architectures, the availability of very large data sets, as well as
advances in science, engineering, communication, and business. These
developments have created a fertile ground for the emergence of new applications
and data acquisition modalities, in addition to new methods for data management,
interpretation, and modeling. These are the driving forces behind big data and
machine learning research. In addition, there is a greater urgency in many
disciplines for addressing questions concerning design, efficiency, risk, and
inference, as well as model selection, system identification, and error and
uncertainty quantification. This research project aims to develop new methods in
non-smooth optimization and to study the practical impact of these methods. The
research will emphasize underlying optimization tools, including model
development, the design of numerical solution procedures, and the assessment and
quantification of model validity, sensitivity, robustness, and uncertainty.

This project concerns the methods and theory associated with the use of
smoothing techniques in optimization. In order to extract solutions having pre-
specified properties, objective functions in modern optimization problems are
often non-differentiable, with the non-differentiability being a key descriptive
component. In addition, non-differentiability is present in an essential way
when the problem is constrained. Problems possessing non-differentiability
appear across a broad spectrum of applications. These include robust statistical
modeling, regularization formulations to encode prior information, system
identification, sparsity optimization, matrix completion, semi-definite
programming, and any problem possessing constraints. In addition, many modern
problems are very large scale. Hence, there is a focus on the development of
fast optimization algorithms for large-scale applications in the presence of
non-smooth/non-convex objectives. Smoothing methods are designed to approximate
these non-smooth problems, with the goal of rapidly obtaining good approximate
solutions. This project is devoted to the development and understanding of new
and emerging smoothing methods for non-smooth optimization, to providing a
mathematical foundation for these methods, and to studying the practical impact
of these methods on a range of applications. Primary objectives include (1)
developing a framework for convergence analysis, (2) extending results for
convex problems to non-convex problems, (3) providing a calculus for smoothing
techniques, (4) developing error bounds using duality theory, (5) continued
development of the level set method for optimization, and (6) consideration of
parametrized optimal value functions.