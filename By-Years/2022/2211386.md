* 2211386
* Collaborative Research: IIS: RI: Medium: Lifelong learning with hyper dimensional computing
* CSE,IIS
* 09/01/2022,08/31/2025
* Sanjoy Dasgupta, University of California-San Diego
* Standard Grant
* Rebecca Hwa
* 08/31/2025
* USD 800,000.00

The use of artificial intelligence (AI) has enabled computers to solve some
problems that were out of reach just a decade ago, such as recognizing familiar
objects in images, or translating between languages with reasonable accuracy. In
each case, a specific task (such as "translate spoken Mandarin into spoken
Spanish") is defined, data is collected (consisting, say, of utterances in the
two languages), and an AI system is trained to achieve this functionality. To
further expand the scope of AI, it is important to build systems that are not
just geared towards highly-specific and static predefined tasks, but are able to
take on new tasks as they arise (new words, new accents, and new dialects, for
instance). This is often called "lifelong learning", and it means, basically,
that the systems are adaptive to change. This project develops an approach to
lifelong learning using a brain-inspired framework for distributed computing,
yielding machines that potentially can solve tasks more flexibly and consume
significantly less power than traditional AI systems. It will: (1) advance the
ability of AI systems to handle changing environments, (2) enable a host of new
low-power AI systems with applications such as environmental sensing, (3)
strengthen mathematical connections between computer science and neuroscience,
and (4) serve as the basis for educational and outreach
activities.&lt;br/&gt;&lt;br/&gt;This project will develop lifelong learning
within the framework of "hyperdimensional computing", a neurally-inspired model
of computation in which information is encoded using randomized distributed
high-dimensional representations, often with limited precision (e.g., with
binary components), and processing consists of a few elementary operations such
as vector summation. We will build HD algorithms for some fundamental
statistical primitives -- similarity search, density estimation, and clustering
-- and then use these as building blocks for various forms of lifelong learning.
These will rest on mathematical advances in (1) the analysis of sparse codes
produced by expansive random maps and (2) algorithmic exploitation of kernel
properties of high-dimensional randomized representations. Our algorithms will
be implemented in hardware, deployed on a network of low-power sensors, and
evaluated experimentally in a lifelong learning task involving air quality
sensing.&lt;br/&gt;&lt;br/&gt;This award reflects NSF's statutory mission and
has been deemed worthy of support through evaluation using the Foundation's
intellectual merit and broader impacts review criteria.