* 2306023
* CIF: Small:  Accelerating Stochastic Approximation  for Optimization and Reinforcement Learning
* CSE,CCF
* 07/01/2023,06/30/2026
* Sean Meyn, University of Florida
* Standard Grant
* Alfred Hero
* 06/30/2026
* USD 600,000.00

This project concerns the design and analysis of recursive algorithms, which
have broad applications in engineering and computer science. Recursive
algorithms play a crucial role in machine learning systems like ChatGPT, which
rely on large amounts of data for training. Reinforcement learning, a field with
numerous famous examples, utilizes recursive algorithms for training computer
programs; among the most famous examples include computer programs that excel in
games such as GO and chess. Training is interpreted as "learning" optimal
responses (e.g., the next move) based on observations (the current configuration
of a chessboard). While stochastic approximation is recognized as a mathematical
model for recursive algorithms and plays a major role in the mathematical theory
of learning, the supporting theory has not kept pace with empirical success. In
reinforcement learning, it is often uncertain if training will be successful or
how much training is required. Along with fundamental research to create new
foundations for algorithmic learning, the research project also involves
graduate student mentoring, dissemination of new and existing research results
through online video lectures, and also dissemination through the Workshop on
Cognition and Control organized by the investigator, which is held annually at
the University of Florida attracting speakers from across the U.S. and abroad.
Techniques will be developed to ensure stability and accelerate convergence of
stochastic approximation algorithms in terms of transients and variance. New
approaches to algorithm design will include techniques based on ordinary
differential equation methods, recent theory of Markov processes, and approaches
to learning based on quasi-random exploration. Much of the work in algorithm
design reduces to a feedback control problem, initially posed in continuous time
to leverage concepts from nonlinear control and stability theory. A remarkable
example is the Newton-Raphson flow which is globally convergent under mild
assumptions. A dependable "algorithmic feedback law" in continuous time is then
translated into a reliable and efficient algorithm implemented in discrete time.
The general theory will be developed within two specific application areas:
reinforcement learning and gradient-free optimization. Reinforcement learning
presents the greatest challenge because, to-date, there is little theory
available to establish the stability of these recursive algorithms outside of
very special cases. Moreover, in recent work the investigator with his students
have shown that Markovian memory can result in very slow convergence, even when
the algorithm is optimized; in such cases it is necessary to change the
algorithmic goal without negatively impacting the quality of the final solution
delivered by the algorithm. In the case of reinforcement learning the primary
objective is to efficiently learn an effective rule for decision making (i.e., a
policy). Fortunately, there is great freedom in choosing a criterion of fit for
learning the best policy within a given class.

This award reflects NSF's statutory mission and has been deemed worthy of
support through evaluation using the Foundation's intellectual merit and broader
impacts review criteria.