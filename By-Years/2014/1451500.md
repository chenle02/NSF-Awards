* 1451500
* EAGER: New Optimization Methods for Machine Learning
* CSE,IIS
* 09/01/2014,05/31/2018
* Tony Jebara, Columbia University
* Standard Grant
* Weng-keen Wong
* 05/31/2018
* USD 100,000.00

This proposal explores the optimization of complicated nonlinear equations that
underlie machine learning problems by reducing them to simpler easy-to-solve
update rules. The learning problems include classification, regression,
unsupervised learning and more. Through a method known as majorization,
complicated optimization problems are handled by iteratively solving simpler
problems like least-squares and traditional linear algebra operations. The
proposal focuses on how to parallelize this method so that it can efficiently
leverage many CPUs/GPUs simultaneously and in a distributed manner. Furthermore,
by making the method stochastic, faster convergence on large or streaming data-
sets becomes possible. Other variations are explored such as sparse learning
where the recovered solution is forced to be compact which also leads to further
efficiency.

Increasingly, the vast majority of machine learning problems in the literature
are optimized by using generic first- and second-order methods. The approach in
this proposal is designed specifically for machine learning optimization
problems and uses majorization and bounding to guarantee monotonic convergence.
In preliminary work, majorization has produced faster convergence in practice as
well as novel theoretical guarantees. To make the method truly viable in
practice, this proposal puts forward distributed, parallel, stochastic and
sparse extensions. Since such extensions may violate monotonic convergence
guarantees, the proposal explores additional algorithmic and theoretical efforts
to preserve guarantees while also obtaining fast algorithms. In particular,
parallelization and distributed computation is performed by wrapping current
state-of-the-art least squares solvers with bound majorization steps. Stochastic
computation is explored using singleton, small-batch and variable-sized batch
methods. Sparsity is achieved by iterating current large-scale sparse solvers
like FISTA and QUIC within the bound majorization technique. In terms of broader
impact, one graduate student will be supported and will help produce
downloadable tools for machine learning experts as well as practitioners.
Modules will be developed to add to the PI's existing courses in machine
learning. The PI will organize a one-day workshop on majorization methods. The
proposal also provides a public project website with access to research
publications, software/data downloads and schedules of upcoming events.