* 2147212
* FAI: Breaking the Tradeoff Barrier in Algorithmic Fairness
* CSE,IIS
* 06/01/2022,05/31/2025
* AARON ROTH, University of Pennsylvania
* Standard Grant
* Todd Leen
* 05/31/2025
* USD 392,992.00

In order to be robust and trustworthy, algorithmic systems need to usefully
serve diverse populations of users. Standard machine learning methods can easily
fail in this regard, e.g. by optimizing for majority populations represented
within their training data at the expense of worse performance on minority
populations. A large literature on "algorithmic fairness" has arisen to address
this widespread problem. However, at a technical level, this literature has
viewed various technical notions of "fairness" as constraints, and has therefore
viewed "fair learning" through the lens of constrained optimization. Although
this has been a productive viewpoint from the perspective of algorithm design,
it has led to tradeoffs being centered as the central object of study in "fair
machine learning". In the standard framing, adding new protected populations, or
quantitatively strengthening fairness constraints, necessarily leads to
decreased accuracy overall and within each group. This has the effect of pitting
the interests of different stakeholders against one another, and making it
difficult to build consensus around "fair machine learning" techniques. The
over-arching goal of this project is to break through this "fairness/accuracy
tradeoff" paradigm. &lt;br/&gt;&lt;br/&gt;Specifically, we will draw on ideas
from learning theory and uncertainty estimation to introduce notions of fairness
that can be satisfied in ways that are monotonically error improving. For
example, if it is discovered that a deployed model has error that is
unacceptably high on some population, our aim will be to find ways to decrease
the error on that population without increasing the error on any other
population. We also aim to find methods that do not require identifying which
groups might be disadvantaged by a particular application of machine learning
ahead of time, since this can be very hard to predict. Instead, we will develop
methods to dynamically update models as it is discovered that they are
performing poorly on populations of interest. Finally, rather than talking about
"fairness" of predictive models in the abstract, we will aim to formulate and
implement notions of fairness that have meaning in the context of particular
downstream applications, and find methods of training upstream predictive
methods that will guarantee these kinds of fairness when the predictive models
are deployed in these downstream use case. In addition to research papers and
software, this project will develop human capital by training PhD students to be
leading researchers in trustworthy machine learning. It will also develop
educational materials aimed at researchers, students, and the general
public.&lt;br/&gt;&lt;br/&gt;This award reflects NSF's statutory mission and has
been deemed worthy of support through evaluation using the Foundation's
intellectual merit and broader impacts review criteria.