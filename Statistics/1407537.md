
* 1407537
* Non-gaussian graphical models via additive conditional independence and nonlinear dimension reduction
* DMS,STATISTICS
* 07/15/2014,07/08/2014
* Bing Li,PA,Pennsylvania State Univ University Park
* Standard Grant
* Gabor Szekely
* 06/30/2017
* USD 210,000.00

Statistical networks and graphical models are two of the most important
components of contemporary data analysis. They have important applications in
Genomics, sociology, machine learning, study of the internet, and homeland
security. Current statistical graphical models require strong assumptions in
order to be computationally feasible for estimating large-scale networks but
these assumptions also severely limit their applications. In this project the
principal investigator will lay out the groundwork for developing a new class of
statistical graphical models that do not rely on these strong assumptions but at
the same time retain the computational simplicity of the current models. The new
class of models will greatly expand the scope and capability of current methods
for analyzing networks that are becoming increasingly prevalent in modern
applications.

In this project the principal investigator will develop a class of nonparametric
graphical models that avoid the Gaussian or copula Gaussian assumptions. This
new class of models can handle intrinsically nonlinear interactions that cannot
be captured by a copula Gaussian model. A fully nonparametric approach, however,
would involve high-dimensional kernels, which perform poorly due to the "curse
of dimensionality." This disadvantage is especially noticeable for large-scale
networks. For this reason, the principal investigator will introduce two
dimension reduction mechanisms into the nonparametric approach: additive
conditional independence and nonlinear sufficient dimension reduction. Additive
conditional independence is a new statistical relation that resembles the
Gaussian interaction structure without being restricted by the Gaussian (or
copula Gaussian) distributional assumption. The graphical models based on
additive conditional independence can capture intrinsically nonlinear
interactions and at the same time avoid high-dimensional kernels. The second
mechanism incorporates ideas and techniques from the most recent advances in
nonlinear sufficient dimension in statistics and machine learning into the
graphical models to reduce the dimension of the mapping kernels.
