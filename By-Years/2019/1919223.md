* 1919223
* SPX: Parallel Models and Algorithms for Emerging Memory Systems
* CSE,CCF
* 10/01/2019,09/30/2023
* Guy Blelloch, Carnegie-Mellon University
* Standard Grant
* Damian Dechev
* 09/30/2023
* USD 1,200,000.00

With the advent of highly-parallel many-core (computing) machines, memory has
increasingly become a limiting factor in continued performance improvement and
scalability, in terms of energy usage, component density, latency, bandwidth,
and reliability. To help deal with these and other problems, the semiconductor
industry has been developing new byte-addressable nonvolatile random access
memory (NVRAM) technologies. These offer the promise of significantly lower
energy needs and higher density than standard dynamic random access memory
(DRAM), while not losing their state on power loss. However, in NVRAM
technology, operations that write to memory are more costly in terms of
throughput and energy than operations that read from memory. This project is
developing and testing new abstractions for emerging large and extreme-scale
computer systems based on NVRAM, and how to effectively leverage this asymmetry
for better performance in large computing systems. The focus will be on
combining theory and practice, and considering issues across multiple levels of
abstraction, from the hardware itself, to high-level algorithms and programming
models. The project will include an educational component that will teach
students about the new technology and how to effectively use
it.&lt;br/&gt;&lt;br/&gt;The project consists of three main components: (1)
developing methodologies for systems combining volatile and nonvolatile memory
that allow individual processors to fail while permitting the overall system to
continue correctly, (2) developing efficient algorithms and caching policies for
settings where writes are more expensive than reads, and (3) developing
techniques to take advantage of the significant computing capability in each
memory controller. In the first component, the project is studying how to
automatically convert arbitrary concurrent programs into a setting where
processors can fail so that the overhead for both running the converted program
and recovering from a failure is low. In the second component, the project is
developing general purpose techniques to reduce the numbers of writes compared
to reads, or reduce the fraction of the memory that needs to be written to, and
applying the techniques across a broad class of algorithms. The research team
will both develop theory and experimentally measure the effectiveness of these
techniques and algorithms. In the third component, the project is looking at how
to use the memory controllers to reduce the cost of fault tolerance and allow
for weaker memory models, with the purpose of scaling to large systems. A key
intellectual challenge is to ensure that the models, techniques, and algorithms
are simultaneously simple, elegant, and practical.&lt;br/&gt;&lt;br/&gt;This
award reflects NSF's statutory mission and has been deemed worthy of support
through evaluation using the Foundation's intellectual merit and broader impacts
review criteria.