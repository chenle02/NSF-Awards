* 2132887
* NRI: Mutually Assistive Robotics
* CSE,IIS
* 01/01/2022,12/31/2024
* Jivko Sinapov, Tufts University
* Standard Grant
* Ephraim Glinert
* 12/31/2024
* USD 1,499,499.00

This project will advance the state-of-the-art for how robots that render
assistance to users with disabilities interact with and learn from those users.
Most current work takes a deficit-based approach to disability, in which the
robot is assumed to be the more competent partner in the interaction and the
user with disabilities provides high-level goals to a system that primarily
helps them with mundane tasks of daily living. This approach is deficient in two
significant ways: first, it removes agency and control from users, directly
counteracting the psychological benefits of assistive technology and potentially
replacing a loss of independence to caregivers with a loss of independence to
robots; and second, it fails to address tasks that improve quality of life, such
as artistic expression or grooming, where the specific sequence of actions, the
manner in which those actions are carried out, and control over those actions is
the goal itself. This research takes a strengths-based approach to assistive
robotics, developing new methods that allow the robot and user to freely assist
each other to complete tasks, and evaluating those methods in activities that
improve people's quality of life and where users' autonomy and control over both
the goal and manner of completing a task are important. Project outcomes will
include new methods for robot learning that empower people with disabilities to
collaboratively design, control, and influence robot behavior while engaging in
pleasurable hobbies, controlling their own appearance, and generally engaging in
creative interaction with the world. These methods will help to ensure that the
next generation of assistive robotics support the quality of life and joyous
self-expression for people with disabilities, as well as their daily chores This
should significantly improve the lives of the substantial number of Americans of
all ages who live with physical disabilities. Additional broad impact will
derive from algorithms for human-robot interaction that significantly advance
not only that field but also inform future work in interactive reinforcement
learning, learning for robotics, and intelligent assistive
technologies.&lt;br/&gt;&lt;br/&gt;Leveraging the team's expertise in assistive
technology, human-robot interaction, augmented reality, and human-robot
interaction, project goals will be achieved through three technological
innovations. First, algorithms to enhance initial model learning with mutual
assistance from robot to human and human to robot at multiple levels of
abstraction, from direct control to language. Second, new methods for giving
users usable mental models of the robot, such as selecting and displaying
information through augmented reality to empower users to understand robot
perception and decision-making and improve their ability to influence robot
behavior. Finally, new interactive learning algorithms that enable users to
exploit feedback after initial learning and ensure that users can influence the
manner in which tasks are conducted as well as task goals. These algorithms will
be united in a three-layer architecture for assistive robotics that explicitly
supports assistance from both robot to human and human to robot at each level.
At the lowest level is a data-driven mapping from sensory state to movement. At
the middle level, those motions are named as atomic actions such as reaching,
pouring, or grasping, and grouped based on parameters such as target objects or
features of the motion. At the highest level, actions are represented
symbolically with pre- and post-conditions and combined into multi-step plans
that achieve user-specified goals while being modified online by the user. In
addition to supporting both robot-to-human and human-to-robot assistance at all
levels, this architecture will also allow for the flow of information between
levels, especially in robot-to-human assistance. The work will be validated with
the help of expert user-collaborators with disabilities, as well as in larger-
scale studies that validate foundational technological
developments.&lt;br/&gt;&lt;br/&gt;This award reflects NSF's statutory mission
and has been deemed worthy of support through evaluation using the Foundation's
intellectual merit and broader impacts review criteria.