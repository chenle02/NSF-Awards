* 2312840
* Collaborative Research: RI: Medium: Principles for Optimization, Generalization, and Transferability via Deep Neural Collapse
* CSE,IIS
* 07/01/2023,06/30/2026
* Zhihui Zhu, Ohio State University
* Standard Grant
* Vladimir Pavlovic
* 06/30/2026
* USD 400,000.00

Deep learning has demonstrated unprecedented performance across various domains
in engineering and science. However, the theoretical understanding of their
success has remained elusive. Very recently, researchers discovered and
characterized an elegant mathematical structure within the learned features and
classifiers called Neural Collapse. This phenomenon persists across a variety of
different network architectures, datasets, and data domains. This project will
leverage the symmetry of Neural Collapse to develop a rigorous mathematical
theory to explain when and why it happens and how it can be used to quantify
generalization performance and provide guidelines to understand and improve
transferability. By advancing the mathematical foundations of deep learning,
this project is expected to influence not only the machine learning community,
but also related areas such as optimization, signal and image processing, and
natural language processing. The project also involves an integrated outreach
and education plan, including promoting accessibility and awareness of computing
and STEM concepts for K-12 students.

This project will expand our understanding of the principles behind non-convex
optimization of training deep learning models, and provide new mathematical
insights on their generalization and transferability properties, leading to
practical implications. In particular, the project is focused on the following
three overarching research thrusts: (i) provide a unified framework to analyze
convergence guarantees for training deep and overparametrized models through
general loss functions to states of neural collapse, first for simplified cases
and then for more general deep models that exhibit progressive neural collapse,
with multi-labels and data imbalance; (ii) harness the structure of neural
collapse to provide tighter generalization bounds for deep models, by
characterizing the structure of the resulting classifiers and their mild
dependence on the training data, as well as by making natural distributional
assumptions; (iii) leverage the generalization of progressive neural collapse to
new environments to understand transferability of deep models to new domains and
tasks, and develop principled approaches for improving transferability and
efficient fine-tuning.

This award reflects NSF's statutory mission and has been deemed worthy of
support through evaluation using the Foundation's intellectual merit and broader
impacts review criteria.