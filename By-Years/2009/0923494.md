* 0923494
* MRI: Development of a Next-Generation Multimodal Data Management Human-Sensing Instrument for Trustworthy Research Collaboration and Quality of Life Improvement
* CSE,CNS
* 10/01/2009,09/30/2013
* Fillia Makedon, University of Texas at Arlington
* Standard Grant
* Rita Rodriguez
* 09/30/2013
* USD 770,622.00

Proposal #: CNS 09-23494 &lt;br/&gt;PI(s): Makedon, Fillia S.; Athitsos,
Vassilis; Huang, Heng; Le, Zhengyi; Popa, Dan O.&lt;br/&gt;Institution:
University of Texas - Arlington&lt;br/&gt;Title: MRI/Dev.: Next Generation
Multimodal Data Management Human-Sensing Instrument for &lt;br/&gt; Trustworthy
Research Collaboration and Quality of Life Improvement&lt;br/&gt;Project
Proposed:&lt;br/&gt;This project, developing an instrument that serves as an
interactive personal care and human activity monitoring center, aims to keep a
person with high quality life and safe at home as long as possible. The
instrument enables privacy-preserving and secure data sharing through wireless
connection with remote users in an assistive living environment. Providing
mental and emotional support, the zooscopion (zScope) can connect devices,
humans, objects, and the environment. It can connect to other assistive living
projects, making them interoperable and can deliver a Digital Library of
sanitized research data and cases with high educational and training value.
zScope combines and correlates many types of data and extracts events of
interest that indicate changes, risks, etc. It can analyze facial expressions to
detect pain, environmental data, house data (such as door opening, telephone
sounds, vacuum cleaner, etc.), human performance metrics (e.g., hand strength),
both in continuous and discrete format. Data are modeled and assembled in
meaningful ways to predict and prevent physical and digital problems (e.g.,
respectively, falls and intrusions). Privacy and security are being made part of
the data modeling at the design phase. The instrument will take sensor data,
human body measurements, camera data when requested, known pattern of behavior
from other cases, brain scans, and clinical information, aiming to provide high
resolution displays of longitudinal as well as episodic events. It outputs a
visual interactive display of patterns and significant human behavioral 'events'
valuable in assistive environments, setting where to use non-invasive monitoring
technologies, helping recognize 'behavioral biomarkers' that will be connected
to other types of health indicators that may come from brain imaging, genetic
analysis, clinical results, or psychological evaluations. It will work with the
next generation of data that include behavioral, clinical, body motion, etc.,
and have low latency tracking. &lt;br/&gt;Broader Impacts: &lt;br/&gt;This work
enables human-centric type of experiments and provides novel new ways of
interaction, visualization, and secure collaboration. Developing 'smarter'
living environments for the aged opens new ways to education with immersive
compelling projects that provide a better understanding of the role of science
and engineering when combining health data (genomic information) to behavior,
predict trends, and provide indicators of how medication and clinical
assessments connect to longitudinal behavior. Long term goals include behavioral
markers for assessing the confluence of environment, drugs, and human
psychology. The instrument is also expected to respond to queries regarding
emerging needs for new analysis of collected information. It includes training
and educational modules with search and browsing tools and a recommender
facility to support decision making and use stored strategies. Moreover,
utilizing existing outreach programs, the project will support local minority
students and high school students A new generation of scientists that can work
together across domain silos towards human centered goals might be in the
making!