* 2239301
* CAREER: FLEXIBLE HIERARCHICAL ABSTRACTIONS FOR ACTIONABLE VISUAL PERCEPTION
* CSE,IIS
* 05/01/2023,04/30/2028
* Dinesh Jayaraman, University of Pennsylvania
* Continuing Grant
* Jie Yang
* 04/30/2028
* USD 119,773.00

Humans excel at prioritizing and attending to different parts of the environment
as the need arises. For example, a nurse feeding a patient might carefully
consider individual morsels of food on a plate while picking them up, but
afterwards devote his/her attention to the patient's mouth and facial
expressions. By comparison, today's robots are rigid and inflexible in the ways
that they observe, process, and interact with the world. They either focus on
everything at the cost of becoming sluggish or instead cut corners
indiscriminately and become prone to failures. This project will build
innovative software technologies to allow general-purpose robots to flexibly
adapt to task requirements, much like humans can, improving their agility and
efficiency and making it easier for them to learn new tasks. This will enable
applications of such robots not only in the household and hospital tasks which
this project will use to develop the research, but also in many other socially
relevant settings as farms, constructions sites, and small-scale manufacturing.
Two graduate students and several undergraduate students will receive research
training directly through this project. Further, this project will also draw
from its research findings to improve graduate and undergraduate courses and
summer outreach courses for high-school students.&lt;br/&gt;&lt;br/&gt;This
project explores the hypothesis that one key missing piece is agile and
efficient perception-action loops that can evolve on-the-fly in response to task
requirements. Consider a robot feeding a child food from a rice bowl. The
locations and shapes of individual morsels of food, and the child's detailed
pose and mouth configuration are all relevant over the course of the task and in
each of the eating phases. Such time-varying task requirements are ubiquitous,
yet they are poorly accounted for by the locked-in abstractions in today's
standard computer vision algorithms and control loops. This project aims to
advance visual recognition approaches in the context of embodied action, and to
develop robot learning approaches in tandem that exploit these advances. It
redesigns control loops to be flexible according to task demands, hierarchically
factorized to permit novel compositions of the factor components, and self-
learnable for scalability. To this end, it develops a staged but end-to-end
differentiable control loop structure with interleaved task-specific, fine-
tunable components and task-generic, reusable component stages. It further
proposes novel active self-learning approaches that exploit the agent's own
embodiment to teach it. These advances will enable improved sample efficiency
for robot learning approaches as well as computational efficiency for task
execution.&lt;br/&gt;&lt;br/&gt;This award reflects NSF's statutory mission and
has been deemed worthy of support through evaluation using the Foundation's
intellectual merit and broader impacts review criteria.