* 1362417
* Neurodynamics of Tonality
* SBE,BCS
* 08/17/2013,07/31/2014
* Edward Large, University of Connecticut
* Continuing Grant
* Anne Cleary
* 07/31/2014
* USD 18,291.00

Music is a high-level cognitive capacity, a form of communication that relies on
highly structured temporal sequences comparable in complexity to language. Music
is found among all human cultures, and musical "languages" vary among cultures
and depend upon learning. For example, European melodies use different kinds of
note combinations than Indian melodies, making it difficult for Westerners to
understand Indian music, and vice versa. Unlike language, however, music rarely
refers to the external world. It consists of self-contained patterns of sound,
aspects of which are found universally among musical cultures. Therefore, while
an understanding of the brain processes underlying language is still a distant
goal, discovering the general principles of neural dynamics that underlie music
may now be possible. Tonality refers to the stability relationships that are
perceived among notes in a musical language. Although there are different kinds
of tonality, tonality itself is a universal feature of music, found in virtually
every musical language. The hypothesis of this research is that neural
oscillation underlies tonal cognition and perception. Neural oscillation is
periodic neural activity that, in the auditory system, becomes time-locked to
incoming sounds. Neural oscillations can be complex, but there are now powerful
mathematical tools for analyzing them. Mathematical analyses of time-locking
auditory dynamics suggest constraints on what sorts of tonal relationships
should be possible. They predict that fundamental principles of neural dynamics
combined with fundamental principles of neural plasticity constrain what musical
languages can be learned.&lt;br/&gt;&lt;br/&gt;To make detailed predictions, a
sophisticated computer model of the auditory system will be built, based on the
organization of the auditory system and general neurodynamic principles. Two
simulations will be trained through passive exposure to European and North
Indian melodies. These two are chosen because they represent two very different
musical languages that are each relatively well-studied. The computer model will
be used to predict neurophysiological and perceptual observations about music
perception that have been collected over the past thirty-five years or so.
Success of this model would imply the existence of a musical universal grammar.
Universals predicted by intrinsic neurodynamics would provide a direct link to
neurophysiology, and explain how brain changes during learning can establish
different musical languages. This could lead to fundamental paradigm shifts in
music theory, music cognition and related fields. The success of this model
would be equally influential in cognitive neuroscience. It would imply that
high-level cognition and perception can arise from the interaction of acoustic
signals with the physics of the auditory system. No neurodynamic approach has
ever successfully captured such a high-level cognitive capacity. Researchers are
currently struggling with the question of how to reconcile cognitive theories
with neurodynamic principles and observations, and success in the musical domain
could lead to new insights. This research will elucidate fundamental mechanisms
of hearing and communication, and holds significant promise for understanding
auditory system development. Identification of innate constraints shaping human
communication behavior may have further implications for language learning. This
research has implications for understanding a wide range of hearing and
communication disorders. It has potential applicability to improving the design
of neural prostheses, enhancing the perception of music and other sounds in
cochlear implant patients.