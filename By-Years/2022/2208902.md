* 2208902
* STTR Phase I:  Integrating Vision-Guided Collaborative Robots for Postharvest Processing of Produce
* TIP,TI
* 01/15/2023,09/30/2023
* Prashant Doshi, INVERSAI, INC.
* Standard Grant
* Peter Atherton
* 09/30/2023
* USD 212,153.00

The broader impact of this Small Business Technology Transfer (STTR) Phase I
project is to empower the processors of harvested fruits and vegetables with the
flexibility to use robotic automation to meet their labor needs. The automation
uses collaborative robots (cobots) guided by computer vision, which are
potentially safe around humans. The technology will help assure consistent
produce quality and processing rates. Through a robust cobot-based solution, the
project will provide an affordable, sustainable, and safe means for farms of all
sizes to keep up with their production goals, which will sustain competition and
the nation’s food supply. This project has the added benefit of upskilling
workers in farms by creating openings for more technically oriented positions,
both in monitoring and maintaining the cobots. Instead of tediously programming
the cobot for each use, the project is introducing a new way of translating the
tasks performed by humans to the cobot by learning from camera recordings. It
will also improve understanding of how cobots can safely be used alongside
humans in a shared working space.&lt;br/&gt;&lt;br/&gt;This Small Business
Technology Transfer (STTR) Phase 1 project aims to make it possible to use
cobots with human workers on tasks that go beyond the traditional pick-and-
place. The proposed technology will automate processing line tasks that require
computer vision, which is challenging because accurate and reliable perception
must guide the robot’s motion. Research has coalesced the technical challenges
on the path to a viable commercial product around five steps. These start with a
formal description of the task domain followed by using robust implementations
of noise-tolerant machine learning algorithms for automatically learning the
task, and end with a solution that integrates the learned task behavior with a
vision-guided cobot system. Phase 1 will support research toward addressing two
problems. The first is to design an intuitive way to elicit a precise
specification of the client’s task domain. A digital conversational assistant
will utilize multiple modalities for the elicitation. The second is the
inability of available implementations to generate coworker-aware and efficient
cobot movements. The research will investigate and develop significant
improvements to the cobot motion to improve coworker safety while reducing the
processing time by an expected 50%.&lt;br/&gt;&lt;br/&gt;This award reflects
NSF's statutory mission and has been deemed worthy of support through evaluation
using the Foundation's intellectual merit and broader impacts review criteria.