
* 0505673
* Statistical Theory and Methodology
* DMS,STATISTICS
* 08/01/2005,05/31/2007
* Bradley Efron,CA,Stanford University
* Continuing Grant
* Gabor Szekely
* 07/31/2008
* USD 310,000.00

ABSTRACT

Prop ID: DMS-050 5673 PI: Efron, Bradley and Diaconis, Persi NSF Program:
STATISTICS Institution: Stanford University Title: Statistical Theory and
Methodology

LARGE-SCALE SIMULTANEOUS TESTING (Bradley Efron)

This investigator is studying the analysis of large-scale simultaneous
hypothesis testing situations, for example a microarray experiment searching for
genes that behave differently in HIV positive or negative subjects. A simple
methodology is being developed that requires a minimum of frequentist or
Bayesian modelling assumptions, and provides for both the efficient selection of
the non-null cases, and the estimation of effect sizes. In classical
terminology, both size and power are assessed. This methodology depends on false
discovery rate calculations, implemented via empirical Bayes techniques. A
typical result might report "there are 200 of the 10,000 genes that can be
clearly identified as differentially expressed between the two groups of
subjects, but there are also about 800 other non-null genes that this experiment
was not powerful enough to detect."

Classical 20th Century statistical theory was fashioned to handle small
problems, dozens or maybe hundreds of data points, with one or maybe a few
unknown parameters. 21st Century scientific technology now provides massive data
sets, with millions of individual measurements and thousands of parameters to
consider all at once. Microarrays are the prime example, but similar problems
arise from a variety of devices: proteomic chips, time of flight spectroscopy,
flow cytometry, and fMRI scanners. The goal of this research is an efficient,
computationally efficient methodology for analyzing massive simultaneous testing
problems, without the need for extensive modelling assumptions.



MONTE CARLO METHODS IN PROBABILITY AND STATISTICS (Persi Diaconis)

The main focus of this investigation is on rates of convergence of Monte Carlo
and Markov chain algorithms for statistical computation. One aspect is phase
transitions ( cut-off phenomena), extending Diaconis' recent solution of the
Peres conjecture (joint with Saloff-Coste). The work includes creating new
algorithms using computational tools such as Grobner bases and combinatorial
characterizations such as Tuttes f-factors. This also contributes to Bayesian
methodology studying prior distributions for Markov chains, non-parametrics and
computational tools. A final focus is the development of group theoretic
character theories in non-standard situations such as unipotent groups and Hecke
algebras.

Probability models underlie many areas of modern scientific study, but they
raise puzzling and important questions concerning the connection of the model
with real world phenomena. Diaconis studies foundational topics such as `what
does it mean to say coin flips are random'? This recently led to the discovery
(joint with Susan Holmes and Richard Montgomery) that in fact, natural human
coin flips show a small but significant bias (about 51% come up the same as the
previous flip). Careful looks at the assumptions, justification and validity of
`randomness' apply to widely-used simulation methods (how long should an
algorithm be run until its job is done?). Weather forcasting and air pollution
are just two of the areas that require the dependability of such simulations.
