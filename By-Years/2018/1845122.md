* 1845122
* CAREER: Drawing inferences for human-like language understanding
* CSE,IIS
* 08/15/2019,07/31/2024
* Marie-Catherine de Marneffe, Ohio State University
* Continuing Grant
* Tatiana Korelsky
* 07/31/2024
* USD 499,914.00

When dealing with language, readers and listeners understand more than just the
literal meaning of the words they read or hear; they also draw inferences from
them. For instance, if someone tells you "I said you were mad to come over at
this time. It's a world event. Do you know that Venice is packed with
visitors?", they will likely infer that Venice is indeed packed with visitors.
However in "How long has she been like this? Did you see a doctor? Do you know
that it is incurable?", they will not infer that it is incurable, even though
both events are in a question and embedded under the same string of words "do
you know". Different factors, like the tone used or world knowledge, play a role
in deriving these inferences. The project aims at studying these factors and
developing broad-coverage models that automatically capture inferences. Such
models have implications for natural language processing (NLP) tasks that
require an accurate inference process, such as information extraction. Further,
to achieve human-like language understanding, it is not only crucial for NLP
technologies to develop models that capture what is conveyed in language without
being explicitly said, but to also assess whether the inferences are systematic
for most people, or whether different interpretations arise. This project
investigates how the variability present in "common sense" data that come from
people's intuitions can be accurately represented in the type of datasets on
which NLP systems are currently built, and thereby be captured.
&lt;br/&gt;&lt;br/&gt;Recently a large body of work in NLP has focused on deep
learning, hill-climbing on new tasks and benchmarks. However such ventures do
not help with the understanding of the details of human language or in
determining which features actually matter for language processing. This project
targets both categorical and non-categorical inferences in a diverse set:
inferences about sentiment, agreement and speaker commitment (whether speakers
are committed to the truth of the events they describe), and redefining the kind
of benchmarks needed to achieve human-like natural language understanding. It
investigates how a better synergy between data-driven methods and the use of
specialized linguistic features can lead to fundamental advances in NLP systems.
The project also quantitatively studies the interactions of linguistic features
on a large amount of naturally occurring examples, and has thus an impact not
only for NLP but also for linguistic theories. Results will include a better
grasp of how linguistic insights can be used to automatically achieve human-
level understanding; a publicly available data that better fit human intuitions
on language than current datasets do and can thereby serve to sharpen NLP
models; and course materials for students and demos for the general public that
raise awareness of societal problems engendered by social media, emphasize the
importance of what gets conveyed by language beyond the explicit string of words
in everyday communication, and demonstrate what can be achieved when research in
linguistics and computer science is combined.&lt;br/&gt;&lt;br/&gt;This award
reflects NSF's statutory mission and has been deemed worthy of support through
evaluation using the Foundation's intellectual merit and broader impacts review
criteria.