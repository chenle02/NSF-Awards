* 1849333
* S&amp;AS:FND:Viewer-Centric Spatial Reasoning and Learning for Safe Autonomous Navigation
* CSE,IIS
* 02/15/2019,09/30/2023
* Patricio Vela, Georgia Tech Research Corporation
* Standard Grant
* James Donlon
* 09/30/2023
* USD 469,997.00

Autonomous navigation has emerged as one of contemporary society's most
promising technological advances. Robust, self-improving strategy for robot
navigation will benefit several industries such as commercial and non-commercial
transportation, large-scale infrastructure inspection, industrial warehousing,
disaster response, and assistive robotics. The main challenge to robust
navigation lies in developing the ability to navigate unstructured, dynamic
environments for which there may be insufficient data collected for training
machine learning methods, and for which model-based reasoning is too complex. A
purely learning-based strategy fails to have operational guarantees (i.e.,
collision avoidance is not guaranteed). The research proposes a mixed method
solution whereby physics-based reasoning and machine learning work together to
resolve the unstructured navigation problem. The combined approach will lead to
a cognizant and reflective navigation pipeline whose performance improves with
time. A central claim of this project is that the learning module will act as an
efficient multi-hypothesis generator for potential navigation decisions, for
which options can be processed, scored, and confirmed by the physics-based
component. The learning system will subsequently use these scores for online
improvement. The net result will be mobile robots that are cognizant of their
operation and adaptable to new information gained during task execution.
&lt;br/&gt;&lt;br/&gt;The research goal of this proposal is to derive a safe
autonomous navigation framework for general settings through the use of a
viewer-centric processing paradigm capable of leveraging learning and model
driven methods to overcome the limitations of entirely object-centric approaches
to navigation. Appealing to Marr's framework for visual processing, the project
investigates a viewer-centric approach to navigation. By more tightly linking
perceptual and planning representations through the viewer-centric approach, the
new approach leverages measurements obtained during navigation to provide online
assessment for improving performance and generating knowledge regarding
navigation through unknown scenes. The project investigates the effect of a
viewer-centric model representation for use in local planning, as well as the
connection of such representations to reflective, experiential machine learning
for improved performance that leverage the model-based planning subcomponent.
The research involves meeting the following objectives: 1) Confirming the
robustness of a viewer-centric navigation framework combining model-based and
deep learning-based approaches for safe navigation with cognizant and adaptive
operation; 2) Demonstrating enhanced reasoning through scene-selective
strategies that improve through experience; and 3) Extending the framework to
dynamic scenes through learned models for the relative physics of motion,
whereby moving objects are modeled in the viewer's frame of reference to detect
dangerous relative motion profiles.&lt;br/&gt;&lt;br/&gt;This award reflects
NSF's statutory mission and has been deemed worthy of support through evaluation
using the Foundation's intellectual merit and broader impacts review criteria.