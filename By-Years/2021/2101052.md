* 2101052
* SaTC: CORE: Small: Decentralized Attribution and Secure Training of Generative Models
* CSE,CNS
* 10/01/2021,09/30/2024
* Yi Ren, Arizona State University
* Standard Grant
* James Joshi
* 09/30/2024
* USD 500,000.00

Generative models describe real-world data distributions such as images, texts,
and human motions, and are playing an essential role in a large and growing
range of applications from photo editing to natural language processing to
autonomous driving. There are two open challenges regarding the development and
dissemination of generative models: (1) Adversarial applications of generative
models have created concerning socio-technical disturbances (e.g., espionage
operations and malicious impersonation); and (2) developing generative models
using multiple proprietary datasets (which are needed to reduce data biases)
raises privacy concerns about data leakage. Legislative efforts have recently
been taken in the wake of these challenges, so far with limited consensus on the
format of regulations and knowledge about their technological or social
feasibility. To this end, this project will develop new mathematical theories
and computational tools to assess the feasibility of two connected solutions to
these challenges: Model attribution enforces the owners to be correctly
identified based on their generated contents; secure training ensures zero data
leakage during the collaborative training of attributable generative models. If
successful, the outcomes of the project will provide technical guidance for
future regulation design towards secure development and dissemination of
generative models. Project results will be disseminated through a project
website, open-source software, and public datasets. The impacts of the project
will be broadened through educational activities, including new course modules
on Artificial Intelligence (AI) security, undergraduate research projects, and
outreach to the local community through lab tours, to prepare underrepresented
groups with skills to mitigate risks from malicious impersonation and biased
data/model representations targeting these groups.&lt;br/&gt;&lt;br/&gt;This
project will focus on synergistic research tasks towards decentralized model
attribution and secure training of generative models. In the former, the
research team will study the systematic design of a set of user-end generative
models that can be certifiably attributed by a set of binary classifiers, which
are stored in a decentralized manner to mitigate security risks. The technical
feasibility of decentralized attribution will be measured by the tradeoffs
between attributability, generation quality, and model capacity. In the latter,
the research team will study secure multi-party training of generative models
and the associated binary classifiers for attribution. Data privacy and training
scalability will be balanced through the design of security-friendly model
architectures and learning losses. New knowledge will be created that
differentiates this project from the existing state-of-the-art literature in
digital forensics and secure computation: (1) Sufficient conditions for
decentralized attribution will be developed, which will reveal analytical
connections between attributability, data geometry, model architecture, and
generation quality. (2) The sufficient conditions will enable estimation of the
capacity of attributable models for a given dataset and generation quality
tolerance. (3) Feasibility of sublinear secure vector multiplication will be
studied, which will fundamentally improve the scalability of secure
collaborative training. (4) Privacy-friendly activation and loss functions will
be designed for the training of user-end generative models and the classifiers
for attribution.&lt;br/&gt;&lt;br/&gt;This award reflects NSF's statutory
mission and has been deemed worthy of support through evaluation using the
Foundation's intellectual merit and broader impacts review criteria.