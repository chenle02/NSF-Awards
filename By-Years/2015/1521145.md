* 1521145
* Non-uniform sampling of permutations and large scale hypothesis testing
* MPS,DMS
* 08/01/2015,07/31/2019
* Art Owen, Stanford University
* Continuing Grant
* Christopher Stark
* 07/31/2019
* USD 399,690.00

Modern scientific tools are delivering very large data sets. This is especially
true in biology where expression levels for thousands of genes or even the
specific DNA information at millions of locations on the genome can be measured.
Scientists would like to correlate these variables with other measured
quantities, especially the presence or absence of a disease. When millions of
hypotheses are investigated, it is possible that one of them will correlate with
some genes just by chance. It is common to insist that the observed correlation
for one test be so strong that it would happen by chance at most once in 20
million tries. The usual way to measure chance correlations is to shuffle the
data at random and see how often a strong effect appears. If the event of
interest is a one in 20 million outcome we usually need about ten times that
many random shuffles to be sure. This proposal is about finding more efficient
random shuffling strategies to get a desired answer with fewer shuffles. The
goal is to find important biological variables with much less computation and
greater reliability. Finding the important genes is a first step for followup
work that includes mining the literature and running experiments to understand
the role of those genes and determine whether their relationship is useful or
not. Part of the work will also involve adjusting for other factors measured or
otherwise that could make the observed correlations misleading. New mathematical
methods for finding and measuring rare and unusual outcomes can also be used in
industrial problems where the rare phenomenon is an unusually effective product
design as measured by computer simulations.&lt;br/&gt;&lt;br/&gt;The usual way
to test whether a gene or a gene set is associated with a phenotype (disease,
height, etc.) or a treatment (diet, medicines, etc.) is to run a permutation
test. From n data points, there are as many as n! permutations to run. Usually
this amount of permutations is beyond our budget and we sample from the
permutations as well. If we compute our test statistic M times, once on the
original data and once for each of M-1 permutations, then the smallest p value
we can possibly get is 1/M. That is, to attain a target p value we have to
compute our statistic at least 1/p times. The standard threshold for genome wide
association studies translates into a bare minimum of 20,000,000 computations.
To have adequate power in a permutation test requires more like 10/p
computations. When the phenotype/treatment is binary, the permutation test
reduces to sampling with replacement. This project uses non-uniform sampling of
permutations or combinations. The main method is importance sampling from
mixtures of proposals using the mixture component probabilities as control
variates. Markov chain Monte Carlo methods will be investigated.