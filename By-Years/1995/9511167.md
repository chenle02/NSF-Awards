* 9511167
* Computational Studies of Parameter Setting in Language
* NONE,NONE
* 11/01/1995,10/31/1999
* Kenneth Wexler, Massachusetts Institute of Technology
* Continuing grant
* Catherine N. Ball
* 10/31/1999
* USD 295,000.00

The problem of developing an appropriate and workable computational theory of
parameter-setting in language is a central problem for linguistic theory,
language acquisition, learning theory and cognitive science in general. The
purpose of this grant is to make a significant increase, compared to previous
studies, in the size and scope of the syntactic parameter spaces, and to do
computational parameter-setting investigations of these parameter spaces. We
plan to investigate the learning of a space of 256 grammars or larger, based on
8 parameters (plus a "lexical" parameter). The major questions that we will
investigate include: 1. With a larger set of parameters, does the theory of
parameter-setting work (i.e., for all natural languages stated in terms of those
parameters, does the algorithm converge)? 2. Do different algorithms work better
than others? 3. What are the kinds of special assumptions about markedness or
default values that work in the specific parameter spaces that we study? Are
there general principles of markedness or default values that seem to apply to
many different parameters? Are default values necessary in general? 4. What
happens to computational results in parameter-setting when we "scale up" ? That
is, if a certain pattern of results obtains for a parameter-space, do we find
that this pattern remains when the space is embedded in a larger space? Or were
the results artifacts of the smaller space? 5. Suppose that an algorithm
converges on the correct parameter-settings for a class of parameter-settings
that we know is instantiated (i.e., there are natural languages which exhibit
these parameter-settings), but doesn't converge for some other parameter-
settings. Can we show that these parameter-settings are not instantiated in
natural language, so that in fact there are reasons of learnability that some
parameter-settings don't show up? 6. How fast do particular algorithms converge?
This can be stated in terms of the number of examples t hat need to be given.
Are some algorithms more realistic than others in this regard? 7. What is the
relation of the properties of the algorithms to empirical research in language
acquisition? Can early properties of acquisition be shown to relate to the
algorithms?