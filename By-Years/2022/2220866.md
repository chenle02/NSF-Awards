* 2220866
* Collaborative Research: Visual Tactile Neural Fields for Active Digital Twin Generation
* CSE,CNS
* 10/01/2022,09/30/2025
* Philip Dames, Temple University
* Standard Grant
* Ralph Wachter
* 09/30/2025
* USD 234,439.00

Robots will perform better at everyday activities when they can quickly combine
their sensory data into a model of their environment, just like how humans
instinctively use all their senses and knowledge to accomplish daily tasks.
Robots, however, must be programmed to create these models that humans do
intuitively, effortlessly, and robustly. This robotics project explores a novel
algorithmic approach that combines visual and tactile sensory data with a
knowledge of physics and a capability to learn that makes robot planning and
reasoning more effective, efficient, and adaptable. The project includes the
development and testing of research prototypes, preparation of new curriculum,
and outreach to high school students and teachers and to the general
public.&lt;br/&gt;&lt;br/&gt;This project introduces a new data representation,
called a Visual Tactile Neural Field (VTNF), that allows robots to combine data
from visual and tactile sensors to create a unified model of an object. The VTNF
is designed to be used in a closed-loop manner, where a robot may use data from
its physical interactions with an object to create or improve a model and may
use its current understanding of a model to inform how best to interact with a
physical object. Towards this end, the investigators create the mathematical
techniques, computational tools, and robot hardware necessary to generate a VTNF
model. The investigators also develop techniques to quantify the uncertainty
about an object and use this uncertainty to learn search policies that allow
robots to generate accurate models as quickly as possible. The VTNF, which
allows for the easy addition of new properties about an object, provides a
flexible representational foundation for other researchers and practitioners to
use to enable robots to learn faster by having a more detailed understanding of
both the surrounding environment and their interactions with
it.&lt;br/&gt;&lt;br/&gt;This project is supported by the cross-directorate
Foundational Research program in Robotics and the National Robotics Initiative,
jointly managed and funded by the Directorates for Engineering (ENG) and
Computer and Information Science and Engineering
(CISE).&lt;br/&gt;&lt;br/&gt;This award reflects NSF's statutory mission and has
been deemed worthy of support through evaluation using the Foundation's
intellectual merit and broader impacts review criteria.