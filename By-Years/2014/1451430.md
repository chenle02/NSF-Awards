* 1451430
* EAGER: Discrete Algorithms in NLP
* CSE,IIS
* 09/01/2014,08/31/2015
* Samir Khuller, University of Maryland, College Park
* Standard Grant
* Tatiana Korelsky
* 08/31/2015
* USD 75,000.00

Algorithms that can understand human language must be able to recognize the
underlying structure (e.g., subject-verb-object) of that language. Computational
approaches developed in the natural language processing community typically have
build ad hoc, one-off algorithms for solving the hard, combinatorial
optimization problems that arise in such tasks. Most large-scale systems are
built using complex combinations of heuristics applied to try to make
approximate search techniques better. Concurrently, the algorithms community has
developed scalable exact algorithms and approximation algorithms for solving
many of these hard combinatorial optimization problems. This EArly Grant for
Exploratory Research investigates the connection between these two extremes: the
language processing community with the hard problems they need solved, and the
algorithms community with the provably correct algorithms for solving such hard
problems. The biggest technical challenge this exploration addresses is how to
couple the statistical learning algorithms necessary to build effective language
applications with the types of abstractions that make efficient algorithms
possible. In particular, this project explores the application of "inverse
optimization" to machine learning. For example, if one has access to an
efficient algorithm for solving a particular discrete optimization problem, how
can one learn parameters that make that particular algorithm as high accuracy as
possible? Success in this project will give rise to theoretically principled,
efficient algorithms for learning to solve complex linguistic tasks, which can
transform to downstream applications like machine translation, automatic
question answering and information retrieval.&lt;br/&gt;&lt;br/&gt;This
project's main technical innovation is the coupling of "inverse optimization"
problems with online learning techniques. For instance, suppose that the end
goal is to find some particular structure. The search for this structure can
often be cast as a particular form of dynamic programming problem, which in turn
often becomes a shortest path problem in a hypergraph. The machine learning
challenge then is to learn a model under which the solution to this shortest
path search is actually the desired structure. From an algorithmic perspective,
this requires finding a set of inputs under which a given structure is optimal:
inverse optimization. However, it is not enough for a given structure to be
optimal: it must also beat all other (non-optimal) structures by some given
margin. This project will develop a combination of online learning algorithms
and inverse optimization formulations that enable such advances.