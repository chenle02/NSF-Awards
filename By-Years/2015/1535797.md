* 1535797
* AitF:  FULL: Collaborative Research:   PEARL: Perceptual Adaptive Representation Learning in the Wild
* CSE,CCF
* 09/01/2015,04/30/2017
* Kate Saenko, University of Massachusetts Lowell
* Standard Grant
* Tracy Kimbrel
* 04/30/2017
* USD 200,000.00

Vast amounts of digitized images and videos are now commonly available, and the
advent of search engines has further facilitated their access. This has created
an exceptional opportunity for the application of machine learning techniques to
model human visual perception. However, the data often does not conform to the
core assumption of machine learning that training and test images are drawn from
exactly the same distribution, or "domain." In practice, the training and test
distributions are often somewhat dissimilar, and distributions may even drift
with time. For example, a "dog" detector trained on Flickr may be tested on
images from a wearable camera, where dogs are seen in different viewpoints and
lighting conditions. The problem of compensating for these changes--the domain
adaptation problem--must therefore be addressed both in theory and in practice
for algorithms to be effective. This problem is not just a second-order effect
and its solution does not constitute a small increase in performance. Ignoring
it can lead to dramatically poor results for algorithms "in the
field."&lt;br/&gt;&lt;br/&gt;This project will develop a core suite of theory
and algorithms for PErceptual Adaptive Representation Learning (PEARL), which,
when given a new task domain, and previous experience with related tasks and
domains, will provide a learning architecture likely to achieve optimal
generalization on the new task. We expect PEARL to have a significant impact on
the research community by providing a much-needed theoretical and computational
framework that takes steps toward unifying the subfields of domain adaptation
theory and domain adaptation practice. Our theoretical and practical
advancements will impact many application areas by allowing the use of pre-
trained perceptual models (visual and otherwise) in new situations and across
space and time. For example, in mobile technology and robotics, PEARL will help
personal assistants and robots better adapt their perceptual interfaces to
individual users and particular situated environments. At the core of this
project are three main research thrusts: 1) making theoretical advances for
domain adaptation by developing generalized discrepancy distance minimization;
2) using the theoretical guarantees of generalized discrepancy distance to
develop algorithms for key adaptation scenarios of deep perceptual
representation learning, domain adaptation with active learning, and time-
dependent adaptation; 3) advancing the theory and developing algorithms for the
multiple-source adaptation scenario. In addition to our core aims, we plan to
implement our algorithms within a scalable open-source framework, and evaluate
our algorithms on large-scale visual data sets.