* 2047637
* CAREER: Nonconvex Optimization for Statistical Estimation and Learning: Conditioning, Dynamics, and Nonsmoothness
* MPS,DMS
* 02/15/2021,01/31/2026
* Damek Davis, Cornell University
* Continuing Grant
* Stacey Levine
* 01/31/2026
* USD 272,269.00

Nonconvex statistical estimation and learning algorithms are dramatically
improving our capacity to efficiently learn from massive datasets, reshaping
society through new technological capabilities in healthcare, imaging,
transportation, and information processing. Although such learning algorithms
have had widespread empirical success, we have yet to find a coherent
mathematical foundation that can explain not only why they work and what tasks
they provably solve, but also how practitioners can improve their performance
either by adjusting the algorithm or even the task itself. The investigator aims
to lay this foundation by advancing the design, analysis, and deployment of
rigorously justified nonconvex optimization algorithms. This research will
create guaranteed procedures for training practical machine learning systems
deployed in government and industry, producing more reliable and robust
predictive models with fewer data and computational resources. The investigator
will incorporate results from this project in education efforts, including
course development, local K-12 outreach, and research mentoring of Ph.D. and
undergraduate students.&lt;br/&gt;&lt;br/&gt;In this project, the investigator
designs and analyzes nonconvex optimization algorithms. The project focuses on
simple iterative methods that compute with data in its ambient form, a class of
algorithms that are uniquely scalable to modern high-dimensional statistical
estimation and learning tasks. The overarching goal of the project is to
understand when these methods converge to local or global optima and to provide
efficiency estimates of their performance, measured both in terms of data and
computational resources consumed. To achieve this goal, the investigation will
draw on the techniques of variational analysis, nonsmooth optimization, machine
learning, statistics, and high-dimensional probability. The investigator will
leverage these techniques to design and equip simple, scalable iterative methods
for nonconvex data fitting problems with strong performance guarantees: generic
initialization strategies, rapid local convergence near optima, and seamless
adaptation to nonsmooth constraints, models, priors. Such performance guarantees
guide the practical implementation of reliable and efficient numerical methods
for high-dimensional estimation and learning.&lt;br/&gt;&lt;br/&gt;This award
reflects NSF's statutory mission and has been deemed worthy of support through
evaluation using the Foundation's intellectual merit and broader impacts review
criteria.