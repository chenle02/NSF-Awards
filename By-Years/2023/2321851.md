* 2321851
* Collaborative Research: CISE: Large: Executing Natural Instructions in Realistic Uncertain Worlds
* CSE,CNS
* 10/01/2023,09/30/2028
* Alan Fern, Oregon State University
* Continuing Grant
* Hector Munoz-Avila
* 09/30/2028
* USD 562,500.00

For robots to fluidly operate as human assistants they must be able to take
natural language instructions from humans and act to achieve those instructions
in complex, uncertain environments. While there are commodity robots that are
physically capable of carrying out a wide variety of useful instructions,
current artificial intelligence (AI) frameworks are not able to fully understand
and autonomously execute most of those instructions. Rather, current AI
techniques for robot instruction following have generally been limited to highly
constrained, unrealistic environments and instruction formats that are
unnatural, brittle, and rigid. The overarching goal of the project is to study
the fundamental AI principles that enable robots to reliably execute natural
instructions in realistic, uncertain worlds. The project has the potential to
dramatically increase the physical labor available to society, without
increasing the amount of human labor, by enabling typical human workers to
direct semi-automated robots for a multitude of mundane tasks. This will only be
possible if the machines are easily instructable in natural environments by
humans who only require minimal specialized training. This envisioned labor
multiplier is extremely relevant given the need for increased capacity to build
physical infrastructure in the US. This includes, for example, new and upgraded
public infrastructure such as bridges and energy systems, as well as efficient
construction of affordable housing. These same advances will also result in
broader impacts to other parts of the economy, such as logistics, healthcare,
household assistants. The project will also contribute to education and outreach
through K-12 initiatives, undergraduate research experiences, and recruiting of
underrepresented graduate student talent.

The project will design and develop a novel integrated framework for embodied AI
agents that is comprised of synergistic advances in computer vision, language
understanding, world modeling, planning, and control. The framework will evolve
over a staged plan of increasing capabilities, starting with step-by-step
instruction execution and progressing to executing general types of goal-
oriented instructions. The research will test and demonstrate the framework in
both physically-realistic simulation environments and real-world environments
using commodity robots. In addition, user studies will be conducted at each
capability stage to focus the work toward end-user utility. Central to the
framework is a new knowledge structure for spatio-temporal scenes, the multi-
modal entity map (MEM), which is updated based on vision and language and used
for both planning and skill execution. The research will study new ideas in 3D
vision and language understanding for continually maintaining the MEM based on
realistic inputs in a way that captures uncertainty in the environment. The
project will also study a new approach to low-level full-body control for robot
skills, inspired by recent successes in language modeling, that facilitates both
modular skill learning and knowledge sharing. Finally, the project will advance
automated planning capabilities by studying new ideas for learning dynamics
models over the MEMs, which are used by a novel approach to high-level skill
planning based on dynamics-conditioned language models. Importantly all of these
innovations will be developed in a synchronized way to allow for rigorous
testing and demonstration of the integrated framework.

This award reflects NSF's statutory mission and has been deemed worthy of
support through evaluation using the Foundation's intellectual merit and broader
impacts review criteria.