* 1551614
* EAGER: Scaling Up Machine Learning with Virtual Memory
* CSE,IIS
* 10/01/2015,09/30/2017
* Duen Horng Chau, Georgia Tech Research Corporation
* Standard Grant
* Aidong Zhang
* 09/30/2017
* USD 184,904.00

Large datasets in terabytes or petabytes are increasingly common, calling for
new kinds of scalable machine learning approaches. While state-of-the-art
techniques often use complex designs, specialized methods to store and work with
large datasets, this project proposes a minimalist approach that forgoes such
complexities, by leveraging the fundamental virtual memory capability found on
all modern operating systems, to load into the virtual memory space the large
datasets that are otherwise too large to fit in the computer's main memory. This
main idea will allow developers to easily work with large datasets as if they
were in-memory data, enabling them to create machine learning software that is
significantly easier to develop and maintain, yet faster and more scalable.
Developers will achieve higher work efficiency and make fewer programming
errors; companies will reduce operating costs; and researchers will innovate
methodology without getting bogged down by implementation details and
scalability concerns. The proposed ideas could make a far-reaching impact on
industry and academia, in science, education, and technology, as they face
increasing challenges in applying machine learning on large datasets. The
proposed ideas will also help train the next generation of scientists and
engineers by allowing students to learn to work with large datasets in
significantly simpler ways. As virtual memory is universally available on modern
devices and operating systems, the proposed ideas will also work on mobile, low-
power devices, enabling them to perform computation at unprecedented scales and
speed.

This project investigates a fundamental, radical way to scale up machine
learning algorithms based on virtual memory, one that may be easier to code and
maintain, but currently under-utilized in by both single-machine and multi-
machine distributed approaches. This research aims to develop deep understanding
of this radical idea, its benefits and limitations, and to what extent these
results apply in various settings, with respect to datasets, memory sizes, page
sizes (e.g., from the default 4KB to the jumbo 2MB pages that enable terabyes of
virtual memory space), and architectures (e.g., testing on distributed shared
memory file systems like Lustre that support paging and virtual memory over
large computer clusters). The researchers will build on their preliminary work
on graph algorithms that already demonstrates significant speed-up over state-
of-the-art approaches; they will extend their approach to a wide range of
machine learning and data mining algorithms. They will also develop mathematical
models and systematic approaches to profile and predict algorithm performance
and energy usage based on extensive evaluation across platforms, datasets, and
languages.

For further information, see the project web site at:
http://poloclub.gatech.edu/mmap/.