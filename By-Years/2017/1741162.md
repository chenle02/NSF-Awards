* 1741162
* BIGDATA: F: Reliable Inference with Big Data: Reproducibility, Data Sharing, Heterogeneity
* CSE,IIS
* 09/01/2017,08/31/2021
* Andrea Montanari, Stanford University
* Standard Grant
* Sylvia Spengler
* 08/31/2021
* USD 650,000.00

Over the last decade, 'big data' technologies have allowed the acquisition of
vast amount of data (e.g. through smartphones) and their accumulation into large
scale databases. Powerful hardware and software systems have been developed to
crunch these data and extract statistical models. For instance, the outcome of a
certain medical procedure can be modeled in terms of the features of the
patient, thus in principle providing a personalized risk score for that
procedure. Unfortunately, the increasing complexity of these data and of the
algorithms used has made statistical models significantly less transparent. How
certain are we of these statistical predictions? What is their limit of
validity? How biased is the resulting model?&lt;br/&gt;&lt;br/&gt;This project
focuses on four main challenges that are ubiquitous in big-data, and are crucial
to extract reliable insights: reproducibility; data sharing; missing data; data
heterogeneity. &lt;br/&gt;&lt;br/&gt;(1) Reproducibility requires being able to
compare two models extracted from different data sets (e.g. after additional
data have been accumulated). This is in turn impossible unless we have reliable
procedures to quantify uncertainty and confidence in complex high-dimensional
models. Recently proposed ideas in this direction are still insufficient to cope
with realistic large-scale applications.&lt;br/&gt;&lt;br/&gt;(2) Data sharing
is a key feature of modern data analysis, whereby a single massive data set is
being studied by hundreds of independent researchers. Unguarded statistical
inference by such a population of researchers unavoidably leads to large numbers
of false discoveries. The project builds on false discovery rate-controlling
methods to propose safe approaches for decentralized data
analysis.&lt;br/&gt;&lt;br/&gt;(3) Missing data are ubiquitous in big data.
While several methods have been developed in the past to deal with missing data,
it is unclear to what extent they are applicable to modern scenarios. The
project aims at developing principled guidelines based on a rigorous comparison
of various approaches, and developing new algorithms based on maximum
likelihood.&lt;br/&gt;&lt;br/&gt;(4) Data heterogeneity. Big data are often
produced by the aggregation of multiple data sources. How can we prevent
standard statistical procedures to be critically affected by such
heterogeneities? The project uses new regularization schemes to fusion
information across multiple sources.