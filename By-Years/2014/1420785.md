* 1420785
* Doctoral Dissertation: Investigating the role of grammatical representation in language learnability
* SBE,BCS
* 07/15/2014,12/31/2015
* Edward Gibson, Massachusetts Institute of Technology
* Standard Grant
* William Badecker
* 12/31/2015
* USD 11,710.00

Technologies which process natural language have become ubiquitous in the last
decade. Web search engines, for example, process billions of pages of text, in
order to determine which of those pages best match a user's search query. Many
interfaces for interacting with computers -- for example, Apple's Siri personal
assistant -- take voice-issued commands from their users, and must process these
commands in order to follow the users' instructions. Finally, machine
translation technologies have become available for many of the world's most
common languages, allowing users to automatically translate text that they find
in foreign books or websites. These technologies mostly rely on simple models of
language, known as n-gram models or context-free grammars, which were developed
in the 1950's and 1960's, and refined in later decades. These simple models of
language have many advantages, most notably that they can be used to process
large amounts of data very quickly. Because of their simplicity, however, these
models are not able to capture many aspects of meaning in natural language. This
has resulted in limitations for the technologies discussed above; virtual
personal assistants are only able to process very simple types of instructions,
and machine translations is still far from being as accurate as human
translation. In the current project, Leon Bergen and Dr. Edward Gibson will be
investigating more sophisticated kinds of language models, with the goal of
increasing the ability of computers to understand
language.&lt;br/&gt;&lt;br/&gt;Under the direction of Dr. Gibson, Mr. Berger
will be studying language models known as mildly context-sensitive grammars.
These grammars are able to express certain types of linguistic knowledge that
humans have, but which cannot be expressed using simpler types of grammatical
formalisms. For example, native speakers of English know that a declarative
sentence like "Mary kicked the ball" is closely related in meaning to the
question "What did Mary kick?" Although this fact seems obvious, it is difficult
(or impossible) to express using simple types of grammars. However, mildly
context-sensitive grammars can be used to express this knowledge in a very
natural way. Mr. Bergen and Dr. Gibson will be studying whether mildly context-
sensitive grammars can be automatically learned from examples of grammatical
sentences. To do this, they will be using techniques from machine learning, a
branch of computer science and statistics that develops algorithms that can
automatically learn from data. The researchers will integrate these learning
algorithms with their grammatical formalism, and will test whether their method
learns an accurate grammar. The accuracy of the grammar will be evaluated using
a corpus -- a collection of sentences -- in which every sentence has been
manually annotated with its correct grammatical structure. If accurate mildly
context-sensitive grammars can be learned in this manner, then this provides a
potential method for improving the natural language processing technologies
which were discussed above. In particular, because this method does not require
an expert to write down the complete grammar for a language, it has the
potential to be deployed without tremendous engineering effort, and may be
deployed easily in foreign languages.