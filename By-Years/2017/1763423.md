* 1763423
* SHF: Medium: Fairness in Software Systems
* CSE,CCF
* 09/15/2018,08/31/2023
* Alexandra Meliou, University of Massachusetts Amherst
* Continuing Grant
* Sol Greenspan
* 08/31/2023
* USD 1,107,329.00

Software impacts society in many ways and increasingly automates decision-
making. For example, software transcribes videos, translates documents, selects
what news articles are promoted, and determines who gets a loan or gets hired.
It is possible for software to exhibit bias in its operation, whether or not it
is intended by the customers or developers of the software. For example,
software might be more accurate at transcribing male voices than female ones. Or
software may inject societal stereotypes into automated translations, and risk-
assessment computations may exhibit racial bias. As more societal functions
operate in cyberspace, the importance of software fairness increases. In these
settings, data-driven software has the ability to shape human behavior: it
affects the products we view and purchase, the news articles we read, the social
interactions we engage in, and, ultimately, the opinions we form. Biases in data
and software risk forming, propagating, and perpetuating biases in society. This
project develops theory, techniques and tools to enable software designers and
engineers to describe fairness requirements, test the software for fairness
properties, and debug fairness defects. The outcomes of this project will help
increase the society's trust in software decisions and in the data the software
uses, in turn, increasing potential impact and benefits the software can bring
to society.&lt;br/&gt;&lt;br/&gt;The project addresses scientific questions
behind efficiently and effectively measuring potential bias and helping
stakeholders make informed decisions about software. It is not the project's aim
to devise policies or eliminate bias in software. Instead, the aim is to provide
software testing tools and measures that can be validated for formally specified
software fairness properties. To measure bias, the project develops a novel
approach for measuring causal relationships between program inputs and outputs.
Software testing enables conducting causal experiments consisting of running the
software with nearly identical inputs that vary only in a key input
characteristic under test. Variations in an input characteristic that affect
execution behavior provide evidence of a causal relationship. The project
identifies when causal relationships are appropriate for measuring potential
bias, develops efficient testing methods for measuring these relationships, and
creates tools and techniques to help engineers identify and modify the causes of
these relationships.&lt;br/&gt;&lt;br/&gt;This award reflects NSF's statutory
mission and has been deemed worthy of support through evaluation using the
Foundation's intellectual merit and broader impacts review criteria.