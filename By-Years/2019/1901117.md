* 1901117
* RI: Medium: Learning Disentangled Representations for Text to Aid Interpretability and Transfer
* CSE,IIS
* 07/01/2019,06/30/2024
* Byron Wallace, Northeastern University
* Standard Grant
* Tatiana Korelsky
* 06/30/2024
* USD 999,990.00

Machine learning methods for natural language processing power many technologies
that we use on a day-to-day basis, such as spam filters and translation
software. The models underlying these techniques have become increasingly
sophisticated, yielding improved performance but also increasing complexity. In
particular, "neural network" based approaches have re-emerged as the dominant
class of machine learning models for language processing. These approaches often
perform better than their non-neural counterparts, but also have key downsides.
First, training these models requires human effort and time to generate a
sufficiently large set of training data in the form of manually annotated text.
Second, it is often not obvious whether a model trained on one dataset will
generalize to another. Finally, it is hard to discern why such models make the
specific predictions that they do, largely because predictions are made on the
basis of learned representations of texts which do not naturally afford
transparency. This project proposes technical innovations to address these
interrelated issues using "disentanglement". The idea is to design models such
that the learned representations used to make predictions have known meaning.
This approach has the potential to enable re-use of models (increasing
efficiency and reducing human costs), and aid interpretability, so that one can
have a better idea of why a model made a given
prediction.&lt;br/&gt;&lt;br/&gt;To realize the above goals of improved
interpretability and transferability of models, this work will develop and
evaluate new models that learn representations in which certain dimensions are
imbued with explicit semantics. This is a departure from current approaches,
which indiscriminately code all attributes into a single (entangled)
representation. To achieve disentanglement, this project will explore deep
generative models and sparse, gated neural encoders. These will use inductive
biases and light supervision strategies that guide models toward disentangled
representations. For example, models will be penalized if distances in learned
embedding spaces do not reflect human judgments concerning the relative
similarities of instances with respect to specific aspects of interest. In other
cases, "weak" supervision (e.g., rules) may provide adequate guidance for
disentanglement. Finally, "probing" tasks constitute a third supervision
strategy to be explored: This will involve the use of auxiliary tasks to provide
"supervision" that guides individual aspect-wise embeddings of input. The
project will develop and evaluate such models for representative problems in
natural language processing, specifically: classification, sequence tagging, and
summarization. Models will be evaluated both for predictive performance
(including their generalizability to new domains and the efficiency with which
they do so), and the degree to which learned representations are disentangled
and capture the intended aspects.&lt;br/&gt;&lt;br/&gt;This award reflects NSF's
statutory mission and has been deemed worthy of support through evaluation using
the Foundation's intellectual merit and broader impacts review criteria.