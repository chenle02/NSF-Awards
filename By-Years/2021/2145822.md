* 2145822
* CAREER: Collaborative, Communal, and Continual Model Training for Democratizing Machine Learning
* CSE,IIS
* 07/01/2022,06/30/2027
* Colin Raffel, University of North Carolina at Chapel Hill
* Continuing Grant
* Rebecca Hwa
* 06/30/2027
* USD 93,140.00

This award is funded in whole or in part under the American Rescue Plan Act of
2021 (Public Law 117-2).&lt;br/&gt;&lt;br/&gt;Increasingly, much of the software
used today is based on machine learning models. These models are programs whose
behavior is learned from data. Creating these models can be computationally and
economically expensive. Fortunately, many powerful models are shared openly by
their creators. The most popular models are re-used millions of times and form
the backbone of many important pieces of software. However, these models are
seldom updated - they are often left frozen in their originally-released form.
Furthermore, the cost of creating these models excludes most people from their
development. In contrast, the paradigm of open-source software development
provides a means for a distributed community to collaboratively build software.
This kind of collaborative development is currently infeasible for machine
learning models. The goal of this award is to develop the fundamental research
to enable community-developed and continually-improved models. Moreover, in
order to foster a diverse community of contributors, this award includes
collaborative educational activities that include researchers from groups that
are historically underrepresented in the development of machine learning models.
Finally, the award contains an educational component that focuses on the
development of "role-playing" courses, where students present the material to
one another from different perspectives.&lt;br/&gt;&lt;br/&gt;A major challenge
to community-developed and continually-improved models is that the de facto
status of gradient descent as an optimization method for machine learning models
prevents any kind of collaboration because any change to the model involves
updating all of its parameters. This award considers two alternatives: The first
focuses on methods for allowing sparse and low-rank updates that are cheap to
communicate and store and the second considers model architectures where
submodels are devoted to specific capabilities that can be independently
updated. In both cases, it will be made possible to "merge" conflicting updates
that were proposed by contributors performing training in parallel. Separately,
the ability to propose updates to a model necessitates the ability to determine
whether a given proposed update should be accepted. This award therefore
includes a parallel focus on rapid adaptation and evaluation. Throughout the
duration of the award, a software framework will be developed that enables true
iterative version control of machine learning models. The research advances in
the award will therefore see direct practical
application.&lt;br/&gt;&lt;br/&gt;This award reflects NSF's statutory mission
and has been deemed worthy of support through evaluation using the Foundation's
intellectual merit and broader impacts review criteria.