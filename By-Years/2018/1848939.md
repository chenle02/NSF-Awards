* 1848939
* Visual feature perception during dynamic spatial attention and distraction
* SBE,BCS
* 09/01/2019,08/31/2024
* Julie Golomb, Ohio State University
* Continuing Grant
* Betty Tuller
* 08/31/2024
* USD 450,265.00

How does our visual system make sense of the world around us? Our brains
construct incredibly rich perceptual experiences from the rawest of visual
inputs: patterns of light on the eye's retina. A major challenge is that the
environment presents more information than our visual system can fully process
at a time, so we rely on the mechanisms of attention to prioritize the most
relevant information. Attention and perception are vital cognitive processes
that affect every aspect of our daily functioning. Understanding how these
processes typically work - and when we are susceptible to perceptual errors and
distortions - has critical repercussions for both the healthy visual system and
various disorders, along with broad-reaching applications ranging from
maximizing human behavior to development of artificial intelligence and
technology. The project's education component will increase STEM opportunities
for underrepresented racial and ethnic groups, enhance undergraduate education,
and offer community outreach. &lt;br/&gt;&lt;br/&gt;This proposal outlines a
three-year integrated research and education plan addressing how visual feature
perception is altered during dynamic spatial attention and distraction. The
proposal investigates a fundamental challenge for our visual systems: How do we
successfully integrate information about 'what' an object is with 'where' it is?
While the binding process is challenging enough on its own, it becomes
particularly crucial during dynamic vision and cognition, where there are often
multiple objects or locations of interest in the environment and spatial
attention is constantly shifting. Using a paradigm recently developed by PI
Golomb, we test the hypothesis that unstable spatial attention can cause errors
in feature and object perception. In Aim 1, we focus on how visual feature
perception might be altered during conditions of distraction, in collaboration
with co-PI Leber, an expert in attentional control and distraction. In Aim 2, we
expand in an even more fundamental direction, asking how different types of
dynamic spatial attention might impact object integration for multi-feature
objects. The experiments include a combination of perceptual feature reports,
probabilistic mixture modeling, EEG alpha decoding, eye-tracking, and reward-
based manipulations. This innovative and novel approach strives to advance our
understanding of how we achieve stable and integrated visual perception,
especially under conditions of dynamic attention and
distraction.&lt;br/&gt;&lt;br/&gt;This award reflects NSF's statutory mission
and has been deemed worthy of support through evaluation using the Foundation's
intellectual merit and broader impacts review criteria.