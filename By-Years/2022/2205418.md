* 2205418
* Collaborative Research: SCH: Geometry and Topology for Interpretable and Reliable Deep Learning in Medical Imaging
* CSE,IIS
* 09/01/2022,08/31/2026
* Bei Phillips, University of Utah
* Standard Grant
* Sylvia Spengler
* 08/31/2026
* USD 570,102.00

Deep learning models are being developed for safety-critical applications, such
as health care, autonomous vehicles, and security. Their impressive performance
has the potential to make profound impacts on human lives. For example, deep
neural networks (DNNs) in medical imaging have been shown to have impressive
diagnostic capabilities, often near that of expert radiologists. However, deep
learning has not made it into standard clinical care, primarily due to a lack of
understanding of why a model works and why it fails. The goal of this project is
to develop methods for making machine learning models interpretable and
reliable, and thus bridge the trust gap to make machine learning translatable to
the clinic. This project achieves this goal through investigation of the
mathematical foundations -- specifically the geometry and topology -- of DNNs.
Based on these mathematical foundations, this project will develop computational
tools that will improve the interpretability and reliability of DNNs. The
methods developed in this project will be broadly applicable wherever deep
learning is used, including health care, security, computer vision, natural
language processing, etc.&lt;br/&gt;&lt;br/&gt;The power of a deep neural
network lies in its hidden layers, where the network learns internal
representations of input data. This research project centers around the
hypothesis that geometry and topology provide critical tools for analyzing the
internal representations of DNNs. The first goal of this project is to develop a
rigorous mathematical and algorithmic foundation for describing the geometry and
topology of a neural network's internal representations and then design
efficient algorithms for geometric and topological computations necessary to
explore these spaces. The next aim of this project is to apply these tools to
improve the interpretability of deep learning. This will be done by linking a
model's internal representation with interpretable and trusted features and by
interactive visualization that explores the landscape of a model's internal
representation. The next goal of this project focuses on model reliability,
where geometry and topology will be used for failure identification, mitigation,
and prevention. Finally, this project will test the developed techniques for
reliable and interpretable neural networks in a real-world setting to aid expert
oncologists in predicting patient outcomes in head and neck cancers, e.g.,
whether a tumor will metastasize.&lt;br/&gt;&lt;br/&gt;This award reflects NSF's
statutory mission and has been deemed worthy of support through evaluation using
the Foundation's intellectual merit and broader impacts review criteria.