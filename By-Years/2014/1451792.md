* 1451792
* Doctoral Dissertation Research: The acquisition of phoneme categories
* SBE,BCS
* 05/15/2015,06/30/2017
* Elliott Moreton, University of North Carolina at Chapel Hill
* Standard Grant
* William Badecker
* 06/30/2017
* USD 6,080.00

Language comes effortlessly to children, and only adults learning a second
language get a glimpse of the numerous intricacies involved in learning a
language. These range from correlation of word to meaning, to something as
deceptively simple as the categorization of speech sounds. To best illustrate
this, we can look at two examples: differences in perception between English and
Hindi speakers, and differences in perception between English and Japanese
speakers. English speakers have trouble hearing the difference between the "p"
in "pear" and the "p" in "spare," even though they are in reality pronounced
quite differently. A speaker of Hindi on the other hand would have no trouble
hearing this difference. To take another example, while English speakers could
easily tell you that "light" and "right" begin with two different sounds,
Japanese speakers find it very difficult to hear the difference between these
two words. All of these difficulties arise as a byproduct of the human infant's
very efficient learning mechanism when (s)he is first learning a language. The
human brain unconsciously and automatically sorts the large amount of
information we are constantly receiving into ordered data, even before we make
conscious decisions. This study is concerned with determining what critical
pieces of information were received by the Hindi infant to lead him or her to
(unconsciously) determine that those were two different "p" sounds, while the
English infant determined that they were the same sound; or what information led
the Japanese infant to ignore the difference between "l" and "r" when acquiring
Japanese, but did not lead the English infant to ignore the same difference.
This study both contributes to broad theoretical issues within cognitive science
by addressing how categories are learned by humans, and also leads to practical
applications, such as more efficient second language learning techniques and
advancements in speech pathology.&lt;br/&gt;&lt;br/&gt;Previous work has
identified at least two main cues that humans use when categorizing sounds: the
learner's growing vocabulary ("vocabulary cue") and acoustic properties found
within a stream of speech ("acoustic cue"). The vocabulary cue groups together
sounds occurring in similar word environments ("light" and "right") into a
single sound category. The acoustic cue maps sounds into some acoustic map and
picks out regions of high frequencies on this map. This study makes use of past
observations that adults can unconsciously simulate, at least to some degree,
aspects of the infant's learning experience within a laboratory setting. This
experiment will test how much adults rely on each of these two cues by
presenting participants with an artificial language where the vocabulary cue and
the acoustic cue give the participant conflicting information. In this way, the
research will contribute to a better understanding of what unconscious decisions
the human brain makes when categorizing speech sounds. This study addresses two
fundamental issues within linguistics and within cognitive science: (1) how the
human brain forms complex categories, and (2) how much of language is acquired
through specialized language-specific modules and how much of language can be
attributed to general cognitive mechanisms.