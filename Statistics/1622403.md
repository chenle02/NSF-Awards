
* 1622403
* Collaborative Research: Advancing Statistical Surrogates for Linking Multiple Computer Models with Disparate Data for Quantifying Uncertain Hazards
* DMS,STATISTICS, CDS&E-MSS
* 08/15/2016,08/11/2016
* Robert Wolpert,NC,Duke University
* Standard Grant
* Christopher Stark
* 07/31/2019
* USD 243,785.00

The Oso, Washingon landslide of 2014, which resulted in 43 fatalities, and the
ash plumes from the Eyjafjallaj√∂kull (Iceland) eruption of 2010, which shut down
air travel in Europe, are examples of rare and catastrophic geophysical events.
Their rare nature makes such events nearly impossible to forecast, if forecasts
are based only on previous observations. To capture rare events, researchers
must rely on complex physical and mathematical models that often require
significant computational resources to exercise. Furthermore, events like these
may be best described by a series of different models of different phenomena at
different scales. For example, a researcher may need to combine a model of
rainfall, a model of slope failure, and a model of sliding debris to create on
overall model for a landslide event. The main objective of this research is the
development of efficient statistical and computational strategies to combine
such models, thus advancing the state of the art in hazard forecasting.

Direct simulation-based hazard assessment would require thousands to tens of
thousand of linked, space-time simulations. Furthermore, to be of most use in
hazard assessment, these simulations should be informed and validated by
observational data sets, which themselves can range from sparse data (rare
events) to massive data (e.g. satellite data), and explored for emerging
scenarios. To complicate the matter, a number of features of the problems of
interest are either poorly characterized or unpredictable, and one would like to
run the simulation programs at a range of values of each of them; this quickly
leads to a perceived need to run a simulation program (which may take hours to
complete) for hundreds of thousands or millions of different combinations of
parameter values and conditions. There simply is not enough time or enough
computing power for such a brute force approach to succeed. To tackle the
situation just described, the PIs will continue to develop parallel partial
emulators for massive space-time simulator data allowing emulator construction
on the adaptive space-time grids commonly used in geophysical simulations,
creating smoothers for their output, and enabling the use of reduced input
spaces. The PIs will begin the investigation of a strategy for linking multiple
simulators via multiple emulators. A particularly powerful semi-analytic way of
linking emulators will be pursued, with a variety of research questions arising
centering around the accuracy of the method, as well as the possibility of its
implementation in the huge data scenario envisaged for the parallel partial
emulator. The PIs will also begin to investigate techniques to extract (nearly)
optimal basis sets, data reduction methods, and algorithmic approaches to
accelerate the construction of emulators, all of which contribute to a more
robust handling of large datasets. These new methodologies will provide tools to
rapidly construct probability-based hazard forecast maps for cascading
geophysical events. Rapid forecast maps allow end users to perform hazard
analysis under a wide variety of aleatoric scenarios. Furthermore this new
methodology will enable fast assessment of epistemic uncertainties. This
approach constitutes a dramatic improvement in scientifically-based decision
support.
