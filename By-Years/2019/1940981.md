* 1940981
* EAGER: Learning a High-Fidelity Semantic Parser
* CSE,IIS
* 10/01/2019,05/31/2023
* Lenhart Schubert, University of Rochester
* Standard Grant
* Tatiana Korelsky
* 05/31/2023
* USD 181,108.00

Communication with computers in ordinary language is a long-sought goal of AI
researchers, educational, commercial, and government enterprises, and everyone
who uses computers. The most impressive systems to date depend on coding of
thousands of specialized "skills" by thousands of expert programmers. Ordinary
comments such as "I'm afraid I won't make it to the meeting" and "She managed to
get the insulin shot in time" are not understood well enough to draw obvious
conclusions such as "I won't be at the meeting" and "She got the insulin shot in
time". This exploratory EAGER project takes a step towards machine understanding
of ordinary language, by providing a comprehensive way of representing the
content of language in machines, and developing a machine learning technique
that allows computers to translate language into that representation, and hence
make the kinds of inferences mentioned. This in turn provides immediate tools
for improving systems that require some degree of general understanding and
inference, such as dialogue systems, sentiment analysis systems, and systems
that extract desired knowledge from text. The high-fidelity representations of
meaning produced by the semantic parser also provides a substrate for deriving
deeper meanings, using what we know about the way discourse segments form
coherent passages, and making use of general knowledge about word meanings and
the world. The project team consists of a diverse group guided by the project
principal investigators, several graduate-level and a dozen undergraduate-level
researchers.&lt;br/&gt;&lt;br/&gt;This project focuses on deriving "unscoped
logical forms" (ULFs) reflecting the semantic type structure of standard English
sentences with unprecedented fidelity, covering not only predication but also
quantification, tense, modality, reification, predicate and sentence
modification, comparison structures, and other semantic phenomena. As such, it
moves well beyond the expressive range of current mainstream approaches, such as
Abstract Meaning Representation (AMR). Thanks to its type coherence, ULF
supports forward discourse inferences from text in a more comprehensive way than
Natural Logic, and without requiring knowledge of a target hypothesis to be
confirmed or disconfirmed. Demonstrating inferences from clause-taking verbs,
counterfactuals, questions, and requests provides an important proof of concept.
The semantic ULF parser is produced by supervised learning of a cache transition
parser, much like one previously applied successfully to AMR parsing, but
enhanced by prioritizing type-consistent operator-operand
combinations.&lt;br/&gt;&lt;br/&gt;This award reflects NSF's statutory mission
and has been deemed worthy of support through evaluation using the Foundation's
intellectual merit and broader impacts review criteria.