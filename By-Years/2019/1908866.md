* 1908866
* RI: Small: Sparse Reconfigurable Artificial Neural Systems: Optimal Neuron Selection and Generalization
* CSE,IIS
* 10/01/2019,09/30/2023
* L. Darrell Whitley, Colorado State University
* Standard Grant
* Kenneth Whang
* 09/30/2023
* USD 449,999.00

Machine learning and Artificial Intelligence are fueling a revolution that is
making it possible to do better prediction from data, to better search for
images on the internet, and even to better talk to computers using natural
language. This project introduces novel machine learning methods for training
artificial networks of neurons. This research also has the potential to
contribute to neural science and the understanding of biological brain function.
Humans display fast and flexible learning. How are our brains wired to do what
they do? During normal brain development, the process of programmed cell death
represents a form of "neuron selection" that helps to shape the size and
configuration of different information processing centers in the brain. In
effect, this wires our brain to do particular tasks. This is also thought to
represent one of the most basic forms of learning. This research introduces new
methods for "neuron selection" as a form of learning by
machines.&lt;br/&gt;&lt;br/&gt;Current learning methods for artificial networks
of neurons largely focus on adjusting signal strength between neural cells.
Adjusting the strength of these signals is a slow and repetitive process.
However, human learning is often spontaneous. This project looks at how
artificial networks of neurons can learn by turning neurons on and off, enabling
the same network to be reconfigured for multiple learning tasks. Preliminary
experiments show that this can be highly effective and can result in good
generalization, even when using neurons with fixed randomly generated signals.
The proposed methods do not just identify "useful neurons." Instead, these
methods can identify "coalitions of neurons" that work together as a team to
achieve a particular goal. Neuron selection can be executed much more rapidly
than learning signal strength between neurons. The proposed methods guarantee a
linear time bound on learning. The methods are also guaranteed to converge to
the optimal "team of neurons" relative to a given starting configuration and
learning task.&lt;br/&gt;&lt;br/&gt;This award reflects NSF's statutory mission
and has been deemed worthy of support through evaluation using the Foundation's
intellectual merit and broader impacts review criteria.