* 0906808
* Statistical inference when both the model and/or data dimension is large
* MPS,DMS
* 09/01/2009,08/31/2013
* Peter Bickel, University of California-Berkeley
* Standard Grant
* Gabor Szekely
* 08/31/2013
* USD 519,903.00

The investigator is studying inference for data which is both high dimensional
and complex.The main topics investigated are: I. Identifying graphical models
II.Ascribing explanatory power to variables III.Frequentist behaviour of
nonparametric Bayes procedures IV.Particle filters The framework is non and
semiparametric and the results will be asymptotic.But the qualitative insights
gained have led to the discovery of new methods by the investigator and should
do so again.

A preeminent feature of 21st century data in almost all fields is their
complexity compared to he number of replicates.Images can be viewed as vectors
with dimensions in the thousands,climate models produce vectors giving predicted
values at tens of thousands of locations ,genomes are 3 billion basepairs
long.Accompanying this type of data is a dearth of models for their
generation.What theory there is for such situations tells us that we should be
unable to do anything without impossibly large numbers of replicates.Yet are
coping,we believe because ,if we consider predictions the models are
"sparse"(Most factors are irrelevant) or data are "sparse" (The factors which do
matter are highly dependent and can in fact be represented much more compactly
than is apparent.) The investigator is studying these underlying ideas and
developing models which apply in contexts including climate modeling, genomics,
and astronomy.