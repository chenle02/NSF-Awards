* 1738888
* STTR Phase II:  Dynamic Robust Hand Model for Gesture Intent Recognition
* TIP,TI
* 09/01/2017,08/31/2021
* Karthik Ramani, ZeroUI Inc
* Standard Grant
* Muralidharan Nair
* 08/31/2021
* USD 1,091,828.00

The broader impact/commercial potential of this project stems from addressing
the&lt;br/&gt;important hand gesture based input challenges of VR/AR (expected
to grow to $150B by&lt;br/&gt;2020), Robotics and IoT ($135B by 2019 and $1.7T
by 2020 respectively). This technology,&lt;br/&gt;if successful in mitigating
the high technical risks, represents a huge leap in the state of
the&lt;br/&gt;art in 3D hand models for gesture recognition and has the
potential to be the industry&lt;br/&gt;standard for AR, VR, Robotics and IoT
applications with broad societal impact in education,&lt;br/&gt;medical and
healthcare. Its broader impact is further amplified by the potential in serving
the&lt;br/&gt;needs of the disabled community in improving their quality of life
by being better able to&lt;br/&gt;communicate, learn and adapt to their
interaction needs.&lt;br/&gt;&lt;br/&gt;This Small Business Technology Transfer
(STTR) Phase 2 project aims to significantly&lt;br/&gt;advance current 3D hand
gesture recognition technology by developing a dynamic hand&lt;br/&gt;tracking
model for gesture intent recognition. It is robust against occlusion and
tolerant to&lt;br/&gt;variations in camera orientation and position. This
research will result in a transformative&lt;br/&gt;leap above the current state
of academic and commercial hand models and overcome key&lt;br/&gt;technical
hurdles that have so far proven difficult to overcome. It solves the following
key&lt;br/&gt;challenges and involves very high technical risks: 1) robust hand
tracking while holding&lt;br/&gt;objects and 2) robust tangible interactions
using objects without using any fiducial markers&lt;br/&gt;2) low profile hand
wearable for touch interaction detection. This Phase 2 project
will&lt;br/&gt;achieve these objectives by 1) data acquisition and hand-object
pose estimation, 2)&lt;br/&gt;understanding user intents with enhanced tangible
interactions, and 3) system validation&lt;br/&gt;and user testing.