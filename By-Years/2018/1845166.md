* 1845166
* CAREER: Extracting principles of neural computation from large scale neural recordings through neural network theory and high dimensional statistics
* CSE,IIS
* 10/01/2019,09/30/2024
* Surya Ganguli, Stanford University
* Standard Grant
* Kenneth Whang
* 09/30/2024
* USD 500,000.00

Recent technological advances now enable recordings of thousands of neurons
during complex behaviors. Such experimental capabilities could potentially
reveal how the brain encodes sensations, forms memories, learns tasks, makes
decisions, and generates motor actions. However, there exist major obstacles to
attaining a scientific understanding of how the psychological capabilities of
the mind emerge from the biological wetware of the brain. First, data analytic
methods are not adequate to make sense of the massive datasets currently being
gathered from the brain. Second, theoretical methods are not adequate for both
optimally designing large-scale neural recordings, and bridging scales from the
collective biophysics of many neurons to psychological processes underlying
sensations, thoughts and actions. This project will develop novel data analytic
and theoretical methods to extract a conceptual understanding of how the brain
gives rise to cognition. These methods will be tested in large-scale recordings
from many experimental labs studying perception, memory, learning, decision
making and motor control. They will also be applied to developing better
learning protocols and neural prosthetic devices.&lt;br/&gt;&lt;br/&gt;This
project will pursue three overarching aims. It will build on advances in high
dimensional statistics to develop a theory of when and how subsets of neurons
reflect the collective dynamics of the much larger unobserved circuit in which
they are embedded. This theory will provide quantitative guidance for the
efficient design of future large-scale recording experiments. Second, it will
build on advances in deep learning to develop algorithmic methods for extracting
a conceptual understanding of how complex neural networks solve tasks. These
algorithmic methods will elucidate which aspects of network connectivity and
dynamics are essential to understanding how neural circuits perform their
computations, thereby providing guidance for what to measure in future
neuroscience experiments. Finally, it will advance theories of neural network
learning to better understand how the structure of prior experience determines
learned neural connectivity, and how this learning process can be optimized.
These general theoretical advances will be refined and tested in specific, close
experimental collaborations, involving: identifying feedback control laws in
motor cortex, finding signatures of attractor dynamics in the hippocampal memory
circuits, understanding the neural algorithms for perception in the retina and
decision making in prefrontal cortex, and developing frameworks for
understanding rapid rodent learning built upon prior
experiences.&lt;br/&gt;&lt;br/&gt;This award reflects NSF's statutory mission
and has been deemed worthy of support through evaluation using the Foundation's
intellectual merit and broader impacts review criteria.