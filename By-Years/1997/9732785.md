* 9732785
* A Qualitative Study of Time-Lagged Recurrent Networks
* ENG,ECCS
* 09/01/1998,07/31/2000
* Derong Liu, Stevens Institute of Technology
* Continuing Grant
* Paul Werbos
* 07/31/2000
* USD 116,285.00

ECS-9732785&lt;br/&gt;Liu&lt;br/&gt;Time-Lagged Recurrent Networks (TLRN)are
known to be more powerful than feedforward multilayer neural networks and in
nonlinear systems identification and control. This project will investigate
several fundamentally important theoretical and practical questions concerning
TLRN's in two areas:&lt;br/&gt;&lt;br/&gt;Structural adaptivity and robustness
analysis: For nonlinear function&lt;br/&gt;approximation using neural networks,
the behavior of time-lagged&lt;br/&gt;recurrent networks will be studied in
comparison to feedforward neural networks. &lt;br/&gt;It will be shown that, due
to the existence of recurrencies in the network,&lt;br/&gt;time-lagged recurrent
networks will exhibit behavior equivalent to&lt;br/&gt;multilayer feedforward
neural networks with time-varying parameters. It is&lt;br/&gt;expected that a
multilayer feedforward neural network with time-varying&lt;br/&gt;paxameters
will exhibit adaptive behavior; and the study will show that time-lagged
recurrent networks with fixed parameters will generally exhibit
adaptive&lt;br/&gt;behavior. A thorough study of how time-lagged recurrent
networks will&lt;br/&gt;perform under parameter perturbations will also be
conducted. Results&lt;br/&gt;will be given as bounds for permissible parameter
perturbations which&lt;br/&gt;guarantee to retain the desired performance of the
network.&lt;br/&gt;&lt;br/&gt;Training based on energy function approach:
Stability of recurrent&lt;br/&gt;Neural networks which can be translated into
convergent behavior and bounded&lt;br/&gt;signals and parameters in the network
is of fundamental interest. The&lt;br/&gt;stability properties of tiine-lagged
recurrent networks will be studied&lt;br/&gt;using the Lyapunov's second method.
In applying the Lyapunov second method, one&lt;br/&gt;needs to construct a
Lyapunov function (or energy function). Stability is&lt;br/&gt;guaranteed by the
fart that the Lyapunov function is positive definite and&lt;br/&gt;is
(monotonically) decreasing over time. This stability analysis result
will&lt;br/&gt;shed new lights on the training of neural networks for systems
identification,&lt;br/&gt;which will lead to training algorithms with guaranteed
convergence to the global&lt;br/&gt;minimum or to a solution.&lt;br/&gt;