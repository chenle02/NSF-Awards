
* 9801602
* Quantitative Ergodic Theorems; Spectra of Transfer Operators
* DMS,PROBABILITY, ANALYSIS PROGRAM
* 07/01/1998,11/24/1999
* Mate Wierdl,TN,University of Memphis
* Standard Grant
* Joe W. Jenkins
* 12/31/2001
* USD 69,001.00

Abstract Wierdl This proposal for research concerns questions of almost
everywhere and mean convergence in ergodic theory, and their connections with
harmonic analysis and probability theory. Also considered are spectral questions
for matrix-coefficient transfer operators. Some of the convergence questions
which will be addressed by Wierdl concern the mean convergence of averages of
measurements made on a stochastic process at a random sequence of times that is
chosen in advance. Other questions concern subsequence ergodic theorems for
subsequences coming from members of Hardy fields, along bases for the integers,
and along sequences with big gaps. Results obtained by Wierdl in the previous
grant periods suggest that in the context of Hardy fields, a meaningful
characterization of the ``good'' sequences of measurements is possible. Campbell
and Wierdl will jointly consider another group of convergence questions
concerning upcrossings and related oscillatory behavior of the ergodic averages.
This line of research was initiated by Bishop, Bourgain, Kalikow, B. Weiss and
others. Wierdl and his collaborators discovered a fundamental connection between
Ergodic Theory and martingales. This discovery---which often manifests itself as
a squarefunction of the difference between ergodic averages and certain
martingales---allows one to translate many of the results of Martingale Theory,
such as squarefunction, large deviation or upcrossing inequalities, to ergodic
theoretical results. The so called Ergodic Theorem (ET) is a theoretical
description of a certain statistical behavior of matter. Loosely speaking, the
ET makes it possible to make a large class of physical measurements in two
entirely different ways, but arrive to the same result. It is useful to have an
illustration: suppose we want to find out the average speed of the water
particles in a cup of water. Well, we would have to measure the speed of each
particle separately, and then take the average of these measurements. This
method of calculating the average speed is called averaging in *space* since we
go to different points (particles) in space to make the measurement. But the ET
says, that we would arrive to the same result if we pick a *single* particle,
and start measuring its speed at 1 second, at 2 second, at 3 second, etc, and
after a long enough period of time take the average of these measurements. This
method of calculating the average is called averaging in *time*. So the above is
two very different ways of calculating average speed, and the method to be
chosen depends on the circumstance. Having two possibilities gives great freedom
in experiments. The problem with the ET is twofold: 1) It does not specify
exactly *how many* times are we supposed to measure the speed of the single
particle; the ET just says "eventually" the time average gets close to the space
average. 2) The measurements in time has to be made *exactly* at 1s, 2s, 3s,
etc. But in practice, one cannot make measurements so promptly. The ET says
nothing if the measurements are made at, say, 1.1s, 2.8s, 3.2s, ...Our proposal
addresses the above two problems: 1) we propose various ways to determine how
many times does one need to make measurements in time to get close with
prescribed accuracy to the space average. 2) We examine various randomly
generated times at which measurements made will still give an average
approaching the space average. (Note Randomly generated times are much more
likely to model practical situations.)
