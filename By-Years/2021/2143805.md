* 2143805
* CAREER: Uncovering the neural dynamics of speech monitoring processes during language production
* SBE,BCS
* 08/01/2022,07/31/2027
* Stephanie Ries-Cornou, San Diego State University Foundation
* Continuing Grant
* Jonathan Fritz
* 07/31/2027
* USD 321,649.00

Language is one of the distinctive cognitive faculties that is unique to humans.
We often take our fluency in speech production for granted. Remarkably, although
adult, native-language speakers can produce 2 to 3 words per second as they
speak, they only make speech errors about once every thousand words! Speech
monitoring is the mechanism by which we control our speech production so
efficiently to avoid or correct errors as we are speaking. Importantly, we are
able to monitor our speech both before and after we have heard ourselves speak,
but how our brain supports speech monitoring is still unclear and is the focus
of this study. The researchers will challenge speakers with tongue-twisters and
other speech tasks and use a powerful technique to study brain activity during
task performance. They will investigate which parts of our brain allow us to
monitor our speech production before versus after we hear ourselves speak. They
will also examine how these brain regions interact with one another. The results
of this research will help us understand how humans monitor their speech
production in real time and will be relevant to understanding how and why speech
monitoring function can become impaired in speech disorders such as aphasia.
These studies may also lead to insights as to how speech impairment can be
corrected. Researchers will develop an outreach program centered around lectures
on language and the brain to bring these insights to a broad, general audience,
and to provide educational opportunities especially to under-represented
minorities from elementary school to graduate school. &lt;br/&gt;&lt;br/&gt;In
this project, the combined excellent spatial and temporal resolution of
intracranial electroencephalography (iEEG), a brain imaging technique, will be
used to precisely identify which brain regions are involved at which stage of
speech monitoring while we produce language. Specifically, researchers will use
the high resolution of iEEG to investigate whether speech perception is
independently engaged in monitoring our speech before we actually produce it, or
whether instead, inner speech monitoring relies more on our speech production
system in tandem with domain-general action monitoring. The results of this
research will help resolve competing cognitive models of how we monitor speech
production, and also will illuminate how the different brain regions involved in
speech monitoring functionally interact in real time before versus after speech
output. Because iEEG is an invasive brain imaging technique that is used for
clinical monitoring purposes in patients with epilepsy (to localize epileptic
foci for surgical removal), it is also necessary to establish whether the
behavioral results found in the patients who have volunteered for this study,
fall within the normal range. Therefore, researchers will in parallel collect
behavioral data online in control participants in order to compare the
performance of patients with epilepsy to that of normal, matched controls.
Overall, this research will be transformative and will have tremendous impact in
neuroscience, cognitive psychology and psycholinguistics and provide deeper
understanding of the impressive monitoring and error-correcting mechanisms in
our brains that enable us to speak clearly.&lt;br/&gt;&lt;br/&gt;This award
reflects NSF's statutory mission and has been deemed worthy of support through
evaluation using the Foundation's intellectual merit and broader impacts review
criteria.